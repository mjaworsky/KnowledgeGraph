{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3520, 129)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_ML_nona.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 323us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 84us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 82us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 82us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 87us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 9.3475 - binary_accuracy: 0.39400s - loss: 9.2489 - binary_accuracy: 0.\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 78us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 84us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 87us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 79us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 80us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 87us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 82us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 87us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 87us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 113us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 82us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 75us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 79us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 79us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 72us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 74us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 71us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 71us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 72us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 73us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 71us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 71us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 77us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 86us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 100us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 97us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3475 - binary_accuracy: 0.3940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-49849268.0, -49849624.0, -49850156.0, -49850230.0, -49852596.0, -49852804.0, -49854252.0, -49854360.0, -49854416.0, -49854576.0, -49855220.0, -49855820.0, -49856620.0, -49856960.0, -49856970.0, -49857516.0, -49857830.0, -49858120.0, -49859040.0, -49859044.0, -49859584.0, -49859588.0, -49859624.0, -49859864.0, -49859924.0, -49860210.0, -49860212.0, -49860410.0, -49860464.0, -49860500.0, -49860760.0, -49861050.0, -49861696.0, -49862216.0, -49862228.0, -49864900.0, -49864908.0, -49865450.0, -49866084.0, -49866652.0, -49867204.0, -49906972.0, -49907510.0, -49908092.0, -49908100.0, -49909344.0, -49909348.0, -49909604.0, -49910996.0, -49911284.0, -49911364.0, -49912012.0, -49912210.0, -49912228.0, -49912256.0, -49912308.0, -49912416.0, -49913164.0, -49914084.0, -49914300.0, -49914330.0, -49915212.0, -49915380.0, -49915436.0, -49915760.0, -49916670.0, -49916700.0, -49917296.0, -49917900.0, -49918092.0, -49918096.0, -49919052.0, -49919056.0, -49920200.0, -49920228.0, -49920240.0, -49920544.0, -49922300.0, -49922800.0, -49924330.0, -49924376.0, -49924910.0, -49924916.0, -49925460.0, -49925470.0, -49925476.0, -49925496.0, -49926080.0, -49926144.0, -49926148.0, -49965160.0, -49965190.0, -49966400.0, -49967504.0, -49967524.0, -49967570.0, -49968160.0, -49968388.0, -49968668.0, -49968750.0, -49968910.0, -49968970.0, -49969040.0, -49969084.0, -49969096.0, -49969292.0, -49969332.0, -49969336.0, -49969624.0, -49969756.0, -49970320.0, -49971064.0, -49971430.0, -49971460.0, -49971596.0, -49971620.0, -49972012.0, -49972556.0, -49972624.0, -49972904.0, -49973376.0, -49973470.0, -49974988.0, -49976104.0, -49976640.0, -49976990.0, -49977280.0, -49977296.0, -49977904.0, -49978104.0, -49978770.0, -49979388.0, -49979904.0, -49979924.0, -49980090.0, -49980228.0, -49980452.0, -49980788.0, -49981028.0, -49981636.0, -49981960.0, -49982612.0, -49982628.0, -49984988.0, -50023480.0, -50024096.0, -50024576.0, -50024624.0, -50024656.0, -50025210.0, -50025230.0, -50025236.0, -50025256.0, -50025800.0, -50025812.0, -50025844.0, -50025868.0, -50026450.0, -50027056.0, -50027268.0, -50029028.0, -50029068.0, -50029640.0, -50029660.0, -50029664.0, -50029668.0, -50029680.0, -50030170.0, -50030336.0, -50030748.0, -50031130.0, -50031140.0, -50031468.0, -50032012.0, -50032076.0, -50032620.0, -50033184.0, -50033190.0, -50033228.0, -50033812.0, -50034020.0, -50034292.0, -50034316.0, -50034410.0, -50037276.0, -50037948.0, -50038520.0, -50038532.0, -50038980.0, -50039604.0, -50040268.0, -50040290.0, -50040780.0, -50040790.0, -50040890.0, -50041296.0, -50041444.0, -50042670.0, -50043784.0, -50043788.0, -50082204.0, -50082330.0, -50084180.0, -50085252.0, -50085536.0, -50085572.0, -50085820.0, -50086190.0, -50086200.0, -50086390.0, -50086470.0, -50086748.0, -50086908.0, -50087308.0, -50087884.0, -50088160.0, -50088400.0, -50088504.0, -50088692.0, -50089630.0, -50089640.0, -50089892.0, -50089960.0, -50090812.0, -50090836.0, -50091464.0, -50092080.0, -50092664.0, -50092670.0, -50093140.0, -50094356.0, -50094576.0, -50094656.0, -50095016.0, -50097280.0, -50097860.0, -50097904.0, -50097916.0, -50097932.0, -50097960.0, -50098484.0, -50098544.0, -50099076.0, -50099620.0, -50101450.0, -50102036.0, -50102064.0, -50102636.0, -50139770.0, -50139844.0, -50140910.0, -50141732.0, -50142136.0, -50142256.0, -50142340.0, -50142904.0, -50143268.0, -50143320.0, -50143336.0, -50143360.0, -50143916.0, -50144410.0, -50144944.0, -50145068.0, -50145224.0, -50145816.0, -50145836.0, -50145880.0, -50145904.0, -50146180.0, -50146516.0, -50146884.0, -50147304.0, -50147384.0, -50147868.0, -50148016.0, -50149056.0, -50149060.0, -50149080.0, -50149130.0, -50149172.0, -50149656.0, -50150292.0, -50150990.0, -50151390.0, -50151428.0, -50151480.0, -50151536.0, -50151564.0, -50151584.0, -50153816.0, -50153892.0, -50154960.0, -50155580.0, -50156210.0, -50157330.0, -50157916.0, -50159070.0, -50159710.0, -50160828.0, -50199956.0, -50200596.0, -50200932.0, -50200950.0, -50201612.0, -50202296.0, -50202304.0, -50202710.0, -50202770.0, -50203120.0, -50203228.0, -50203524.0, -50203840.0, -50203920.0, -50204180.0, -50204564.0, -50205136.0, -50205268.0, -50205320.0, -50206132.0, -50206428.0, -50206824.0, -50206870.0, -50207090.0, -50208040.0, -50208636.0, -50208640.0, -50208772.0, -50208828.0, -50208860.0, -50209200.0, -50209216.0, -50209412.0, -50209456.0, -50209990.0, -50210000.0, -50210240.0, -50210510.0, -50210572.0, -50211460.0, -50212120.0, -50212136.0, -50212656.0, -50212716.0, -50212776.0, -50212904.0, -50213216.0, -50213228.0, -50213230.0, -50213324.0, -50213350.0, -50213440.0, -50213530.0, -50213550.0, -50213836.0, -50213956.0, -50214052.0, -50214136.0, -50214324.0, -50214384.0, -50214440.0, -50215000.0, -50215080.0, -50215104.0, -50215540.0, -50215616.0, -50256830.0, -50259776.0, -50259820.0, -50260930.0, -50261610.0, -50261656.0, -50262090.0, -50262230.0, -50262704.0, -50262760.0, -50262790.0, -50262810.0, -50263590.0, -50263960.0, -50264344.0, -50264388.0, -50265652.0, -50265696.0, -50265704.0, -50265864.0, -50265910.0, -50266148.0, -50266200.0, -50266264.0, -50266804.0, -50267020.0, -50267084.0, -50267356.0, -50269100.0, -50269290.0, -50269750.0, -50269776.0, -50269980.0, -50270416.0, -50270540.0, -50270612.0, -50270624.0, -50270892.0, -50270910.0, -50270924.0, -50271024.0, -50271070.0, -50271090.0, -50271188.0, -50271416.0, -50271508.0, -50271550.0, -50271572.0, -50271620.0, -50271780.0, -50272056.0, -50272348.0, -50272660.0, -50272948.0, -50273390.0, -50273540.0, -50273868.0, -50273900.0, -50274024.0, -50274120.0, -50274132.0, -50274140.0, -50274410.0, -50274570.0, -50276080.0, -50277380.0, -50315836.0, -50316290.0, -50316320.0, -50317440.0, -50318276.0, -50318640.0, -50319120.0, -50319388.0, -50320604.0, -50321156.0, -50321252.0, -50321830.0, -50321840.0, -50322028.0, -50322080.0, -50322280.0, -50322332.0, -50322370.0, -50323030.0, -50323550.0, -50323936.0, -50324120.0, -50324428.0, -50324516.0, -50324532.0, -50324770.0, -50325692.0, -50326184.0, -50326424.0, -50326844.0, -50327508.0, -50327936.0, -50328160.0, -50328230.0, -50328244.0, -50328588.0, -50328708.0, -50328816.0, -50329464.0, -50329724.0, -50329860.0, -50330044.0, -50330296.0, -50330908.0, -50330984.0, -50330996.0, -50331016.0, -50331456.0, -50331490.0, -50331508.0, -50331640.0, -50332096.0, -50332636.0, -50332696.0, -50333308.0, -50333816.0, -50333860.0, -50337290.0, -50337376.0, -50337970.0, -50374556.0, -50374560.0, -50377050.0, -50378172.0, -50378740.0, -50379680.0, -50380250.0, -50380356.0, -50380492.0, -50380930.0, -50381052.0, -50381230.0, -50381510.0, -50381520.0, -50381644.0, -50381784.0, -50382050.0, -50382092.0, -50382772.0, -50382988.0, -50383276.0, -50383292.0, -50383310.0, -50383620.0, -50383744.0, -50383890.0, -50383944.0, -50383984.0, -50384340.0, -50384484.0, -50384510.0, -50384916.0, -50385030.0, -50385332.0, -50385590.0, -50385684.0, -50385760.0, -50386204.0, -50386210.0, -50386230.0, -50386480.0, -50386536.0, -50386864.0, -50386880.0, -50386930.0, -50386932.0, -50388830.0, -50389276.0, -50390572.0, -50390620.0, -50390660.0, -50391030.0, -50393308.0, -50393830.0, -50394496.0, -50395104.0, -50395110.0, -50395130.0, -50395600.0, -50395644.0, -50396264.0, -50396812.0, -50435110.0, -50436540.0, -50437064.0, -50437496.0, -50437710.0, -50438330.0, -50438690.0, -50439456.0, -50439756.0, -50440380.0, -50440692.0, -50440828.0, -50441550.0, -50441584.0, -50443224.0, -50443560.0, -50443624.0, -50443804.0, -50443896.0, -50444150.0, -50444160.0, -50444410.0, -50444544.0, -50444804.0, -50445064.0, -50445096.0, -50445332.0, -50445348.0, -50445390.0, -50445650.0, -50445680.0, -50445870.0, -50446444.0, -50446532.0, -50446596.0, -50447124.0, -50447340.0, -50447444.0, -50447500.0, -50447644.0, -50447730.0, -50448010.0, -50448520.0, -50448610.0, -50448860.0, -50449116.0, -50449184.0, -50449464.0, -50449508.0, -50449800.0, -50449810.0, -50449844.0, -50449850.0, -50449950.0, -50450404.0, -50450628.0, -50450924.0, -50450956.0, -50450996.0, -50451210.0, -50451570.0, -50451836.0, -50452160.0, -50452164.0, -50452176.0, -50452180.0, -50452680.0, -50452740.0, -50454410.0, -50454480.0, -50454508.0, -50454520.0, -50455024.0, -50455588.0, -50455628.0, -50490050.0, -50491200.0, -50493410.0, -50493548.0, -50494176.0, -50494210.0, -50494220.0, -50494228.0, -50494240.0, -50494256.0, -50494800.0, -50495050.0, -50496344.0, -50496932.0, -50496936.0, -50496944.0, -50497400.0, -50497424.0, -50497490.0, -50497964.0, -50498020.0, -50498060.0, -50498480.0, -50498620.0, -50498652.0, -50498724.0, -50499176.0, -50499184.0, -50499256.0, -50500330.0, -50500340.0, -50500372.0, -50500860.0, -50500880.0, -50500924.0, -50501630.0, -50501650.0, -50501692.0, -50502810.0, -50502812.0, -50504540.0, -50504560.0, -50504576.0, -50504588.0, -50504590.0, -50505148.0, -50505172.0, -50506820.0, -50509120.0, -50509140.0, -50509710.0, -50510270.0, -50510436.0, -50510960.0, -50512064.0, -50513270.0, -85571500.0, -85572380.0, -85573420.0, -85573810.0, -85573840.0, -85573900.0, -85574010.0, -85574320.0, -85574350.0, -85574400.0, -85574420.0, -85574920.0, -85574930.0, -85575760.0, -85575860.0, -85576840.0, -85576856.0, -85576860.0, -85576940.0, -85577760.0, -85579256.0, -85580740.0, -85580770.0, -85580820.0, -85581240.0, -85581730.0, -85581760.0, -85581780.0, -85582270.0, -85582296.0, -85583230.0, -85583300.0, -85583304.0, -85583670.0, -85583700.0, -85583740.0, -85584080.0, -85584170.0, -85584180.0, -85584190.0, -85584210.0, -85584240.0, -85584260.0, -85584330.0, -85584670.0, -85584730.0, -85584800.0, -85585930.0, -85586100.0, -85587080.0, -85618950.0, -85619480.0, -85619540.0, -85619544.0, -85619990.0, -85620000.0, -85620080.0, -85620400.0, -85620480.0, -85620500.0, -85620530.0, -85620550.0, -85621390.0, -85621420.0, -85621944.0, -85622000.0, -85622010.0, -85622040.0, -85622410.0, -85622420.0, -85622450.0, -85622456.0, -85622824.0, -85622910.0, -85622960.0, -85623250.0, -85623340.0, -85623360.0, -85623420.0, -85623430.0, -85623900.0, -85623950.0, -85623970.0, -85624310.0, -85624400.0, -85624440.0, -85624480.0, -85624820.0, -85624830.0, -85624856.0, -85624860.0, -85624890.0, -85625370.0, -85625416.0, -85625460.0, -85625800.0, -85625864.0, -85625970.0, -85626310.0, -85626380.0, -85626390.0, -85626410.0, -85628380.0, -85628410.0, -85628740.0, -85628840.0, -85629170.0, -85629256.0, -85629340.0, -85629720.0, -85629780.0, -85630310.0, -85630770.0, -85631180.0, -85631256.0, -85631290.0, -85631304.0, -85631790.0, -85632190.0, -85632230.0, -85632300.0, -85632690.0, -85668510.0, -85669010.0, -85669016.0, -85669420.0, -85669520.0, -85669990.0, -85670020.0, -85670070.0, -85670350.0, -85670410.0, -85670450.0, -85670504.0, -85670580.0, -85670900.0, -85670970.0, -85671920.0, -85672430.0, -85672510.0, -85673464.0, -85673940.0, -85673970.0, -85674390.0, -85674860.0, -85674904.0, -85675830.0, -85676360.0, -85676810.0, -85676850.0, -85676860.0, -85676870.0, -85677210.0, -85677300.0, -85677304.0, -85677336.0, -85677390.0, -85677730.0, -85677800.0, -85677850.0, -85677880.0, -85677920.0, -85678184.0, -85678340.0, -85678350.0, -85678820.0, -85678860.0, -85679140.0, -85679304.0, -85679340.0, -85679780.0, -85679810.0, -85679820.0, -85680740.0, -85680744.0, -85680776.0, -85680780.0, -85681350.0, -85681840.0, -85682170.0, -85682220.0, -85682670.0, -85682730.0, -85682740.0, -85717540.0, -85717976.0, -85718050.0, -85718536.0, -85718980.0, -85718984.0, -85719896.0, -85720936.0, -85720970.0, -85720984.0, -85721016.0, -85721420.0, -85721470.0, -85721496.0, -85721860.0, -85721900.0, -85721940.0, -85721970.0, -85721980.0, -85722350.0, -85722380.0, -85722420.0, -85722530.0, -85722856.0, -85722930.0, -85723010.0, -85723410.0, -85723450.0, -85723460.0, -85723870.0, -85723980.0, -85724340.0, -85724400.0, -85724460.0, -85725340.0, -85725420.0, -85725864.0, -85725896.0, -85726020.0, -85726350.0, -85726776.0, -85727230.0, -85727304.0, -85727310.0, -85727820.0, -85727850.0, -85728300.0, -85728360.0, -85728620.0, -85728810.0, -85729304.0, -85729800.0, -85730280.0, -85730700.0, -85730780.0, -85731200.0, -85731240.0, -85732140.0, -85766040.0, -85766400.0, -85767030.0, -85767510.0, -85767520.0, -85767540.0, -85767544.0, -85767550.0, -85768010.0, -85768100.0, -85768456.0, -85768470.0, -85768536.0, -85768930.0, -85768990.0, -85769010.0, -85769050.0, -85769440.0, -85769580.0, -85769920.0, -85769980.0, -85769990.0, -85770020.0, -85770420.0, -85770490.0, -85770500.0, -85770930.0, -85771040.0, -85771890.0, -85771920.0, -85771980.0, -85772450.0, -85772900.0, -85772930.0, -85772936.0, -85773360.0, -85773380.0, -85773384.0, -85773440.0, -85774360.0, -85774400.0, -85774430.0, -85774790.0, -85774904.0, -85775850.0, -85775890.0, -85775940.0, -85775950.0, -85776290.0, -85776320.0, -85776360.0, -85776800.0, -85776850.0, -85776856.0, -85777290.0, -85777310.0, -85777320.0, -85777370.0, -85778320.0, -85778330.0, -85778820.0, -85778824.0, -85780740.0, -85780820.0, -85815416.0, -85815570.0, -85816520.0, -85816530.0, -85816940.0, -85816980.0, -85817064.0, -85817090.0, -85817450.0, -85817464.0, -85817520.0, -85817560.0, -85817570.0, -85817576.0, -85817940.0, -85817944.0, -85817980.0, -85818024.0, -85818060.0, -85818550.0, -85819470.0, -85819480.0, -85820020.0, -85820380.0, -85820400.0, -85820410.0, -85820430.0, -85820504.0, -85820520.0, -85820540.0, -85820900.0, -85820990.0, -85821000.0, -85821030.0, -85821430.0, -85821450.0, -85821460.0, -85821496.0, -85821940.0, -85821970.0, -85822420.0, -85822936.0, -85823450.0, -85823880.0, -85824020.0, -85824740.0, -85824760.0, -85824900.0, -85824940.0, -85825330.0, -85825830.0, -85825860.0, -85825870.0, -85826410.0, -85826856.0, -85826900.0, -85827280.0, -85827300.0, -85827310.0, -85827330.0, -85827370.0, -85828900.0, -85829230.0, -85829770.0, -85829820.0, -85865020.0, -85865110.0, -85866090.0, -85866440.0, -85867560.0, -85868040.0, -85868470.0, -85868500.0, -85868520.0, -85868580.0, -85868940.0, -85868990.0, -85869420.0, -85869990.0, -85870456.0, -85870880.0, -85871020.0, -85871464.0, -85871870.0, -85872460.0, -85872890.0, -85872960.0, -85873900.0, -85873920.0, -85873960.0, -85874370.0, -85874380.0, -85874400.0, -85874420.0, -85874470.0, -85874880.0, -85874900.0, -85874950.0, -85875304.0, -85875310.0, -85875360.0, -85875470.0, -85875736.0, -85876350.0, -85876370.0, -85876830.0, -85876860.0, -85877300.0, -85877336.0, -85877350.0, -85877370.0, -85877900.0, -85878200.0, -85878320.0, -85878360.0, -85878750.0, -85913630.0, -85914130.0, -85914160.0, -85914580.0, -85916000.0, -85916620.0, -85916936.0, -85917020.0, -85917040.0, -85917064.0, -85917080.0, -85917530.0, -85917540.0, -85917544.0, -85917570.0, -85918020.0, -85920010.0, -85920530.0, -85920880.0, -85921016.0, -85921440.0, -85921940.0, -85921990.0, -85922000.0, -85922380.0, -85922424.0, -85922990.0, -85923256.0, -85923384.0, -85923410.0, -85923900.0, -85924030.0, -85924310.0, -85924424.0, -85924440.0, -85924984.0, -85925320.0, -85925420.0, -85925840.0, -85925950.0, -85926300.0, -85926820.0, -85927350.0, -85927360.0, -85927390.0, -85927420.0, -85927790.0, -85927870.0, -85962640.0, -85962670.0, -85963650.0, -85964584.0, -85964630.0, -85965060.0, -85965090.0, -85965130.0, -85965560.0, -85965590.0, -85966070.0, -85966450.0, -85967030.0, -85967050.0, -85967496.0, -85967500.0, -85968000.0, -85969944.0, -85969976.0, -85970450.0, -85970456.0, -85970460.0, -85971020.0, -85971416.0, -85971540.0, -85972000.0, -85972350.0, -85972456.0, -85972860.0, -85972890.0, -85973480.0, -85973900.0, -85973970.0, -85974370.0, -85974376.0, -85974380.0, -85974400.0, -85974410.0, -85974420.0, -85974440.0, -85974800.0, -85974920.0, -85974930.0, -85974940.0, -85974960.0, -85974980.0, -85975224.0, -85975340.0, -85975450.0, -85975910.0, -85976340.0, -85976360.0, -86011224.0, -86011510.0, -86011600.0, -86012210.0, -86012580.0, -86012590.0, -86013190.0, -86013590.0, -86013630.0, -86013700.0, -86014010.0, -86014020.0, -86015096.0, -86015140.0, -86015460.0, -86015570.0, -86017570.0, -86017900.0, -86018000.0, -86018024.0, -86018060.0, -86018456.0, -86018510.0, -86019100.0, -86019410.0, -86019500.0, -86020010.0, -86020070.0, -86021460.0, -86021464.0, -86021490.0, -86021890.0, -86021920.0, -86021950.0, -86021960.0, -86021990.0, -86022470.0, -86022490.0, -86022880.0, -86022910.0, -86022950.0, -86022990.0, -86023010.0, -86023920.0, -86023980.0, -86024296.0, -86024376.0, -86024380.0, -86024470.0, -86024720.0, -86024900.0, -86024904.0, -86024930.0, -86025010.0, -86025020.0, -86025180.0, -86060280.0, -86060650.0, -86060660.0, -86060670.0, -86060730.0, -86061080.0, -86061096.0, -86061120.0, -86061130.0, -86061176.0, -86061210.0, -86061624.0, -86061630.0, -86062120.0, -86062136.0, -86062150.0, -86062160.0, -86064024.0, -86064060.0, -86065544.0, -86065570.0, -86065600.0, -86066050.0, -86066530.0, -86066580.0, -86066610.0, -86067520.0, -86067590.0, -86068070.0, -86068520.0, -86068536.0, -86068930.0, -86069020.0, -86069410.0, -86069900.0, -86070400.0, -86070510.0, -86071390.0, -86071400.0, -86071410.0, -86071840.0, -86072420.0, -86072430.0, -86072440.0, -86072824.0, -86072856.0, -86072920.0, -86073410.0, -86074380.0, -86109710.0, -86110250.0, -86110710.0, -86110720.0, -86110790.0, -86111150.0, -86111250.0, -86111656.0, -86111690.0, -86111784.0, -86112136.0, -86112150.0, -86112280.0, -86112560.0, -86112730.0, -86114216.0, -86114560.0, -86115096.0, -86115110.0, -86115120.0, -86115540.0, -86115560.0, -86115570.0, -86115976.0, -86116050.0, -86116080.0, -86116550.0, -86116580.0, -86117030.0, -86117490.0, -86118504.0, -86118580.0, -86120440.0, -86120456.0, -86120530.0, -86121416.0, -86121940.0, -86123896.0, -86123980.0, -86124376.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n",
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8210227272727273\n",
      "Hamming Loss: 0.08948863636363637\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       578\n",
      "           1       0.18      1.00      0.30       126\n",
      "\n",
      "    accuracy                           0.18       704\n",
      "   macro avg       0.09      0.50      0.15       704\n",
      "weighted avg       0.03      0.18      0.05       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 273us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 86us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 168us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 106us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 113us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 113us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 6.1066 - binary_accuracy: 0.6017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [100075176.0, 100075336.0, 100076140.0, 100080220.0, 100080230.0, 100080330.0, 100080620.0, 100080660.0, 100080700.0, 100080840.0, 100081016.0, 100081040.0, 100081064.0, 100081130.0, 100081540.0, 100082210.0, 100082260.0, 100082700.0, 100082710.0, 100082930.0, 100083410.0, 100083550.0, 100083700.0, 100083760.0, 100083784.0, 100084610.0, 100084616.0, 100084620.0, 100084690.0, 100084830.0, 100084860.0, 100085030.0, 100085750.0, 100086070.0, 100086140.0, 100086320.0, 100086430.0, 100086584.0, 100094750.0, 100115784.0, 100118744.0, 100119390.0, 100119464.0, 100119490.0, 100119860.0, 100119890.0, 100120120.0, 100120130.0, 100120190.0, 100120370.0, 100120780.0, 100121290.0, 100121320.0, 100121530.0, 100121610.0, 100121650.0, 100122110.0, 100122130.0, 100122340.0, 100122450.0, 100122470.0, 100122510.0, 100122530.0, 100123530.0, 100123544.0, 100123600.0, 100123990.0, 100124424.0, 100124450.0, 100124540.0, 100124610.0, 100124780.0, 100124830.0, 100125140.0, 100125150.0, 100125170.0, 100125224.0, 100125260.0, 100125370.0, 100125540.0, 100125630.0, 100125650.0, 100125860.0, 100126020.0, 100126170.0, 100126230.0, 100126420.0, 100126430.0, 100126450.0, 100126800.0, 100126824.0, 100126850.0, 100126856.0, 100126990.0, 100127020.0, 100127630.0, 100127730.0, 100127870.0, 100127920.0, 100127950.0, 100128040.0, 100128220.0, 100128300.0, 100128450.0, 100128820.0, 100130050.0, 100130420.0, 100130430.0, 100131980.0, 100133280.0, 100133380.0, 100158744.0, 100158880.0, 100158920.0, 100159070.0, 100159280.0, 100159360.0, 100159460.0, 100159660.0, 100159820.0, 100160050.0, 100160060.0, 100160090.0, 100160100.0, 100160344.0, 100160390.0, 100160664.0, 100160810.0, 100161000.0, 100161040.0, 100161140.0, 100161180.0, 100161550.0, 100161600.0, 100161790.0, 100161900.0, 100162310.0, 100162390.0, 100162740.0, 100162800.0, 100162880.0, 100164104.0, 100164584.0, 100167144.0, 100167500.0, 100167810.0, 100168620.0, 100168950.0, 100169700.0, 100170490.0, 100170900.0, 100171230.0, 100172370.0, 100191010.0, 100195070.0, 100195120.0, 100195160.0, 100195650.0, 100195770.0, 100195790.0, 100195850.0, 100195870.0, 100195960.0, 100196030.0, 100196040.0, 100196056.0, 100196250.0, 100196500.0, 100196616.0, 100196696.0, 100196900.0, 100196920.0, 100196940.0, 100196960.0, 100197140.0, 100197310.0, 100197680.0, 100197780.0, 100197840.0, 100197850.0, 100197870.0, 100197940.0, 100198240.0, 100198660.0, 100198740.0, 100198750.0, 100198760.0, 100198830.0, 100199140.0, 100199150.0, 100199360.0, 100199410.0, 100199700.0, 100199850.0, 100200080.0, 100200780.0, 100200880.0, 100200900.0, 100201320.0, 100201660.0, 100201980.0, 100202010.0, 100202040.0, 100202744.0, 100203100.0, 100203940.0, 100203970.0, 100204630.0, 100205710.0, 100205736.0, 100206160.0, 100206504.0, 100206880.0, 100231490.0, 100231550.0, 100232140.0, 100232170.0, 100232510.0, 100232620.0, 100233390.0, 100233630.0, 100233920.0, 100233970.0, 100234580.0, 100234610.0, 100235000.0, 100235170.0, 100235864.0, 100236090.0, 100236110.0, 100236190.0, 100236296.0, 100236350.0, 100236560.0, 100236590.0, 100236620.0, 100236770.0, 100236904.0, 100237500.0, 100237620.0, 100237670.0, 100238340.0, 100238380.0, 100238490.0, 100238590.0, 100238630.0, 100239120.0, 100239410.0, 100239420.0, 100239440.0, 100239460.0, 100239500.0, 100239510.0, 100239570.0, 100240240.0, 100240990.0, 100242090.0, 100243200.0, 100243210.0, 100245730.0, 100245780.0, 100245840.0, 100269250.0, 100270080.0, 100270160.0, 100270190.0, 100270670.0, 100270840.0, 100270850.0, 100270960.0, 100271090.0, 100271110.0, 100271290.0, 100271330.0, 100271510.0, 100271760.0, 100271930.0, 100272140.0, 100272210.0, 100272260.0, 100272616.0, 100272660.0, 100272696.0, 100272870.0, 100273170.0, 100273180.0, 100273230.0, 100273280.0, 100273300.0, 100273340.0, 100273400.0, 100273630.0, 100273790.0, 100274370.0, 100274504.0, 100275140.0, 100275470.0, 100275640.0, 100275900.0, 100276170.0, 100276180.0, 100276264.0, 100276510.0, 100276630.0, 100276880.0, 100277336.0, 100277600.0, 100279550.0, 100281060.0, 100282520.0, 100282536.0, 100309070.0, 100309210.0, 100309700.0, 100310020.0, 100310260.0, 100310340.0, 100310470.0, 100310616.0, 100310820.0, 100311470.0, 100311680.0, 100311704.0, 100311870.0, 100312110.0, 100312136.0, 100312800.0, 100313160.0, 100313540.0, 100313660.0, 100313736.0, 100313890.0, 100313930.0, 100314210.0, 100314360.0, 100314424.0, 100314780.0, 100314790.0, 100314960.0, 100315850.0, 100315890.0, 100316530.0, 100317210.0, 100317230.0, 100317620.0, 100318330.0, 100319110.0, 100319190.0, 100319200.0, 100319440.0, 100319620.0, 100319860.0, 100320376.0, 100320910.0, 100321630.0, 100321760.0, 100341660.0, 100343960.0, 100344120.0, 100344430.0, 100344450.0, 100344460.0, 100344530.0, 100344610.0, 100344820.0, 100345010.0, 100345080.0, 100345170.0, 100345490.0, 100345570.0, 100345630.0, 100345780.0, 100346080.0, 100346424.0, 100346620.0, 100346770.0, 100346860.0, 100346936.0, 100347200.0, 100347380.0, 100347420.0, 100347816.0, 100348220.0, 100348440.0, 100348530.0, 100348560.0, 100348600.0, 100348780.0, 100349010.0, 100349304.0, 100349450.0, 100349800.0, 100349870.0, 100349880.0, 100349920.0, 100350220.0, 100350280.0, 100350290.0, 100350310.0, 100350320.0, 100350650.0, 100350740.0, 100350990.0, 100351020.0, 100351220.0, 100351630.0, 100351656.0, 100351784.0, 100351920.0, 100352100.0, 100352240.0, 100352460.0, 100352500.0, 100352510.0, 100352600.0, 100352610.0, 100352880.0, 100353200.0, 100353500.0, 100353510.0, 100353540.0, 100353560.0, 100354370.0, 100355430.0, 100355450.0, 100355800.0, 100355830.0, 100355896.0, 100357100.0, 100357650.0, 100357940.0, 100358024.0, 100358420.0, 100358450.0, 100358780.0, 100381080.0, 100381550.0, 100381624.0, 100382600.0, 100382616.0, 100382670.0, 100383420.0, 100383860.0, 100385080.0, 100385090.0, 100385140.0, 100385256.0, 100385690.0, 100385960.0, 100386220.0, 100386260.0, 100386340.0, 100386660.0, 100386960.0, 100386980.0, 100386990.0, 100387020.0, 100387190.0, 100387420.0, 100387790.0, 100387864.0, 100387940.0, 100388110.0, 100388140.0, 100388280.0, 100388420.0, 100388500.0, 100388560.0, 100388800.0, 100388824.0, 100388830.0, 100388850.0, 100388860.0, 100389224.0, 100389360.0, 100390200.0, 100390290.0, 100390300.0, 100390344.0, 100390390.0, 100391070.0, 100391304.0, 100391464.0, 100391630.0, 100392136.0, 100392190.0, 100393250.0, 100393336.0, 100393680.0, 100393736.0, 100393850.0, 100394056.0, 100394440.0, 100394460.0, 100394810.0, 100394830.0, 100394860.0, 100395550.0, 100395864.0, 100413460.0, 100418960.0, 100419000.0, 100419020.0, 100419660.0, 100420100.0, 100420740.0, 100421180.0, 100421250.0, 100421900.0, 100421940.0, 100421970.0, 100421980.0, 100422010.0, 100422620.0, 100423120.0, 100423140.0, 100423490.0, 100423816.0, 100424020.0, 100424050.0, 100424110.0, 100424190.0, 100424240.0, 100424500.0, 100424504.0, 100424540.0, 100424580.0, 100424830.0, 100424910.0, 100424960.0, 100425200.0, 100425250.0, 100425260.0, 100425310.0, 100425320.0, 100425650.0, 100425730.0, 100425870.0, 100426450.0, 100426700.0, 100426720.0, 100426750.0, 100427060.0, 100427070.0, 100427120.0, 100427140.0, 100427200.0, 100427220.0, 100427820.0, 100428000.0, 100428520.0, 100428720.0, 100428740.0, 100429240.0, 100429260.0, 100430050.0, 100430090.0, 100430610.0, 100430800.0, 100430820.0, 100430856.0, 100431176.0, 100431260.0, 100431790.0, 100432320.0, 100454300.0, 100454904.0, 100455624.0, 100456160.0, 100456450.0, 100456510.0, 100456880.0, 100457910.0, 100457920.0, 100457970.0, 100458400.0, 100458690.0, 100458696.0, 100458700.0, 100458760.0, 100459340.0, 100459420.0, 100460030.0, 100460150.0, 100460260.0, 100460530.0, 100460890.0, 100461330.0, 100461410.0, 100461630.0, 100462460.0, 100462690.0, 100462744.0, 100462984.0, 100463020.0, 100463060.0, 100463070.0, 100463080.0, 100463090.0, 100463260.0, 100463460.0, 100464340.0, 100464700.0, 100465420.0, 100465600.0, 100465650.0, 100465680.0, 100465690.0, 100465710.0, 100466050.0, 100466350.0, 100466370.0, 100466740.0, 100466790.0, 100467600.0, 100468240.0, 100468590.0, 100468960.0, 100469010.0, 100469340.0, 100469380.0, 100469464.0, 100469624.0, 100469700.0, 100469730.0, 100469760.0, 100489120.0, 100492376.0, 100492380.0, 100492824.0, 100493070.0, 100493130.0, 100493176.0, 100493544.0, 100493550.0, 100493950.0, 100494340.0, 100494430.0, 100494710.0, 100495380.0, 100495750.0, 100496110.0, 100496160.0, 100496184.0, 100496290.0, 100496420.0, 100496830.0, 100497270.0, 100497280.0, 100497520.0, 100497580.0, 100497704.0, 100497930.0, 100497976.0, 100498000.0, 100498050.0, 100498160.0, 100498210.0, 100498260.0, 100498420.0, 100498456.0, 100498720.0, 100499360.0, 100499496.0, 100499770.0, 100499780.0, 100499920.0, 100500190.0, 100500200.0, 100500560.0, 100502830.0, 100503180.0, 100503500.0, 100504216.0, 100504290.0, 100504310.0, 100505440.0, 100505784.0, 100506160.0, 100506710.0, 100506820.0, 65652260.0, 65652970.0, 65653560.0, 65653600.0, 65653750.0, 65654830.0, 65655308.0, 65655580.0, 65655700.0, 65656320.0, 65657268.0, 65657280.0, 65657364.0, 65658464.0, 65658696.0, 65658950.0, 65659090.0, 65659460.0, 65660428.0, 65660436.0, 65660500.0, 65660920.0, 65661372.0, 65661610.0, 65662616.0, 65662764.0, 65663132.0, 65663148.0, 65663200.0, 65663316.0, 65663570.0, 65663810.0, 65663816.0, 65663860.0, 65663910.0, 65664050.0, 65664176.0, 65664250.0, 65664452.0, 65664572.0, 65664628.0, 65664748.0, 65664868.0, 65665344.0, 65665764.0, 65665850.0, 65665950.0, 65665984.0, 65666410.0, 65666424.0, 65666500.0, 65666984.0, 65667040.0, 65667084.0, 65668108.0, 65668588.0, 65706920.0, 65707036.0, 65707084.0, 65707140.0, 65707496.0, 65707504.0, 65707572.0, 65708030.0, 65708096.0, 65708204.0, 65708760.0, 65709124.0, 65710412.0, 65711316.0, 65711344.0, 65711348.0, 65711760.0, 65711868.0, 65711904.0, 65712268.0, 65712304.0, 65712740.0, 65712790.0, 65712850.0, 65712930.0, 65712970.0, 65712988.0, 65713336.0, 65714380.0, 65714508.0, 65715170.0, 65715410.0, 65715484.0, 65715604.0, 65716024.0, 65716530.0, 65716572.0, 65716576.0, 65716650.0, 65716660.0, 65716676.0, 65716710.0, 65717136.0, 65717144.0, 65717436.0, 65717560.0, 65718092.0, 65718650.0, 65718692.0, 65718772.0, 65718796.0, 65719692.0, 65719870.0, 65720696.0, 65720820.0, 65720844.0, 65720876.0, 65720940.0, 65720964.0, 65721348.0, 65721976.0, 65759204.0, 65759668.0, 65759720.0, 65760110.0, 65760732.0, 65760790.0, 65761330.0, 65761356.0, 65761410.0, 65761796.0, 65761824.0, 65762040.0, 65762230.0, 65762320.0, 65762376.0, 65762404.0, 65762496.0, 65762840.0, 65762844.0, 65762910.0, 65763090.0, 65763144.0, 65763376.0, 65763400.0, 65763440.0, 65763480.0, 65763896.0, 65763920.0, 65763984.0, 65764070.0, 65764080.0, 65764090.0, 65764470.0, 65765080.0, 65765244.0, 65765284.0, 65766124.0, 65766176.0, 65766220.0, 65767016.0, 65767040.0, 65767124.0, 65767148.0, 65767216.0, 65767308.0, 65768188.0, 65768260.0, 65768388.0, 65768436.0, 65768820.0, 65768970.0, 65768984.0, 65769784.0, 65770356.0, 65770430.0, 65770960.0, 65771092.0, 65771416.0, 65771436.0, 65771500.0, 65771544.0, 65771624.0, 65772060.0, 65772136.0, 65772652.0, 65774812.0, 65774844.0, 65813190.0, 65813676.0, 65813844.0, 65814350.0, 65814784.0, 65814824.0, 65814850.0, 65815130.0, 65815360.0, 65815372.0, 65815700.0, 65815936.0, 65816172.0, 65816212.0, 65816296.0, 65816400.0, 65816864.0, 65816904.0, 65817308.0, 65817936.0, 65818410.0, 65818584.0, 65818652.0, 65818830.0, 65819428.0, 65819900.0, 65820070.0, 65820580.0, 65821156.0, 65821224.0, 65821228.0, 65821276.0, 65821320.0, 65822176.0, 65822750.0, 65822780.0, 65823148.0, 65823240.0, 65823324.0, 65823344.0, 65823356.0, 65823376.0, 65823550.0, 65823668.0, 65823724.0, 65823784.0, 65823836.0, 65823956.0, 65824330.0, 65824436.0, 65825280.0, 65825296.0, 65825800.0, 65825980.0, 65826020.0, 65826024.0, 65826084.0, 65826424.0, 65826980.0, 65827070.0, 65828580.0, 65828652.0, 65828716.0, 65828720.0, 65863976.0, 65864944.0, 65865116.0, 65866536.0, 65867016.0, 65867060.0, 65867084.0, 65867340.0, 65867440.0, 65867660.0, 65868092.0, 65868108.0, 65868364.0, 65869480.0, 65870108.0, 65870130.0, 65870740.0, 65870784.0, 65870812.0, 65871268.0, 65871320.0, 65871530.0, 65871636.0, 65872020.0, 65872150.0, 65872224.0, 65872656.0, 65872790.0, 65872896.0, 65873148.0, 65873440.0, 65873456.0, 65873524.0, 65873852.0, 65873916.0, 65873960.0, 65874012.0, 65874270.0, 65874400.0, 65874496.0, 65874744.0, 65874770.0, 65875076.0, 65875270.0, 65875420.0, 65875436.0, 65875564.0, 65875616.0, 65875628.0, 65876172.0, 65876468.0, 65876668.0, 65877016.0, 65877056.0, 65877424.0, 65877572.0, 65877610.0, 65877650.0, 65877972.0, 65878276.0, 65878464.0, 65878550.0, 65878616.0, 65878624.0, 65878692.0, 65878740.0, 65878800.0, 65878870.0, 65879184.0, 65879212.0, 65879220.0, 65879300.0, 65879372.0, 65879412.0, 65879420.0, 65879532.0, 65879844.0, 65880220.0, 65880376.0, 65880796.0, 65919120.0, 65919564.0, 65919700.0, 65919804.0, 65919820.0, 65919840.0, 65920732.0, 65920820.0, 65920940.0, 65921196.0, 65921520.0, 65921536.0, 65921836.0, 65921840.0, 65921856.0, 65922440.0, 65922892.0, 65922972.0, 65923016.0, 65923030.0, 65923588.0, 65923804.0, 65923988.0, 65924300.0, 65924736.0, 65925004.0, 65925068.0, 65925356.0, 65925492.0, 65925516.0, 65925660.0, 65925700.0, 65925772.0, 65926092.0, 65926220.0, 65926250.0, 65927156.0, 65927304.0, 65927840.0, 65929136.0, 65929160.0, 65931104.0, 65931170.0, 65932204.0, 65934330.0, 65934348.0, 65934810.0, 65971628.0, 65974308.0, 65974364.0, 65974676.0, 65975160.0, 65976116.0, 65976250.0, 65976336.0, 65976788.0, 65977036.0, 65977570.0, 65977740.0, 65977796.0, 65977816.0, 65977844.0, 65978540.0, 65978676.0, 65978810.0, 65978830.0, 65979344.0, 65979740.0, 65980444.0, 65980990.0, 65981020.0, 65981028.0, 65981040.0, 65981070.0, 65981076.0, 65981092.0, 65981476.0, 65982108.0, 65982440.0, 65982464.0, 65982696.0, 65983084.0, 65983340.0, 65983364.0, 65983520.0, 65983612.0, 65983692.0, 65983710.0, 65983730.0, 65984124.0, 65984136.0, 65984200.0, 65984800.0, 65984904.0, 65985200.0, 65985390.0, 65985930.0, 65986040.0, 65986970.0, 66020344.0, 66020652.0, 66026940.0, 66027120.0, 66027156.0, 66027780.0, 66029036.0, 66029500.0, 66030148.0, 66030524.0, 66030628.0, 66030710.0, 66030772.0, 66030788.0, 66031020.0, 66032144.0, 66032170.0, 66032424.0, 66032636.0, 66033096.0, 66033250.0, 66033652.0, 66033810.0, 66033950.0, 66034228.0, 66034336.0, 66034370.0, 66034404.0, 66034840.0, 66034870.0, 66035064.0, 66035176.0, 66035856.0, 66035950.0, 66036140.0, 66036144.0, 66036160.0, 66036188.0, 66036204.0, 66036324.0, 66036450.0, 66036496.0, 66036564.0, 66037020.0, 66037060.0, 66037444.0, 66037776.0, 66037804.0, 66039852.0, 66040250.0, 66040976.0, 66081724.0, 66083030.0, 66083412.0, 66083456.0, 66083588.0, 66084236.0, 66084380.0, 66084476.0, 66084540.0, 66084692.0, 66084772.0, 66084970.0, 66085080.0, 66085092.0, 66086052.0, 66086300.0, 66086400.0, 66086588.0, 66086644.0, 66086776.0, 66087092.0, 66087144.0, 66087260.0, 66087344.0, 66087520.0, 66087604.0, 66087624.0, 66087692.0, 66088108.0, 66088184.0, 66088290.0, 66088576.0, 66088730.0, 66088740.0, 66088948.0, 66089160.0, 66089216.0, 66089260.0, 66089344.0, 66089348.0, 66089404.0, 66089424.0, 66090092.0, 66090120.0, 66090252.0, 66090270.0, 66090364.0, 66090584.0, 66090824.0, 66090916.0, 66091470.0, 66091564.0, 66091604.0, 66091620.0, 66091636.0, 66091690.0, 66091900.0, 66091910.0, 66091964.0, 66091990.0, 66092530.0, 66092564.0, 66093810.0, 66094820.0, 66094908.0, 66132676.0, 66134240.0, 66134748.0, 66135310.0, 66136416.0, 66136656.0, 66136736.0, 66137204.0, 66137276.0, 66137300.0, 66137404.0, 66137852.0, 66138060.0, 66138104.0, 66139040.0, 66139136.0, 66139148.0, 66139564.0, 66139760.0, 66140172.0, 66140180.0, 66140196.0, 66140950.0, 66141196.0, 66141508.0, 66141532.0, 66142090.0, 66142144.0, 66142576.0, 66142644.0, 66143176.0, 66143580.0, 66143690.0, 66143700.0, 66143730.0, 66144244.0, 66144510.0, 66144740.0, 66144804.0, 66144852.0, 66145316.0, 66145364.0, 66145430.0, 66146468.0, 66179308.0, 66186060.0, 66186324.0, 66186756.0, 66188464.0, 66188490.0, 66188636.0, 66188740.0, 66188972.0, 66189150.0, 66190696.0, 66191436.0, 66191460.0, 66191540.0, 66191612.0, 66192764.0, 66192780.0, 66192896.0, 66192964.0, 66192972.0, 66193000.0, 66193556.0, 66193660.0, 66193676.0, 66194308.0, 66194550.0, 66195520.0, 66195864.0, 66195884.0, 66195976.0, 66196280.0, 66196410.0, 66197016.0, 66197044.0, 66197404.0, 66197710.0, 66197904.0, 66199068.0, 66199116.0, 66199270.0, 66199470.0, 66199576.0, 66199590.0, 66199620.0, 66199924.0, 66200064.0, 66200250.0, 66200420.0, 66200692.0, 66200844.0, 66200896.0, 66200996.0, 66201196.0, 66201220.0, 66201308.0, 66201344.0, 66201420.0, 66201536.0, 66201620.0, 66201644.0, 66201756.0, 66202024.0, 66202080.0, 66202764.0, 66202820.0, 66202860.0, 66203852.0, 66203890.0, 66203950.0, 66204980.0, 66205020.0, 66256610.0, 66260090.0, 66260388.0, 66261110.0, 66261176.0, 66261536.0, 66261864.0, 66264140.0, 66264304.0, 66265304.0, 66265320.0, 66265360.0, 66265450.0, 66266456.0, 66266868.0, 66267110.0, 66267120.0, 66268210.0, 66269216.0, 66269220.0, 66270570.0, 66270644.0, 66270900.0, 66271536.0, 66272830.0, 66273640.0, 66273650.0, 66273680.0, 66273764.0, 66274320.0, 66274350.0, 66274404.0, 66274790.0, 66274950.0, 66275120.0, 66275148.0, 66276240.0, 66276628.0, 66276630.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7684659090909091\n",
      "Hamming Loss: 0.125\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91       584\n",
      "           1       0.00      0.00      0.00       120\n",
      "\n",
      "    accuracy                           0.83       704\n",
      "   macro avg       0.41      0.50      0.45       704\n",
      "weighted avg       0.69      0.83      0.75       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 278us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 87us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 97us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 97us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - ETA: 0s - loss: 9.3081 - binary_accuracy: 0.396 - 0s 105us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-26119120.0, -26119688.0, -26119720.0, -26119792.0, -26119928.0, -26120096.0, -26120200.0, -26120260.0, -26120540.0, -26120712.0, -26120816.0, -26120872.0, -26121020.0, -26121880.0, -26122320.0, -26122368.0, -26122400.0, -26122636.0, -26122728.0, -26122880.0, -26123008.0, -26123900.0, -26124928.0, -26125160.0, -26125376.0, -26127312.0, -26129912.0, -26130204.0, -26130708.0, -26131328.0, -26131612.0, -26132224.0, -26132240.0, -26132360.0, -26132464.0, -26161660.0, -26162544.0, -26162596.0, -26162740.0, -26162756.0, -26162864.0, -26162900.0, -26162996.0, -26164228.0, -26164240.0, -26164252.0, -26164516.0, -26164596.0, -26164640.0, -26164664.0, -26164820.0, -26164872.0, -26164968.0, -26165016.0, -26165176.0, -26165240.0, -26165480.0, -26165576.0, -26165664.0, -26166128.0, -26166256.0, -26166280.0, -26167020.0, -26167116.0, -26167356.0, -26167456.0, -26167496.0, -26167596.0, -26167924.0, -26168312.0, -26168404.0, -26168528.0, -26168664.0, -26168708.0, -26168780.0, -26168792.0, -26169592.0, -26169728.0, -26169928.0, -26169952.0, -26170080.0, -26170344.0, -26170392.0, -26170576.0, -26170584.0, -26170816.0, -26170924.0, -26171160.0, -26171180.0, -26171280.0, -26171344.0, -26172064.0, -26172208.0, -26172296.0, -26172396.0, -26172472.0, -26172520.0, -26172568.0, -26172600.0, -26172648.0, -26172680.0, -26172704.0, -26172768.0, -26172816.0, -26173076.0, -26173124.0, -26173632.0, -26173696.0, -26203468.0, -26204240.0, -26204704.0, -26204712.0, -26205196.0, -26205440.0, -26205736.0, -26205952.0, -26206048.0, -26206168.0, -26206336.0, -26206372.0, -26206584.0, -26206800.0, -26206960.0, -26207176.0, -26207376.0, -26207388.0, -26207456.0, -26207680.0, -26207788.0, -26208232.0, -26208392.0, -26208440.0, -26209096.0, -26209112.0, -26209516.0, -26209588.0, -26209632.0, -26209720.0, -26209848.0, -26209944.0, -26210016.0, -26210128.0, -26210744.0, -26210948.0, -26210960.0, -26210996.0, -26211052.0, -26211104.0, -26211240.0, -26211308.0, -26211536.0, -26211696.0, -26211704.0, -26211840.0, -26211904.0, -26211920.0, -26212116.0, -26212248.0, -26212488.0, -26212816.0, -26212832.0, -26213052.0, -26214284.0, -26214396.0, -26214940.0, -26215024.0, -26215068.0, -26215392.0, -26215420.0, -26215460.0, -26215476.0, -26215548.0, -26215600.0, -26215776.0, -26216296.0, -26216944.0, -26246708.0, -26246892.0, -26247112.0, -26247240.0, -26247464.0, -26247556.0, -26247688.0, -26248000.0, -26248088.0, -26248168.0, -26248296.0, -26248352.0, -26248576.0, -26248840.0, -26249240.0, -26249820.0, -26250040.0, -26250048.0, -26250056.0, -26250128.0, -26250228.0, -26250272.0, -26250344.0, -26250380.0, -26250696.0, -26250832.0, -26250896.0, -26250928.0, -26250996.0, -26251080.0, -26251196.0, -26251248.0, -26251340.0, -26251448.0, -26251544.0, -26252096.0, -26252136.0, -26252184.0, -26252228.0, -26252668.0, -26252808.0, -26253024.0, -26253080.0, -26253176.0, -26253200.0, -26253632.0, -26253852.0, -26254132.0, -26254140.0, -26254240.0, -26254376.0, -26254412.0, -26254680.0, -26254784.0, -26254904.0, -26255016.0, -26255056.0, -26255472.0, -26255524.0, -26255944.0, -26256336.0, -26256520.0, -26256832.0, -26257080.0, -26257168.0, -26257184.0, -26257368.0, -26257588.0, -26258080.0, -26258208.0, -26287712.0, -26287852.0, -26288088.0, -26288728.0, -26288760.0, -26288776.0, -26288872.0, -26290336.0, -26290512.0, -26290800.0, -26290856.0, -26291536.0, -26291924.0, -26292032.0, -26292320.0, -26292408.0, -26292460.0, -26292864.0, -26293272.0, -26293688.0, -26293784.0, -26293856.0, -26294104.0, -26294272.0, -26294360.0, -26294520.0, -26294568.0, -26294896.0, -26294968.0, -26295152.0, -26295264.0, -26295368.0, -26295648.0, -26295780.0, -26295828.0, -26295864.0, -26296108.0, -26297136.0, -26297144.0, -26297188.0, -26297224.0, -26297352.0, -26297412.0, -26297648.0, -26297668.0, -26297672.0, -26297720.0, -26297800.0, -26297992.0, -26298072.0, -26298152.0, -26298592.0, -26298848.0, -26299016.0, -26299088.0, -26299104.0, -26299664.0, -26300216.0, -26300404.0, -26300432.0, -26301000.0, -26301056.0, -26301228.0, -26301416.0, -26301584.0, -26301700.0, -26301904.0, -26330208.0, -26330252.0, -26330960.0, -26331044.0, -26331224.0, -26331284.0, -26331316.0, -26331580.0, -26332180.0, -26332232.0, -26332560.0, -26332688.0, -26332720.0, -26332760.0, -26332992.0, -26333160.0, -26333408.0, -26333936.0, -26334128.0, -26334176.0, -26334776.0, -26335216.0, -26335368.0, -26335588.0, -26335760.0, -26335772.0, -26336056.0, -26336344.0, -26336508.0, -26336512.0, -26336556.0, -26336676.0, -26336728.0, -26336856.0, -26336880.0, -26336960.0, -26337108.0, -26337136.0, -26337256.0, -26337784.0, -26337888.0, -26338096.0, -26338236.0, -26338312.0, -26338540.0, -26338696.0, -26338800.0, -26338880.0, -26339204.0, -26339272.0, -26339296.0, -26339312.0, -26339368.0, -26339492.0, -26339560.0, -26339568.0, -26339840.0, -26340016.0, -26340072.0, -26340464.0, -26341268.0, -26341320.0, -26341372.0, -26341616.0, -26341632.0, -26341680.0, -26341952.0, -26342212.0, -26342500.0, -26342840.0, -26343340.0, -26343504.0, -26372960.0, -26373144.0, -26373348.0, -26373624.0, -26373984.0, -26374088.0, -26374132.0, -26374968.0, -26375024.0, -26375180.0, -26377024.0, -26378244.0, -26378408.0, -26378696.0, -26379148.0, -26379748.0, -26380136.0, -26380200.0, -26380376.0, -26380400.0, -26380424.0, -26380564.0, -26380588.0, -26380608.0, -26380688.0, -26380824.0, -26381024.0, -26381028.0, -26381104.0, -26381384.0, -26382160.0, -26382264.0, -26382488.0, -26382616.0, -26382632.0, -26382920.0, -26382992.0, -26383024.0, -26383040.0, -26383168.0, -26383492.0, -26383636.0, -26383744.0, -26383800.0, -26383944.0, -26383960.0, -26384120.0, -26385136.0, -26385440.0, -26385616.0, -26385784.0, -26385984.0, -26415480.0, -26415624.0, -26415676.0, -26415936.0, -26416064.0, -26416156.0, -26416324.0, -26416336.0, -26416368.0, -26416504.0, -26416676.0, -26416888.0, -26417352.0, -26417852.0, -26418116.0, -26418256.0, -26418280.0, -26418532.0, -26418612.0, -26418728.0, -26418804.0, -26418988.0, -26418996.0, -26419052.0, -26419068.0, -26419212.0, -26419568.0, -26419872.0, -26420344.0, -26420588.0, -26420756.0, -26422360.0, -26422432.0, -26422472.0, -26422624.0, -26422748.0, -26423040.0, -26423404.0, -26423640.0, -26424044.0, -26424080.0, -26424140.0, -26424168.0, -26424648.0, -26424832.0, -26425056.0, -26425200.0, -26425328.0, -26425384.0, -26425860.0, -26426108.0, -26426672.0, -26426940.0, -26427436.0, -26427540.0, -26428480.0, -26457888.0, -26458048.0, -26458248.0, -26458272.0, -26458840.0, -26459716.0, -26460528.0, -26460544.0, -26460828.0, -26460920.0, -26461136.0, -26461184.0, -26461320.0, -26461584.0, -26462064.0, -26462164.0, -26462240.0, -26462676.0, -26463240.0, -26463712.0, -26463912.0, -26464152.0, -26464936.0, -26465140.0, -26465176.0, -26465224.0, -26465360.0, -26465408.0, -26465648.0, -26465736.0, -26465960.0, -26466824.0, -26466976.0, -26466984.0, -26467104.0, -26467640.0, -26467884.0, -26468164.0, -26469308.0, -26500120.0, -26500480.0, -26500684.0, -26500992.0, -26501120.0, -26501360.0, -26501784.0, -26501804.0, -26501956.0, -26502020.0, -26502104.0, -26502128.0, -26502472.0, -26502608.0, -26502644.0, -26503112.0, -26503404.0, -26503536.0, -26503624.0, -26503880.0, -26504360.0, -26504440.0, -26504592.0, -26504904.0, -26504912.0, -26505128.0, -26505148.0, -26505324.0, -26505664.0, -26505980.0, -26506000.0, -26506208.0, -26506256.0, -26506616.0, -26506984.0, -26507152.0, -26507904.0, -26508108.0, -26508592.0, -26508672.0, -26509152.0, -26509648.0, -26509816.0, -26510104.0, -26510720.0, -26510856.0, -26511224.0, -26542216.0, -26543240.0, -26544272.0, -26544816.0, -26544848.0, -26545124.0, -26545464.0, -26545936.0, -26545952.0, -26545960.0, -26546272.0, -26546344.0, -26546408.0, -26546940.0, -26547228.0, -26547232.0, -26547368.0, -26547892.0, -26547928.0, -26547952.0, -26547964.0, -26548080.0, -26548204.0, -26548360.0, -26548436.0, -26548548.0, -26548616.0, -26548856.0, -26549180.0, -26549344.0, -26549352.0, -26549560.0, -26549752.0, -26549968.0, -26549988.0, -26550080.0, -26550408.0, -26550544.0, -26552048.0, -26552216.0, -26552264.0, -26552416.0, -26552472.0, -26553148.0, -26553160.0, -26553376.0, -26553412.0, -26553420.0, -26553424.0, -26553440.0, -26553520.0, -26553796.0, -26554048.0, -26554432.0, -26584696.0, -26584728.0, -26584816.0, -26585308.0, -26585424.0, -26585688.0, -26585944.0, -26585964.0, -26586016.0, -26586224.0, -26586496.0, -26586528.0, -26586712.0, -26586768.0, -26586776.0, -26586848.0, -26587040.0, -26587072.0, -26587088.0, -26587180.0, -26587244.0, -26587360.0, -26587680.0, -26588120.0, -26588132.0, -26588220.0, -26588448.0, -26588648.0, -26588752.0, -26589336.0, -26589360.0, -26589392.0, -26589456.0, -26589832.0, -26590300.0, -26590508.0, -26590552.0, -26590616.0, -26590720.0, -26590848.0, -26591000.0, -26591336.0, -26591460.0, -26591624.0, -26591752.0, -26591772.0, -26591936.0, -26592152.0, -26592176.0, -26592312.0, -26592392.0, -26592732.0, -26592936.0, -26593424.0, -26593464.0, -26594080.0, -26595480.0, -26595640.0, -26595680.0, -26595704.0, -26595760.0, -26596024.0, -26596576.0, -26596892.0, -26596936.0, -41452480.0, -41452692.0, -41453804.0, -41454692.0, -41454700.0, -41454776.0, -41455596.0, -41455644.0, -41456724.0, -41460504.0, -41460556.0, -41460596.0, -41461564.0, -41462396.0, -41462416.0, -41462452.0, -41462480.0, -41463224.0, -41463412.0, -41463564.0, -41463684.0, -41464330.0, -41464344.0, -41465460.0, -41465540.0, -41466256.0, -41466260.0, -41466292.0, -41466300.0, -41466460.0, -41466464.0, -41467372.0, -41469250.0, -41470176.0, -41470212.0, -41470250.0, -41470276.0, -41471116.0, -41471210.0, -41471372.0, -41473176.0, -41473490.0, -41474070.0, -41474184.0, -41474228.0, -41474316.0, -41475004.0, -41475016.0, -41475216.0, -41475230.0, -41475430.0, -41476110.0, -41477064.0, -41477170.0, -41477196.0, -41477296.0, -41478070.0, -41478092.0, -41478130.0, -41478348.0, -41479064.0, -41479228.0, -41479876.0, -41480936.0, -41482076.0, -41551188.0, -41551268.0, -41551320.0, -41552400.0, -41552652.0, -41553268.0, -41553320.0, -41553390.0, -41554228.0, -41554296.0, -41554348.0, -41554388.0, -41554440.0, -41555304.0, -41555310.0, -41555344.0, -41560050.0, -41560070.0, -41560160.0, -41560988.0, -41561190.0, -41561468.0, -41562004.0, -41562076.0, -41562084.0, -41562252.0, -41562256.0, -41563104.0, -41563900.0, -41563910.0, -41564156.0, -41564212.0, -41565890.0, -41565970.0, -41566028.0, -41566052.0, -41566096.0, -41567080.0, -41567110.0, -41567176.0, -41567890.0, -41567988.0, -41568084.0, -41568890.0, -41568916.0, -41570732.0, -41570844.0, -41571040.0, -41571700.0, -41571948.0, -41572896.0, -41572908.0, -41577856.0, -41578780.0, -41650108.0, -41651892.0, -41652040.0, -41652150.0, -41653084.0, -41654944.0, -41655944.0, -41656016.0, -41656244.0, -41656900.0, -41657876.0, -41659850.0, -41660750.0, -41660756.0, -41660988.0, -41661052.0, -41661890.0, -41662656.0, -41662764.0, -41663060.0, -41663610.0, -41663612.0, -41664868.0, -41665796.0, -41665828.0, -41666660.0, -41666680.0, -41666690.0, -41667624.0, -41668644.0, -41668696.0, -41669428.0, -41669708.0, -41669736.0, -41670484.0, -41670544.0, -41670588.0, -41670596.0, -41671420.0, -41672320.0, -41672470.0, -41672636.0, -41672736.0, -41673400.0, -41675610.0, -41676452.0, -41676660.0, -41748964.0, -41751548.0, -41751576.0, -41751816.0, -41751940.0, -41752708.0, -41753776.0, -41754504.0, -41754624.0, -41754740.0, -41755524.0, -41756456.0, -41756536.0, -41756750.0, -41757670.0, -41757720.0, -41758400.0, -41758744.0, -41759410.0, -41759576.0, -41761684.0, -41762396.0, -41762428.0, -41764340.0, -41764420.0, -41764496.0, -41765268.0, -41765332.0, -41765372.0, -41767252.0, -41767400.0, -41768292.0, -41768316.0, -41769336.0, -41771270.0, -41772148.0, -41772190.0, -41772230.0, -41775264.0, -41843696.0, -41844530.0, -41845628.0, -41846570.0, -41847548.0, -41848404.0, -41848740.0, -41849476.0, -41849510.0, -41849520.0, -41850468.0, -41850508.0, -41851160.0, -41851264.0, -41851412.0, -41852470.0, -41853376.0, -41853584.0, -41854300.0, -41854436.0, -41855500.0, -41856396.0, -41856452.0, -41856524.0, -41857364.0, -41859092.0, -41860136.0, -41861224.0, -41862228.0, -41862520.0, -41863308.0, -41865076.0, -41865120.0, -41865204.0, -41865228.0, -41865280.0, -41865330.0, -41866144.0, -41866148.0, -41866270.0, -41866360.0, -41867190.0, -41867532.0, -41870108.0, -41870156.0, -41870932.0, -41870976.0, -41871080.0, -41871830.0, -41871900.0, -41871964.0, -41872132.0, -41873016.0, -41873036.0, -41873100.0, -41941396.0, -41941456.0, -41941560.0, -41942416.0, -41944268.0, -41944452.0, -41946150.0, -41946170.0, -41946256.0, -41946344.0, -41946372.0, -41947244.0, -41947252.0, -41947270.0, -41948130.0, -41948290.0, -41948292.0, -41948450.0, -41949124.0, -41949250.0, -41949252.0, -41949256.0, -41949360.0, -41950884.0, -41950980.0, -41951028.0, -41951110.0, -41952920.0, -41953040.0, -41954030.0, -41954080.0, -41954092.0, -41954176.0, -41954204.0, -41954210.0, -41954212.0, -41955010.0, -41955176.0, -41955984.0, -41956852.0, -41957148.0, -41959016.0, -41962860.0, -41966780.0, -41966924.0, -41966950.0, -41968736.0, -41968744.0, -41968868.0, -41969584.0, -41969744.0, -41970604.0, -41970880.0, -42040010.0, -42040108.0, -42040150.0, -42041012.0, -42042092.0, -42043110.0, -42043296.0, -42044012.0, -42044156.0, -42044936.0, -42044990.0, -42044996.0, -42046908.0, -42047024.0, -42047828.0, -42047868.0, -42047910.0, -42047960.0, -42048670.0, -42048696.0, -42048932.0, -42049020.0, -42049860.0, -42049892.0, -42049924.0, -42050896.0, -42050930.0, -42051788.0, -42051796.0, -42051804.0, -42051936.0, -42051950.0, -42051990.0, -42052828.0, -42053870.0, -42053890.0, -42054784.0, -42054804.0, -42054876.0, -42055750.0, -42055784.0, -42055788.0, -42055810.0, -42055864.0, -42056540.0, -42056796.0, -42056816.0, -42057650.0, -42057844.0, -42058576.0, -42058664.0, -42059516.0, -42061532.0, -42061536.0, -42062770.0, -42063468.0, -42063516.0, -42063560.0, -42064536.0, -42064544.0, -42064610.0, -42065656.0, -42065684.0, -42066336.0, -42067410.0, -42067412.0, -42067430.0, -42067660.0, -42068216.0, -42068440.0, -42068460.0, -42136740.0, -42136810.0, -42136890.0, -42136916.0, -42137748.0, -42137824.0, -42137836.0, -42137844.0, -42137856.0, -42138588.0, -42138676.0, -42141896.0, -42142620.0, -42142750.0, -42142864.0, -42143644.0, -42143716.0, -42144560.0, -42144664.0, -42144780.0, -42145556.0, -42145704.0, -42145720.0, -42145724.0, -42145916.0, -42146532.0, -42146596.0, -42146650.0, -42146744.0, -42146756.0, -42148310.0, -42148496.0, -42148696.0, -42149296.0, -42150428.0, -42150464.0, -42150516.0, -42150550.0, -42151532.0, -42151564.0, -42151652.0, -42153360.0, -42153396.0, -42153412.0, -42153500.0, -42153532.0, -42154360.0, -42155384.0, -42155560.0, -42156332.0, -42157330.0, -42157348.0, -42157390.0, -42158116.0, -42158428.0, -42160330.0, -42162172.0, -42162196.0, -42162224.0, -42163230.0, -42165924.0, -42166024.0, -42166056.0, -42166092.0, -42166150.0, -42166164.0, -42166176.0, -42235740.0, -42237476.0, -42237788.0, -42238516.0, -42238740.0, -42239120.0, -42239440.0, -42239464.0, -42239480.0, -42241410.0, -42242270.0, -42242532.0, -42242572.0, -42244324.0, -42244348.0, -42244350.0, -42244388.0, -42244450.0, -42244640.0, -42245372.0, -42246212.0, -42246384.0, -42246388.0, -42246580.0, -42247204.0, -42248228.0, -42248310.0, -42248376.0, -42248476.0, -42249240.0, -42249284.0, -42250290.0, -42251056.0, -42251180.0, -42251196.0, -42251280.0, -42252130.0, -42252384.0, -42252852.0, -42253030.0, -42253068.0, -42253110.0, -42253160.0, -42253196.0, -42253212.0, -42253310.0, -42253330.0, -42254070.0, -42255068.0, -42255180.0, -42255264.0, -42255308.0, -42255980.0, -42256084.0, -42256144.0, -42256204.0, -42257864.0, -42258050.0, -42258076.0, -42259120.0, -42259190.0, -42259196.0, -42260076.0, -42260948.0, -42261030.0, -42261188.0, -42261828.0, -42262000.0, -42262170.0, -42262250.0, -42262970.0, -42332304.0, -42333516.0, -42334544.0, -42335240.0, -42335292.0, -42336132.0, -42336190.0, -42336212.0, -42336228.0, -42336240.0, -42336330.0, -42336344.0, -42338184.0, -42338296.0, -42340910.0, -42341050.0, -42341180.0, -42341224.0, -42342120.0, -42343068.0, -42343828.0, -42343836.0, -42344004.0, -42344020.0, -42344024.0, -42344080.0, -42344104.0, -42344836.0, -42345940.0, -42345944.0, -42345948.0, -42345950.0, -42345988.0, -42346036.0, -42346670.0, -42346816.0, -42347936.0, -42347944.0, -42347984.0, -42349004.0, -42349864.0, -42349908.0, -42349990.0, -42350028.0, -42350830.0, -42352800.0, -42352812.0, -42352856.0, -42352936.0, -42352988.0, -42353056.0, -42353920.0, -42353990.0, -42354730.0, -42354908.0, -42355680.0, -42355776.0, -42356720.0, -42356736.0, -42356780.0, -42356836.0, -42357800.0, -42358544.0, -42358684.0, -42359496.0, -42359596.0, -42359840.0, -42361436.0, -42361640.0, -42432756.0, -42432930.0, -42432932.0, -42432990.0, -42433012.0, -42433864.0, -42433996.0, -42434070.0, -42434190.0, -42434800.0, -42434964.0, -42434972.0, -42435028.0, -42435840.0, -42435876.0, -42437868.0, -42437896.0, -42437900.0, -42437910.0, -42438550.0, -42438724.0, -42438840.0, -42438850.0, -42438870.0, -42439644.0, -42439650.0, -42439756.0, -42439760.0, -42439920.0, -42440732.0, -42441510.0, -42442604.0, -42442780.0, -42443556.0, -42443724.0, -42443856.0, -42444732.0, -42445492.0, -42445576.0, -42445590.0, -42445600.0, -42445820.0, -42447490.0, -42447496.0, -42447628.0, -42448640.0, -42449510.0, -42449572.0, -42449620.0, -42450556.0, -42451372.0, -42451420.0, -42451468.0, -42451600.0, -42451720.0, -42452388.0, -42452470.0, -42452492.0, -42452496.0, -42452590.0, -42453510.0, -42453536.0, -42454544.0, -42455268.0, -42455372.0, -42456052.0, -42456690.0, -42457380.0, -42457436.0, -42457504.0, -42459210.0, -42527764.0, -42527810.0, -42527828.0, -42527836.0, -42528516.0, -42528850.0, -42530604.0, -42531572.0, -42532900.0, -42538820.0, -42543452.0, -42543644.0, -42544364.0, -42547348.0, -42549150.0, -42549336.0, -42549384.0, -42550050.0, -42550090.0, -42550176.0, -42550264.0, -42550320.0, -42551084.0, -42552064.0, -42552930.0, -42553130.0, -42553964.0, -42554068.0, -42554908.0, -42555060.0, -42555064.0, -42555076.0, -42555170.0, -42555970.0, -42556060.0, -42556172.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7784090909090909\n",
      "Hamming Loss: 0.11931818181818182\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       578\n",
      "           1       0.18      1.00      0.30       126\n",
      "\n",
      "    accuracy                           0.18       704\n",
      "   macro avg       0.09      0.50      0.15       704\n",
      "weighted avg       0.03      0.18      0.05       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 302us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 114us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 113us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 132us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 114us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 106us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 113us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 116us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 106us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 159us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 106us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 132us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 116us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 121us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 1s 199us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 167us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 97us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 117us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 169us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 149us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 168us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 159us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 122us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 120us/sample - loss: 13.6198 - binary_accuracy: 0.1147 - loss: 13.6200 - binary_accuracy\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 1s 184us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 1s 195us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 1s 192us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 163us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 132us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 120us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 133us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 1s 449us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-59687560.0, -59688010.0, -59688572.0, -59688870.0, -59689344.0, -59689770.0, -59689776.0, -59689996.0, -59690004.0, -59690720.0, -59691400.0, -59691464.0, -59693190.0, -59693256.0, -59693700.0, -59693736.0, -59693764.0, -59693852.0, -59694156.0, -59694300.0, -59694520.0, -59694600.0, -59695064.0, -59695216.0, -59695690.0, -59695732.0, -59695870.0, -59696100.0, -59696270.0, -59696316.0, -59697080.0, -59697490.0, -59697556.0, -59697610.0, -59698220.0, -59698340.0, -59698696.0, -59698700.0, -59698748.0, -59698824.0, -59698864.0, -59698900.0, -59699080.0, -59699456.0, -59699600.0, -59699616.0, -59700070.0, -59700148.0, -59700308.0, -59700690.0, -59700820.0, -59700840.0, -59700870.0, -59701364.0, -59701424.0, -59701450.0, -59701680.0, -59701830.0, -59701920.0, -59701948.0, -59701976.0, -59701980.0, -59702060.0, -59702070.0, -59703304.0, -59703796.0, -59703884.0, -59703976.0, -59704130.0, -59704388.0, -59704476.0, -59704620.0, -59705084.0, -59705564.0, -59705910.0, -59706376.0, -59706576.0, -59707104.0, -59750092.0, -59752030.0, -59752336.0, -59753080.0, -59753670.0, -59753816.0, -59753820.0, -59753870.0, -59754508.0, -59755068.0, -59755736.0, -59756948.0, -59756990.0, -59757356.0, -59757544.0, -59757764.0, -59758124.0, -59758144.0, -59758260.0, -59758750.0, -59758860.0, -59758900.0, -59759084.0, -59759476.0, -59759940.0, -59760530.0, -59760664.0, -59760740.0, -59760764.0, -59760830.0, -59761160.0, -59761396.0, -59761452.0, -59761544.0, -59761908.0, -59761996.0, -59762020.0, -59762028.0, -59762036.0, -59762056.0, -59762210.0, -59762692.0, -59762790.0, -59763140.0, -59763308.0, -59763356.0, -59764360.0, -59764390.0, -59765092.0, -59765096.0, -59765116.0, -59765140.0, -59765812.0, -59765896.0, -59766060.0, -59766252.0, -59766444.0, -59766484.0, -59766524.0, -59766530.0, -59766540.0, -59766548.0, -59767050.0, -59812510.0, -59812676.0, -59813740.0, -59814016.0, -59814370.0, -59814428.0, -59815160.0, -59815200.0, -59815830.0, -59816360.0, -59817100.0, -59817544.0, -59817810.0, -59818284.0, -59818372.0, -59818388.0, -59818924.0, -59819044.0, -59819570.0, -59819584.0, -59819652.0, -59819656.0, -59820176.0, -59820196.0, -59820230.0, -59820256.0, -59820304.0, -59821630.0, -59821996.0, -59822070.0, -59822470.0, -59822732.0, -59822744.0, -59822770.0, -59822804.0, -59822830.0, -59823090.0, -59823244.0, -59823348.0, -59823460.0, -59823748.0, -59823764.0, -59824052.0, -59824496.0, -59824544.0, -59824580.0, -59824824.0, -59825116.0, -59825220.0, -59825260.0, -59825796.0, -59825810.0, -59825836.0, -59825924.0, -59826344.0, -59826424.0, -59826628.0, -59826690.0, -59827090.0, -59827132.0, -59827170.0, -59827224.0, -59827844.0, -59828444.0, -59829040.0, -59829730.0, -59830388.0, -59830856.0, -59830984.0, -59831548.0, -59831664.0, -59832656.0, -59832810.0, -59877150.0, -59878170.0, -59878308.0, -59878380.0, -59878440.0, -59879540.0, -59879572.0, -59879696.0, -59880116.0, -59880156.0, -59880170.0, -59880270.0, -59880748.0, -59880784.0, -59881316.0, -59881596.0, -59881956.0, -59882188.0, -59882220.0, -59882250.0, -59882830.0, -59884704.0, -59884716.0, -59885290.0, -59886652.0, -59888284.0, -59888350.0, -59888492.0, -59888596.0, -59889204.0, -59889748.0, -59890770.0, -59890840.0, -59890870.0, -59890910.0, -59890920.0, -59890970.0, -59890996.0, -59891030.0, -59891056.0, -59891516.0, -59891560.0, -59892316.0, -59892772.0, -59892784.0, -59892850.0, -59893510.0, -59893610.0, -59893948.0, -59894036.0, -59894160.0, -59894252.0, -59897452.0, -59940344.0, -59940856.0, -59940960.0, -59941496.0, -59941636.0, -59944588.0, -59944784.0, -59945316.0, -59945736.0, -59947188.0, -59947240.0, -59947868.0, -59948304.0, -59948572.0, -59949444.0, -59950470.0, -59951030.0, -59951670.0, -59951704.0, -59951756.0, -59952230.0, -59952776.0, -59952876.0, -59952900.0, -59952936.0, -59952956.0, -59952960.0, -59952970.0, -59953468.0, -59953890.0, -59954100.0, -59954788.0, -59954804.0, -59955676.0, -59956960.0, -59957100.0, -59957124.0, -59957336.0, -59957356.0, -59957428.0, -59957550.0, -59958456.0, -59959900.0, -59960230.0, -59960236.0, -59961404.0, -60004120.0, -60004690.0, -60004716.0, -60004776.0, -60005096.0, -60005376.0, -60005536.0, -60005784.0, -60005870.0, -60006060.0, -60006350.0, -60006668.0, -60007190.0, -60007796.0, -60007860.0, -60008524.0, -60008812.0, -60009130.0, -60009176.0, -60009640.0, -60010080.0, -60011344.0, -60011616.0, -60011708.0, -60011932.0, -60011988.0, -60013836.0, -60016956.0, -60017052.0, -60017680.0, -60020280.0, -60020990.0, -60021080.0, -60021144.0, -60022300.0, -60023284.0, -60025210.0, -60066476.0, -60066516.0, -60067240.0, -60067292.0, -60067684.0, -60067700.0, -60068460.0, -60068840.0, -60069532.0, -60070188.0, -60070764.0, -60071204.0, -60071292.0, -60071824.0, -60071972.0, -60072210.0, -60072936.0, -60073428.0, -60074012.0, -60074496.0, -60074556.0, -60074730.0, -60074828.0, -60075108.0, -60075228.0, -60075384.0, -60075724.0, -60075788.0, -60075844.0, -60075948.0, -60076330.0, -60076880.0, -60076892.0, -60076904.0, -60076990.0, -60077036.0, -60077510.0, -60077984.0, -60078556.0, -60078748.0, -60078910.0, -60079852.0, -60079936.0, -60080430.0, -60081560.0, -60082660.0, -60083204.0, -60083784.0, -60125464.0, -60128140.0, -60129012.0, -60129476.0, -60130052.0, -60130520.0, -60131036.0, -60131160.0, -60131296.0, -60132220.0, -60132510.0, -60132790.0, -60132904.0, -60133010.0, -60133040.0, -60133332.0, -60133496.0, -60133908.0, -60134492.0, -60134588.0, -60134960.0, -60135212.0, -60136924.0, -60136940.0, -60136948.0, -60137060.0, -60137396.0, -60137444.0, -60137476.0, -60137510.0, -60137580.0, -60137630.0, -60138636.0, -60139176.0, -60139652.0, -60140160.0, -60140172.0, -60140212.0, -60140308.0, -60140356.0, -60140380.0, -60140540.0, -60140590.0, -60141036.0, -60141430.0, -60183496.0, -60183756.0, -60184856.0, -60184936.0, -60185304.0, -60185356.0, -60185370.0, -60185404.0, -60185532.0, -60185550.0, -60185970.0, -60186668.0, -60187010.0, -60187240.0, -60188108.0, -60188184.0, -60188212.0, -60188256.0, -60188696.0, -60188950.0, -60189170.0, -60189576.0, -60189800.0, -60190016.0, -60191092.0, -60191716.0, -60192056.0, -60192544.0, -60192664.0, -60193280.0, -60193410.0, -60194400.0, -60194470.0, -60194520.0, -60194530.0, -60194610.0, -60194656.0, -60195184.0, -60195636.0, -60196280.0, -60196676.0, -60196696.0, -60196704.0, -60196772.0, -60196876.0, -60196908.0, -60196988.0, -60197336.0, -60197344.0, -60197464.0, -60197550.0, -60198388.0, -60198468.0, -60198720.0, -60199590.0, -60199692.0, -60240660.0, -60240820.0, -60240830.0, -60241216.0, -60241270.0, -60241732.0, -60241876.0, -60241988.0, -60242050.0, -60242496.0, -60242844.0, -60242990.0, -60243580.0, -60243616.0, -60243624.0, -60244060.0, -60244230.0, -60244616.0, -60244748.0, -60244828.0, -60244890.0, -60245216.0, -60245240.0, -60245252.0, -60245464.0, -60245470.0, -60245896.0, -60246020.0, -60246240.0, -60246424.0, -60246452.0, -60246830.0, -60246852.0, -60247504.0, -60248068.0, -60248550.0, -60248736.0, -60248790.0, -60248856.0, -60249756.0, -60249796.0, -60250056.0, -60251344.0, -60251520.0, -60251572.0, -60251576.0, -60251644.0, -60252060.0, -60252090.0, -60253430.0, -60253524.0, -60253940.0, -60254092.0, -60254736.0, -60255024.0, -60256230.0, -60256252.0, -60256270.0, -60256304.0, -60256324.0, -60257240.0, -60257970.0, -60258424.0, -60298756.0, -60298920.0, -60299120.0, -60299456.0, -60299540.0, -60299560.0, -60300196.0, -60300776.0, -60301160.0, -60301804.0, -60301920.0, -60302212.0, -60302240.0, -60302340.0, -60302776.0, -60302820.0, -60302868.0, -60303070.0, -60303308.0, -60303680.0, -60304596.0, -60304676.0, -60304812.0, -60305084.0, -60305210.0, -60305230.0, -60305284.0, -60305764.0, -60305890.0, -60306296.0, -60306300.0, -60306310.0, -60306324.0, -60306390.0, -60306440.0, -60306588.0, -60306884.0, -60307096.0, -60307108.0, -60307140.0, -60307460.0, -60307496.0, -60307692.0, -60307776.0, -60307936.0, -60308180.0, -60308536.0, -60308692.0, -60308790.0, -60309104.0, -60309240.0, -60309292.0, -60309360.0, -60309390.0, -60309436.0, -60309540.0, -60309744.0, -60309860.0, -60310332.0, -60310380.0, -60310410.0, -60310484.0, -60310524.0, -60310550.0, -60310630.0, -60311036.0, -60311132.0, -60311164.0, -60311600.0, -60311692.0, -60312224.0, -60312560.0, -60313050.0, -60313250.0, -60313510.0, -60313572.0, -60313708.0, -60314044.0, -60314140.0, -60314480.0, -60316124.0, -60316212.0, -60353188.0, -60354136.0, -60354204.0, -60354340.0, -60354880.0, -60354930.0, -60355510.0, -60355720.0, -60355948.0, -60355988.0, -60356652.0, -60356748.0, -60357096.0, -60357156.0, -60357464.0, -60357684.0, -60357724.0, -60357840.0, -60358668.0, -60359332.0, -60359824.0, -60360588.0, -60361868.0, -60362212.0, -60362224.0, -60362330.0, -60362630.0, -60362740.0, -60362812.0, -60363000.0, -60363244.0, -60363276.0, -60363348.0, -60363396.0, -60363850.0, -60363950.0, -60365076.0, -60366816.0, -60366884.0, -60366930.0, -60366972.0, -60367524.0, -60369076.0, -60369108.0, -60369616.0, -60369800.0, -60369850.0, -60369868.0, 38738430.0, 38738490.0, 38738652.0, 38739200.0, 38739264.0, 38739396.0, 38739510.0, 38739584.0, 38739710.0, 38739736.0, 38740050.0, 38740350.0, 38740496.0, 38740576.0, 38740720.0, 38743176.0, 38743410.0, 38743452.0, 38743930.0, 38744030.0, 38744400.0, 38744550.0, 38744590.0, 38744600.0, 38744630.0, 38744656.0, 38744892.0, 38744990.0, 38745024.0, 38745096.0, 38745100.0, 38745110.0, 38745144.0, 38745148.0, 38745184.0, 38745240.0, 38745260.0, 38745290.0, 38745310.0, 38745332.0, 38745384.0, 38745616.0, 38745624.0, 38745630.0, 38745640.0, 38745732.0, 38745816.0, 38745864.0, 38745884.0, 38745936.0, 38745996.0, 38746012.0, 38746216.0, 38746240.0, 38746280.0, 38746290.0, 38746410.0, 38746424.0, 38746440.0, 38746452.0, 38746504.0, 38746570.0, 38746576.0, 38746624.0, 38746630.0, 38746640.0, 38746650.0, 38746660.0, 38746664.0, 38746704.0, 38746816.0, 38746900.0, 38746976.0, 38747024.0, 38747096.0, 38747176.0, 38747200.0, 38747250.0, 38747356.0, 38747372.0, 38747464.0, 38747496.0, 38747736.0, 38747750.0, 38748010.0, 38748092.0, 38748184.0, 38748240.0, 38748310.0, 38748372.0, 38748376.0, 38748424.0, 38748536.0, 38748760.0, 38748876.0, 38749096.0, 38751904.0, 38752010.0, 38752100.0, 38752580.0, 38752596.0, 38752690.0, 38752724.0, 38752748.0, 38753150.0, 38753224.0, 38753370.0, 38753376.0, 38753490.0, 38753510.0, 38753536.0, 38753664.0, 38753704.0, 38753708.0, 38753736.0, 38753920.0, 38753972.0, 38753980.0, 38754256.0, 38754304.0, 38754456.0, 38754490.0, 38754772.0, 38754816.0, 38754870.0, 38754880.0, 38754900.0, 38754936.0, 38754996.0, 38755064.0, 38755136.0, 38755332.0, 38755370.0, 38755376.0, 38755430.0, 38755464.0, 38755504.0, 38755524.0, 38755570.0, 38755572.0, 38755576.0, 38755588.0, 38755812.0, 38755840.0, 38756056.0, 38756140.0, 38756196.0, 38756304.0, 38756330.0, 38756616.0, 38756630.0, 38756710.0, 38756800.0, 38756816.0, 38756830.0, 38757024.0, 38757120.0, 38757250.0, 38757256.0, 38757456.0, 38757730.0, 38757770.0, 38757904.0, 38757944.0, 38760256.0, 38760264.0, 38760304.0, 38760436.0, 38760550.0, 38760824.0, 38761090.0, 38761396.0, 38761544.0, 38761616.0, 38761696.0, 38761820.0, 38761824.0, 38761868.0, 38761950.0, 38762104.0, 38762170.0, 38762370.0, 38762508.0, 38762576.0, 38762600.0, 38762624.0, 38762630.0, 38762788.0, 38762800.0, 38762810.0, 38762856.0, 38762870.0, 38762940.0, 38763064.0, 38763090.0, 38763100.0, 38763210.0, 38763250.0, 38763292.0, 38763320.0, 38763450.0, 38763452.0, 38763456.0, 38763576.0, 38763650.0, 38763670.0, 38763676.0, 38763730.0, 38763796.0, 38763800.0, 38763836.0, 38764156.0, 38764160.0, 38764320.0, 38764344.0, 38764476.0, 38764576.0, 38764604.0, 38764870.0, 38764880.0, 38765190.0, 38765370.0, 38765508.0, 38765664.0, 38765696.0, 38765720.0, 38765856.0, 38766350.0, 38769130.0, 38769276.0, 38769376.0, 38769540.0, 38769800.0, 38770332.0, 38770640.0, 38770710.0, 38770732.0, 38770790.0, 38770824.0, 38770860.0, 38770890.0, 38770976.0, 38770988.0, 38771084.0, 38771110.0, 38771130.0, 38771140.0, 38771170.0, 38771190.0, 38771216.0, 38771244.0, 38772320.0, 38772348.0, 38772390.0, 38772410.0, 38772416.0, 38772424.0, 38772450.0, 38772464.0, 38772544.0, 38772550.0, 38772596.0, 38772664.0, 38772704.0, 38772784.0, 38772816.0, 38772844.0, 38772880.0, 38772890.0, 38772944.0, 38772950.0, 38773016.0, 38773484.0, 38773810.0, 38773864.0, 38776344.0, 38777860.0, 38778036.0, 38778096.0, 38778210.0, 38778550.0, 38778804.0, 38779120.0, 38779144.0, 38779236.0, 38779690.0, 38780096.0, 38780576.0, 38780652.0, 38780664.0, 38780690.0, 38780704.0, 38780710.0, 38780776.0, 38780788.0, 38780830.0, 38780930.0, 38780970.0, 38781028.0, 38781144.0, 38781160.0, 38781416.0, 38781430.0, 38781436.0, 38781516.0, 38781544.0, 38781704.0, 38784056.0, 38784290.0, 38784496.0, 38784530.0, 38784920.0, 38784950.0, 38785050.0, 38786560.0, 38787150.0, 38787664.0, 38787760.0, 38787820.0, 38787828.0, 38787910.0, 38787980.0, 38787984.0, 38787990.0, 38788010.0, 38788184.0, 38788210.0, 38788376.0, 38788380.0, 38788400.0, 38788464.0, 38788468.0, 38788470.0, 38788480.0, 38788500.0, 38788544.0, 38788800.0, 38790360.0, 38791196.0, 38791610.0, 38791730.0, 38791864.0, 38791970.0, 38792024.0, 38792228.0, 38792596.0, 38792668.0, 38792770.0, 38793144.0, 38793456.0, 38793530.0, 38793650.0, 38793730.0, 38794104.0, 38794144.0, 38794156.0, 38794170.0, 38794348.0, 38794400.0, 38794440.0, 38794492.0, 38794550.0, 38794556.0, 38794560.0, 38794564.0, 38794652.0, 38794656.0, 38794840.0, 38794936.0, 38795044.0, 38795932.0, 38795944.0, 38795964.0, 38795976.0, 38795984.0, 38796030.0, 38796056.0, 38796240.0, 38796256.0, 38796396.0, 38796430.0, 38796440.0, 38796480.0, 38796530.0, 38796560.0, 38797644.0, 38797976.0, 38798144.0, 38798200.0, 38798370.0, 38799344.0, 38799470.0, 38799524.0, 38799550.0, 38799570.0, 38799656.0, 38799680.0, 38799690.0, 38799696.0, 38799700.0, 38799708.0, 38799720.0, 38799750.0, 38799776.0, 38799780.0, 38799790.0, 38799812.0, 38799836.0, 38799840.0, 38799916.0, 38799960.0, 38799964.0, 38799970.0, 38800004.0, 38800010.0, 38800040.0, 38800100.0, 38800124.0, 38800144.0, 38800200.0, 38800236.0, 38800268.0, 38800430.0, 38801200.0, 38801290.0, 38801304.0, 38801308.0, 38801360.0, 38801536.0, 38801564.0, 38801664.0, 38801744.0, 38801772.0, 38802776.0, 38802790.0, 38802800.0, 38802824.0, 38802836.0, 38802844.0, 38802850.0, 38802852.0, 38802864.0, 38802892.0, 38802930.0, 38802936.0, 38802944.0, 38802990.0, 38803010.0, 38803068.0, 38803070.0, 38803096.0, 38803120.0, 38803130.0, 38803176.0, 38803230.0, 38803260.0, 38803304.0, 38803320.0, 38803360.0, 38803412.0, 38803416.0, 38803424.0, 38803444.0, 38803490.0, 38803504.0, 38803510.0, 38803524.0, 38803580.0, 38803584.0, 38803596.0, 38803690.0, 38803720.0, 38804544.0, 38804624.0, 38804770.0, 38804960.0, 38805096.0, 38805170.0, 38805240.0, 38805270.0, 38805284.0, 38805290.0, 38805370.0, 38806116.0, 38806136.0, 38806184.0, 38806216.0, 38806240.0, 38806250.0, 38806264.0, 38806304.0, 38806310.0, 38806350.0, 38806360.0, 38806370.0, 38806400.0, 38806410.0, 38806416.0, 38806420.0, 38806424.0, 38806430.0, 38806492.0, 38806510.0, 38806520.0, 38806590.0, 38806596.0, 38806600.0, 38806610.0, 38806628.0, 38806640.0, 38806650.0, 38806700.0, 38806704.0, 38806750.0, 38806760.0, 38806764.0, 38806770.0, 38806776.0, 38806784.0, 38806796.0, 38806800.0, 38806868.0, 38806910.0, 38806920.0, 38806936.0, 38806940.0, 38806944.0, 38806976.0, 38807040.0, 38807090.0, 38807150.0, 38807176.0, 38807212.0, 38807290.0, 38807450.0, 38807470.0, 38807492.0, 38807600.0, 38807630.0, 38807664.0, 38807736.0, 38807830.0, 38807860.0, 38807892.0, 38808236.0, 38808284.0, 38808292.0, 38808452.0, 38808530.0, 38809090.0, 38809416.0, 38809424.0, 38809480.0, 38809548.0, 38809550.0, 38809556.0, 38809570.0, 38809630.0, 38809660.0, 38809664.0, 38809724.0, 38809732.0, 38809744.0, 38809760.0, 38809790.0, 38809796.0, 38809800.0, 38809804.0, 38809810.0, 38809816.0, 38809864.0, 38809870.0, 38809880.0, 38809904.0, 38809920.0, 38809950.0, 38809976.0, 38810010.0, 38810020.0, 38810030.0, 38810052.0, 38810056.0, 38810060.0, 38810068.0, 38810080.0, 38810090.0, 38810096.0, 38810104.0, 38810110.0, 38810120.0, 38810172.0, 38810190.0, 38810196.0, 38810200.0, 38810204.0, 38810216.0, 38810230.0, 38810256.0, 38810264.0, 38810270.0, 38810290.0, 38810324.0, 38810330.0, 38810360.0, 38810370.0, 38810384.0, 38810404.0, 38810416.0, 38810424.0, 38810450.0, 38810484.0, 38811816.0, 38812050.0, 38812120.0, 38812256.0, 38812290.0, 38812896.0, 38812948.0, 38812970.0, 38812984.0, 38813000.0, 38813024.0, 38813030.0, 38813040.0, 38813068.0, 38813084.0, 38813090.0, 38813120.0, 38813136.0, 38813176.0, 38813216.0, 38813228.0, 38813230.0, 38813296.0, 38813330.0, 38813336.0, 38813340.0, 38813416.0, 38813510.0, 38813544.0, 38813550.0, 38813670.0, 38813690.0, 38813720.0, 38813736.0, 38813788.0, 38813830.0, 38813852.0, 38816280.0, 38816344.0, 38816390.0, 38816430.0, 38816500.0, 38816548.0, 38816604.0, 38816636.0, 38816650.0, 38816656.0, 38816660.0, 38816668.0, 38816676.0, 38816680.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7997159090909091\n",
      "Hamming Loss: 0.10014204545454546\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       563\n",
      "           1       0.20      1.00      0.33       141\n",
      "\n",
      "    accuracy                           0.20       704\n",
      "   macro avg       0.10      0.50      0.17       704\n",
      "weighted avg       0.04      0.20      0.07       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 3s 1ms/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 116us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 133us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 169us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 123us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 132us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 115us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - ETA: 0s - loss: 6.1438 - binary_accuracy: 0.599 - 0s 109us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 100us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 100us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1393 - binary_accuracy: 0.5996s - loss: 6.0345 - binary_accuracy: 0\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 100us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 6.1393 - binary_accuracy: 0.5996s - loss: 6.0569 - binary_accuracy: 0.\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 103us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 100us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - ETA: 0s - loss: 6.1213 - binary_accuracy: 0.600 - 0s 101us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 1s 243us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 1s 207us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 130us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 159us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 164us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 168us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 155us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 1s 276us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 145us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 123us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 121us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 162us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 1s 193us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 170us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 155us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 172us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 1s 205us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 1s 191us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 158us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 167us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 168us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 121us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 6.1393 - binary_accuracy: 0.5996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [2411080.0, 2412856.0, 2413136.0, 2413200.0, 2413208.0, 2413656.0, 2414040.0, 2414128.0, 2414176.0, 2414304.0, 2414368.0, 2415032.0, 2415112.0, 2415152.0, 2415184.0, 2415520.0, 2415624.0, 2415632.0, 2415736.0, 2416512.0, 2416568.0, 2416800.0, 2417024.0, 2417256.0, 2417336.0, 2417408.0, 2417600.0, 2417688.0, 2417744.0, 2417792.0, 2417880.0, 2417912.0, 2417968.0, 2417976.0, 2417984.0, 2418088.0, 2418568.0, 2418776.0, 2418792.0, 2418928.0, 2418992.0, 2419200.0, 2419712.0, 2419800.0, 2420368.0, 2420472.0, 2420784.0, 2421208.0, 2421336.0, 2424720.0, 2424744.0, 2426064.0, 2426224.0, 2433024.0, 2433944.0, 2434384.0, 2434480.0, 2434768.0, 2435160.0, 2435192.0, 2435240.0, 2436088.0, 2436120.0, 2436192.0, 2436632.0, 2436768.0, 2436784.0, 2436808.0, 2436840.0, 2437008.0, 2437144.0, 2437280.0, 2437472.0, 2437488.0, 2437536.0, 2437616.0, 2437632.0, 2437696.0, 2437744.0, 2437800.0, 2437880.0, 2437936.0, 2438104.0, 2438272.0, 2438512.0, 2438544.0, 2438672.0, 2438696.0, 2438808.0, 2438848.0, 2438936.0, 2439048.0, 2439120.0, 2439152.0, 2439168.0, 2439256.0, 2439408.0, 2439888.0, 2440144.0, 2440200.0, 2440600.0, 2441312.0, 2441352.0, 2441984.0, 2442008.0, 2442360.0, 2442432.0, 2442888.0, 2445272.0, 2445384.0, 2445504.0, 2445656.0, 2445920.0, 2446144.0, 2446312.0, 2446616.0, 2447040.0, 2448600.0, 2448608.0, 2454136.0, 2454464.0, 2454592.0, 2454872.0, 2454880.0, 2454896.0, 2455920.0, 2456208.0, 2456376.0, 2456584.0, 2456976.0, 2457096.0, 2457224.0, 2457264.0, 2457424.0, 2458000.0, 2458152.0, 2458192.0, 2458224.0, 2458688.0, 2458920.0, 2458952.0, 2459000.0, 2459056.0, 2459232.0, 2459256.0, 2459352.0, 2459568.0, 2459768.0, 2459824.0, 2459968.0, 2459992.0, 2460512.0, 2460560.0, 2460640.0, 2460696.0, 2461096.0, 2461272.0, 2461512.0, 2461528.0, 2461536.0, 2461568.0, 2461592.0, 2461704.0, 2461736.0, 2461784.0, 2461976.0, 2462112.0, 2462176.0, 2462192.0, 2462240.0, 2462344.0, 2462640.0, 2462648.0, 2463488.0, 2463568.0, 2464024.0, 2464048.0, 2464104.0, 2464128.0, 2464160.0, 2464472.0, 2464480.0, 2464560.0, 2464616.0, 2464688.0, 2464976.0, 2465032.0, 2466008.0, 2466496.0, 2467064.0, 2467344.0, 2467616.0, 2470080.0, 2470528.0, 2477344.0, 2477464.0, 2477552.0, 2477720.0, 2477768.0, 2478224.0, 2478584.0, 2478832.0, 2479144.0, 2479304.0, 2479320.0, 2479672.0, 2479760.0, 2480256.0, 2480368.0, 2481240.0, 2481344.0, 2481680.0, 2481704.0, 2481816.0, 2481920.0, 2481936.0, 2482024.0, 2482136.0, 2482400.0, 2482416.0, 2482440.0, 2482712.0, 2482752.0, 2482832.0, 2483224.0, 2483312.0, 2483472.0, 2483632.0, 2483640.0, 2483648.0, 2483760.0, 2484232.0, 2484248.0, 2484752.0, 2484816.0, 2485024.0, 2485128.0, 2485376.0, 2485760.0, 2485904.0, 2486280.0, 2486328.0, 2486360.0, 2486416.0, 2486680.0, 2488016.0, 2490048.0, 2490200.0, 2499560.0, 2500840.0, 2500968.0, 2501304.0, 2501336.0, 2501368.0, 2501832.0, 2502936.0, 2502944.0, 2503176.0, 2503400.0, 2503408.0, 2503432.0, 2504080.0, 2504120.0, 2504232.0, 2504328.0, 2504432.0, 2504472.0, 2504560.0, 2504768.0, 2505176.0, 2505456.0, 2505512.0, 2505960.0, 2505992.0, 2506040.0, 2506104.0, 2506320.0, 2506336.0, 2506696.0, 2506800.0, 2507256.0, 2507608.0, 2508200.0, 2508616.0, 2508680.0, 2508752.0, 2508848.0, 2509240.0, 2509264.0, 2509896.0, 2510976.0, 2510984.0, 2522744.0, 2524136.0, 2524256.0, 2524496.0, 2524752.0, 2524768.0, 2524912.0, 2525096.0, 2525376.0, 2525400.0, 2525456.0, 2525600.0, 2525864.0, 2525896.0, 2525904.0, 2526040.0, 2526088.0, 2526272.0, 2526312.0, 2526328.0, 2526416.0, 2526440.0, 2526552.0, 2526896.0, 2527040.0, 2527672.0, 2527712.0, 2527728.0, 2527864.0, 2527904.0, 2527960.0, 2528080.0, 2528448.0, 2528472.0, 2528520.0, 2528832.0, 2529216.0, 2529232.0, 2529352.0, 2529600.0, 2530000.0, 2530016.0, 2530120.0, 2530368.0, 2530472.0, 2530544.0, 2530768.0, 2530856.0, 2530928.0, 2531016.0, 2531032.0, 2531216.0, 2531600.0, 2531704.0, 2532888.0, 2533144.0, 2533912.0, 2533936.0, 2533968.0, 2534008.0, 2534160.0, 2534504.0, 2534768.0, 2534848.0, 2534968.0, 2535824.0, 2545176.0, 2545664.0, 2545936.0, 2545976.0, 2546952.0, 2547304.0, 2547448.0, 2547480.0, 2547584.0, 2547600.0, 2547640.0, 2547832.0, 2547976.0, 2548016.0, 2548072.0, 2548104.0, 2548128.0, 2548464.0, 2548808.0, 2548992.0, 2549000.0, 2549144.0, 2549288.0, 2549344.0, 2549424.0, 2549504.0, 2549560.0, 2549584.0, 2549656.0, 2550152.0, 2550896.0, 2551096.0, 2551304.0, 2551640.0, 2552016.0, 2552288.0, 2552488.0, 2552560.0, 2552664.0, 2552760.0, 2552832.0, 2554336.0, 2554360.0, 2554560.0, 2554808.0, 2555040.0, 2555592.0, 2555784.0, 2555832.0, 2556344.0, 2556424.0, 2556680.0, 2557008.0, 2557104.0, 2557456.0, 2557552.0, 2557704.0, 2557792.0, 2568144.0, 2568896.0, 2569240.0, 2569288.0, 2569296.0, 2569304.0, 2569320.0, 2569456.0, 2569480.0, 2570168.0, 2570504.0, 2570520.0, 2570544.0, 2570552.0, 2570760.0, 2570888.0, 2571176.0, 2571304.0, 2571320.0, 2571472.0, 2571880.0, 2571896.0, 2572112.0, 2572304.0, 2572368.0, 2572544.0, 2572560.0, 2572568.0, 2572584.0, 2573672.0, 2573840.0, 2574160.0, 2574600.0, 2574912.0, 2574984.0, 2575088.0, 2576448.0, 2576696.0, 2577656.0, 2578632.0, 2578848.0, 2579520.0, 2584384.0, 2584400.0, 2584424.0, 2584480.0, 2584896.0, 2585472.0, 2585576.0, 2585608.0, 2585832.0, 2585944.0, 2585952.0, 2585984.0, 2586040.0, 2586088.0, 2586112.0, 2586120.0, 2586144.0, 2586240.0, 2586272.0, 2586296.0, 2586384.0, 2586448.0, 2586512.0, 2586560.0, 2586584.0, 2586624.0, 2586664.0, 2586672.0, 2586704.0, 2586912.0, 2587136.0, 2587288.0, 2587400.0, 2587408.0, 2587440.0, 2587504.0, 2587784.0, 2588032.0, 2588144.0, 2588192.0, 2588208.0, 2588264.0, 2588360.0, 2588392.0, 2588400.0, 2588472.0, 2588616.0, 2588632.0, 2588712.0, 2588752.0, 2588760.0, 2588776.0, 2588840.0, 2588904.0, 2588944.0, 2588960.0, 2588976.0, 2589000.0, 2589008.0, 2589016.0, 2589032.0, 2589096.0, 2589128.0, 2589144.0, 2589160.0, 2589184.0, 2589192.0, 2589200.0, 2589232.0, 2589248.0, 2589256.0, 2589264.0, 2589288.0, 2589296.0, 2589344.0, 2589360.0, 2589416.0, 2590048.0, 2590072.0, 2590096.0, 2590176.0, 2590184.0, 2590192.0, 2590224.0, 2590240.0, 2590248.0, 2590304.0, 2590312.0, 2590344.0, 2590352.0, 2590376.0, 2590416.0, 2590440.0, 2590464.0, 2590472.0, 2590488.0, 2590544.0, 2590648.0, 2590656.0, 2590704.0, 2590832.0, 2591256.0, 2591520.0, 2591536.0, 2592168.0, 2592216.0, 2592224.0, 2592232.0, 2592240.0, 2592248.0, 2592256.0, 2592272.0, 2592280.0, 2592288.0, 2592304.0, 2592312.0, 2592320.0, 2592360.0, 2592384.0, 2592456.0, 2592488.0, 2592560.0, 2592600.0, 2592616.0, 2592672.0, 2592696.0, 2592856.0, 2593056.0, 2593064.0, 2593072.0, 2593096.0, 2593224.0, 2593336.0, 2593368.0, 2593672.0, 2593696.0, 2593736.0, 2593856.0, 2593904.0, 2593928.0, 2593968.0, 2594080.0, 2594112.0, 2594160.0, 2594328.0, 2594344.0, 2594360.0, 2594376.0, 2594384.0, 2594392.0, 2594400.0, 2594408.0, 2594416.0, 2594424.0, 2594440.0, 2594448.0, 2594456.0, 2594464.0, 2594472.0, 2594480.0, 2594496.0, 2594504.0, 2594512.0, 2594544.0, 2596896.0, 2596952.0, 2596960.0, 2597112.0, 2597192.0, 2597264.0, 2597352.0, 2597360.0, 2597376.0, 2597424.0, 2597512.0, 2597520.0, 2597528.0, 2597568.0, 2597584.0, 2597592.0, 2597768.0, 2597808.0, 2597824.0, 2597848.0, 2598640.0, 2598648.0, 2598680.0, 2598688.0, 2598696.0, 2598704.0, 2598720.0, 2598728.0, 2598760.0, 2598768.0, 2598800.0, 2598832.0, 2598864.0, 77916550.0, 77916600.0, 77918424.0, 77918520.0, 77919270.0, 77919760.0, 77922800.0, 77923070.0, 77923230.0, 77923896.0, 77925000.0, 77925430.0, 77925440.0, 77927176.0, 77928330.0, 77928340.0, 77928440.0, 77928540.0, 77929660.0, 77930720.0, 77931384.0, 77931740.0, 77932420.0, 77932530.0, 77932730.0, 77933630.0, 77933864.0, 77933870.0, 77934050.0, 77934210.0, 77934220.0, 77934936.0, 77935620.0, 77936290.0, 77936450.0, 77937060.0, 77937610.0, 77938180.0, 77938230.0, 77938250.0, 77938260.0, 77938430.0, 77938580.0, 77938984.0, 77939180.0, 77939330.0, 77939700.0, 77942500.0, 77943740.0, 77944610.0, 78016810.0, 78017760.0, 78017910.0, 78017970.0, 78019384.0, 78023580.0, 78024056.0, 78025270.0, 78025600.0, 78025840.0, 78027130.0, 78027510.0, 78027910.0, 78029070.0, 78029110.0, 78029330.0, 78029700.0, 78029704.0, 78030170.0, 78030320.0, 78031090.0, 78031170.0, 78031260.0, 78031710.0, 78032430.0, 78032520.0, 78032640.0, 78032660.0, 78032680.0, 78033550.0, 78033590.0, 78033720.0, 78033860.0, 78033976.0, 78034170.0, 78035020.0, 78035064.0, 78035300.0, 78036056.0, 78036110.0, 78036220.0, 78036370.0, 78036450.0, 78036990.0, 78037010.0, 78037890.0, 78038160.0, 78038340.0, 78039750.0, 78040104.0, 78040140.0, 78040200.0, 78041310.0, 78041540.0, 78041590.0, 78042150.0, 78042240.0, 78042420.0, 78042620.0, 78043600.0, 78044320.0, 78044340.0, 78047416.0, 78047570.0, 78049160.0, 78049176.0, 78119070.0, 78120260.0, 78125060.0, 78125310.0, 78125320.0, 78125360.0, 78125380.0, 78125420.0, 78126430.0, 78126664.0, 78126744.0, 78127544.0, 78128264.0, 78128280.0, 78128350.0, 78129010.0, 78129300.0, 78129500.0, 78129650.0, 78129736.0, 78130140.0, 78130220.0, 78130270.0, 78130560.0, 78130580.0, 78130610.0, 78130770.0, 78131110.0, 78131464.0, 78131570.0, 78131656.0, 78131760.0, 78132490.0, 78133530.0, 78133900.0, 78134320.0, 78134650.0, 78135620.0, 78135890.0, 78135970.0, 78136136.0, 78136600.0, 78138460.0, 78138610.0, 78138660.0, 78138900.0, 78139416.0, 78140660.0, 78140776.0, 78141040.0, 78141976.0, 78142660.0, 78142856.0, 78142860.0, 78142984.0, 78143720.0, 78144830.0, 78145090.0, 78145470.0, 78145730.0, 78145970.0, 78147570.0, 78150830.0, 78219940.0, 78222620.0, 78222730.0, 78224340.0, 78225096.0, 78226700.0, 78226810.0, 78228440.0, 78228536.0, 78228584.0, 78228984.0, 78229020.0, 78229110.0, 78229240.0, 78229270.0, 78229310.0, 78229500.0, 78229650.0, 78229810.0, 78230030.0, 78230070.0, 78230100.0, 78230136.0, 78230240.0, 78230800.0, 78231060.0, 78231250.0, 78231260.0, 78231860.0, 78233110.0, 78233140.0, 78233520.0, 78234850.0, 78235600.0, 78235784.0, 78235944.0, 78236020.0, 78236030.0, 78236050.0, 78236290.0, 78236770.0, 78237430.0, 78237610.0, 78239260.0, 78241290.0, 78241336.0, 78242000.0, 78242250.0, 78242270.0, 78242504.0, 78243190.0, 78243620.0, 78244080.0, 78245416.0, 78245570.0, 78247680.0, 78312330.0, 78314310.0, 78314520.0, 78314660.0, 78314984.0, 78316560.0, 78317120.0, 78318800.0, 78319680.0, 78320080.0, 78320610.0, 78320840.0, 78320900.0, 78321420.0, 78324500.0, 78324744.0, 78324980.0, 78325064.0, 78325090.0, 78325350.0, 78325420.0, 78325890.0, 78327060.0, 78327220.0, 78327230.0, 78327290.0, 78328040.0, 78328290.0, 78329890.0, 78329910.0, 78330910.0, 78330936.0, 78330970.0, 78331620.0, 78331896.0, 78332960.0, 78333960.0, 78334920.0, 78339736.0, 78339890.0, 78341570.0, 78407304.0, 78407860.0, 78408380.0, 78408720.0, 78408920.0, 78409460.0, 78411170.0, 78411370.0, 78411810.0, 78411920.0, 78412520.0, 78412550.0, 78412690.0, 78412930.0, 78413750.0, 78414160.0, 78415750.0, 78415896.0, 78416540.0, 78416910.0, 78417040.0, 78418100.0, 78418216.0, 78418260.0, 78418540.0, 78419220.0, 78419304.0, 78419870.0, 78421060.0, 78421380.0, 78421610.0, 78421730.0, 78423320.0, 78424670.0, 78424770.0, 78424790.0, 78425000.0, 78425130.0, 78425144.0, 78425460.0, 78425710.0, 78425940.0, 78427064.0, 78427920.0, 78428080.0, 78428180.0, 78428584.0, 78429640.0, 78431670.0, 78431700.0, 78431890.0, 78432696.0, 78432730.0, 78432904.0, 78433520.0, 78433570.0, 78435300.0, 78436350.0, 78502960.0, 78505350.0, 78506020.0, 78506040.0, 78506350.0, 78506616.0, 78507070.0, 78507944.0, 78508640.0, 78509656.0, 78510160.0, 78510320.0, 78510424.0, 78510480.0, 78510510.0, 78511770.0, 78511830.0, 78512000.0, 78513150.0, 78514240.0, 78514280.0, 78514430.0, 78515050.0, 78515130.0, 78515310.0, 78516500.0, 78516700.0, 78516810.0, 78517190.0, 78517650.0, 78517720.0, 78517780.0, 78518940.0, 78519000.0, 78519060.0, 78519550.0, 78519710.0, 78519790.0, 78519840.0, 78519910.0, 78519930.0, 78520560.0, 78520770.0, 78520824.0, 78521304.0, 78521500.0, 78521680.0, 78521860.0, 78522290.0, 78522950.0, 78523390.0, 78523740.0, 78524540.0, 78524670.0, 78525304.0, 78525460.0, 78525680.0, 78525864.0, 78526290.0, 78526350.0, 78527010.0, 78527176.0, 78530290.0, 78532104.0, 78533110.0, 78600840.0, 78601170.0, 78601304.0, 78602810.0, 78603170.0, 78605256.0, 78605330.0, 78605810.0, 78606100.0, 78608230.0, 78609250.0, 78609280.0, 78609990.0, 78611450.0, 78611620.0, 78611760.0, 78611970.0, 78612700.0, 78612820.0, 78613580.0, 78613700.0, 78613770.0, 78613810.0, 78614880.0, 78615030.0, 78616330.0, 78616460.0, 78618536.0, 78618890.0, 78620190.0, 78620340.0, 78620440.0, 78620610.0, 78620950.0, 78621180.0, 78622190.0, 78623220.0, 78626390.0, 78626420.0, 78626540.0, 78626930.0, 78627780.0, 78627890.0, 78628100.0, 78698990.0, 78699710.0, 78700140.0, 78700856.0, 78701050.0, 78701490.0, 78702584.0, 78702650.0, 78703064.0, 78703090.0, 78703180.0, 78703280.0, 78705090.0, 78705710.0, 78705736.0, 78705980.0, 78706480.0, 78707740.0, 78707790.0, 78708030.0, 78708420.0, 78708430.0, 78709390.0, 78709816.0, 78711120.0, 78711360.0, 78711390.0, 78712370.0, 78712490.0, 78712510.0, 78712800.0, 78713020.0, 78713410.0, 78713420.0, 78714260.0, 78714280.0, 78714376.0, 78716240.0, 78716300.0, 78716370.0, 78717336.0, 78718130.0, 78718160.0, 78719060.0, 78719310.0, 78719370.0, 78720200.0, 78720216.0, 78721090.0, 78721180.0, 78722800.0, 78722930.0, 78723820.0, 78723990.0, 78724600.0, 78724880.0, 78786910.0, 78788640.0, 78792910.0, 78793000.0, 78794810.0, 78794930.0, 78795170.0, 78796840.0, 78798550.0, 78798744.0, 78798750.0, 78799250.0, 78799520.0, 78799704.0, 78799720.0, 78799740.0, 78799820.0, 78799830.0, 78800010.0, 78800024.0, 78800200.0, 78800360.0, 78800664.0, 78801496.0, 78802220.0, 78803050.0, 78803064.0, 78803480.0, 78803500.0, 78803630.0, 78803870.0, 78804460.0, 78805224.0, 78805290.0, 78805470.0, 78805544.0, 78805570.0, 78805760.0, 78806540.0, 78807030.0, 78807380.0, 78808360.0, 78808400.0, 78809390.0, 78809520.0, 78810400.0, 78811120.0, 78811790.0, 78812140.0, 78813030.0, 78813300.0, 78813304.0, 78813800.0, 78813860.0, 78813976.0, 78814410.0, 78814590.0, 78814760.0, 78815150.0, 78815160.0, 78815520.0, 78815840.0, 78815940.0, 78816890.0, 78817304.0, 78817704.0, 78818000.0, 78819760.0, 78819870.0, 78820104.0, 78822770.0, 78822780.0, 78822830.0, 78823530.0, 78824584.0, 78825550.0, 78882760.0, 78882810.0, 78889420.0, 78889740.0, 78891500.0, 78891736.0, 78892104.0, 78892410.0, 78892904.0, 78893100.0, 78893700.0, 78893770.0, 78894000.0, 78894060.0, 78894240.0, 78895070.0, 78895280.0, 78895580.0, 78896150.0, 78896330.0, 78896456.0, 78896600.0, 78896630.0, 78896770.0, 78897120.0, 78897384.0, 78897510.0, 78897640.0, 78897944.0, 78897960.0, 78898550.0, 78898610.0, 78899096.0, 78899620.0, 78901120.0, 78901360.0, 78901380.0, 78901710.0, 78902020.0, 78903090.0, 78903420.0, 78904170.0, 78904180.0, 78904190.0, 78904250.0, 78904260.0, 78904440.0, 78905090.0, 78905450.0, 78905930.0, 78906030.0, 78906190.0, 78906264.0, 78906880.0, 78907770.0, 78907880.0, 78907944.0, 78908110.0, 78909790.0, 78911624.0, 78911650.0, 78911880.0, 78911980.0, 78912750.0, 78913080.0, 78913920.0, 78917490.0, 78917660.0, 78979240.0, 78980570.0, 78983860.0, 78984210.0, 78985660.0, 78986060.0, 78986280.0, 78987330.0, 78987660.0, 78987784.0, 78989250.0, 78989280.0, 78989410.0, 78989470.0, 78989570.0, 78989700.0, 78990220.0, 78990510.0, 78990584.0, 78990710.0, 78991230.0, 78993370.0, 78993400.0, 78993580.0, 78993620.0, 78994080.0, 78994220.0, 78994370.0, 78994780.0, 78995520.0, 78996664.0, 78998880.0, 78999010.0, 78999020.0, 78999030.0, 78999840.0, 79000020.0, 79000090.0, 79001130.0, 79001250.0, 79001960.0, 79003600.0, 79003800.0, 79003860.0, 79003880.0, 79003970.0, 79005976.0, 79007620.0, 79008424.0, 79010420.0, 79010720.0, 79010800.0, 79010850.0, 79013520.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7599431818181818\n",
      "Hamming Loss: 0.125\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       575\n",
      "           1       0.18      1.00      0.31       129\n",
      "\n",
      "    accuracy                           0.18       704\n",
      "   macro avg       0.09      0.50      0.15       704\n",
      "weighted avg       0.03      0.18      0.06       704\n",
      "\n",
      "y_pred shape: \n",
      "(704, 2)\n",
      "y_pred2 shape:\n",
      "(704, 2)\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         4\n",
      "         1.0       0.20      1.00      0.33         1\n",
      "\n",
      "    accuracy                           0.20         5\n",
      "   macro avg       0.10      0.50      0.17         5\n",
      "weighted avg       0.04      0.20      0.07         5\n",
      "\n",
      "Hamming Loss: 0.11178977272727272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "y_pred3, pred3, y_test3, hl3 = list(),list(),list(),list()\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2)\n",
    "    ]) \n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "\n",
    "  #y_pred2.append(y_pred)\n",
    "  y_pred3 = np.append(y_pred3, y_pred2)\n",
    "  #pred2.append(pred)\n",
    "  pred3 = np.append(pred3, pred2)\n",
    "  #y_test2.append(y_test)\n",
    "  y_test3 = np.append(y_test3, y_test2)\n",
    "  #hl2.append(hl)\n",
    "  hl3 = np.append(hl3, hl)\n",
    "    \n",
    "print(\"y_pred shape: \")\n",
    "print(np.shape(y_pred))\n",
    "\n",
    "print(\"y_pred2 shape:\")\n",
    "print(np.shape(y_pred2))\n",
    "\n",
    "y_pred3 = np.concatenate((y_pred3[0], y_pred3[1], y_pred3[2], y_pred3[3], y_pred3[4]), axis=None)\n",
    "pred3 = np.concatenate((pred3[0], pred3[1], pred3[2], pred3[3], pred3[4]), axis=None)\n",
    "y_test3 = np.concatenate((y_test3[0], y_test3[1], y_test3[2], y_test3[3], y_test3[4]), axis=None)\n",
    "hl3 = np.concatenate((hl3[0], hl3[1], hl3[2], hl3[3], hl3[4]), axis=None)\n",
    "\n",
    "print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\")\n",
    "#print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "print(classification_report(y_test3, pred3))\n",
    "hl3_avg = sum(hl3) / len(hl3)\n",
    "print(\"Hamming Loss: \" + str(hl3_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22053, 33)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_KGFS25.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17642 samples\n",
      "Epoch 1/100\n",
      "17642/17642 [==============================] - 3s 170us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 2/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 3/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 4/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 5/100\n",
      "17642/17642 [==============================] - 2s 95us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 6/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 7/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 8/100\n",
      "17642/17642 [==============================] - 3s 148us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 9/100\n",
      "17642/17642 [==============================] - 3s 145us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 10/100\n",
      "17642/17642 [==============================] - 2s 131us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 11/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 12/100\n",
      "17642/17642 [==============================] - 2s 127us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 13/100\n",
      "17642/17642 [==============================] - 2s 140us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 14/100\n",
      "17642/17642 [==============================] - 2s 140us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 15/100\n",
      "17642/17642 [==============================] - ETA: 0s - loss: 9.2822 - binary_accuracy: 0.398 - 2s 119us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 16/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 17/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 18/100\n",
      "17642/17642 [==============================] - 3s 150us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 19/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 20/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 21/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 22/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 23/100\n",
      "17642/17642 [==============================] - 3s 144us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 24/100\n",
      "17642/17642 [==============================] - 3s 155us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 25/100\n",
      "17642/17642 [==============================] - 2s 137us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 26/100\n",
      "17642/17642 [==============================] - 2s 127us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 27/100\n",
      "17642/17642 [==============================] - 2s 106us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 83/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 84/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 85/100\n",
      "17642/17642 [==============================] - 2s 99us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 86/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 87/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 88/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 89/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 90/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 91/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 92/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 93/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 94/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 95/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 96/100\n",
      "17642/17642 [==============================] - 2s 107us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 97/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 98/100\n",
      "17642/17642 [==============================] - 2s 107us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 99/100\n",
      "17642/17642 [==============================] - 3s 179us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 100/100\n",
      "17642/17642 [==============================] - 2s 142us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-225452880.0, -225452900.0, -225452910.0, -225452930.0, -225452960.0, -225453000.0, -225453060.0, -225453340.0, -225453380.0, -225453390.0, -225453400.0, -225453420.0, -225453440.0, -225453460.0, -225453470.0, -225453490.0, -225453500.0, -225453520.0, -225453540.0, -225453550.0, -225453570.0, -225453580.0, -225453600.0, -225453630.0, -225455900.0, -225455920.0, -225455970.0, -225456080.0, -225456100.0, -225456110.0, -225456130.0, -225456140.0, -225456160.0, -225456180.0, -225456200.0, -225456210.0, -225456220.0, -225456240.0, -225456260.0, -225456270.0, -225456290.0, -225456300.0, -225456320.0, -225456340.0, -225456350.0, -225456370.0, -225456380.0, -225456400.0, -225456420.0, -225456430.0, -225456450.0, -225456460.0, -225456480.0, -225456500.0, -225456510.0, -225456530.0, -225456540.0, -225456560.0, -225456580.0, -225456590.0, -225456600.0, -225456620.0, -225456640.0, -225456660.0, -225456670.0, -225456690.0, -225456700.0, -225456720.0, -225456740.0, -225456750.0, -225456770.0, -225456780.0, -225456800.0, -225456820.0, -225456830.0, -225456850.0, -225456860.0, -225456900.0, -225457950.0, -225457970.0, -225457980.0, -225458020.0, -225458030.0, -225458050.0, -225458060.0, -225458080.0, -225458270.0, -225458290.0, -225458300.0, -225458320.0, -225458340.0, -225458350.0, -225458370.0, -225458380.0, -225458400.0, -225458420.0, -225458430.0, -225458450.0, -225458460.0, -225458480.0, -225458500.0, -225458510.0, -225458530.0, -225458540.0, -225458560.0, -225458580.0, -225458600.0, -225458610.0, -225458620.0, -225458640.0, -225458660.0, -225458670.0, -225458690.0, -225458700.0, -225458720.0, -225458740.0, -225458750.0, -225458770.0, -225458780.0, -225458800.0, -225458820.0, -225458830.0, -225458850.0, -225458860.0, -225458880.0, -225458900.0, -225458910.0, -225458930.0, -225458940.0, -225458960.0, -225458980.0, -225458990.0, -225459000.0, -225459020.0, -225459040.0, -225459070.0, -225462400.0, -225462420.0, -225462430.0, -225462450.0, -225462460.0, -225462480.0, -225462500.0, -225462510.0, -225462530.0, -225462540.0, -225462560.0, -225462580.0, -225462600.0, -225462610.0, -225462620.0, -225462640.0, -225462660.0, -225462670.0, -225462690.0, -225462700.0, -225462720.0, -225462740.0, -225462750.0, -225462770.0, -225462780.0, -225462800.0, -225462820.0, -225462830.0, -225462850.0, -225462860.0, -225462880.0, -225462900.0, -225462910.0, -225462930.0, -225462940.0, -225462960.0, -225462980.0, -225462990.0, -225463000.0, -225463020.0, -225463040.0, -225463060.0, -225463070.0, -225463090.0, -225463100.0, -46532492.0, -46532500.0, -46532504.0, -46532508.0, -46532510.0, -46532516.0, -46532520.0, -46532524.0, -46532536.0, -46532540.0, -46532550.0, -46532556.0, -46532560.0, -46532564.0, -46532570.0, -46532572.0, -46532576.0, -46532580.0, -46532584.0, -46532588.0, -46532590.0, -46532596.0, -46532600.0, -46532604.0, -46532610.0, -46532612.0, -46532616.0, -46532620.0, -46532624.0, -46532628.0, -46532630.0, -46532636.0, -46532640.0, -46532644.0, -46532650.0, -46532652.0, -46532656.0, -46532660.0, -46532664.0, -46532668.0, -46532670.0, -46532676.0, -46532680.0, -46532684.0, -46532690.0, -46532692.0, -46532696.0, -46532700.0, -46532708.0, -46532716.0, -46532720.0, -46532724.0, -46532740.0, -46532750.0, -46535016.0, -46535024.0, -46535036.0, -46535044.0, -46535050.0, -46535052.0, -46535056.0, -46535060.0, -46535064.0, -46535068.0, -46535070.0, -46535076.0, -46535080.0, -46535084.0, -46535090.0, -46535092.0, -46535096.0, -46535100.0, -46535104.0, -46535108.0, -46535110.0, -46535116.0, -46535120.0, -46535124.0, -46535130.0, -46535132.0, -46535136.0, -46535140.0, -46535144.0, -46535148.0, -46535150.0, -46535156.0, -46535160.0, -46535164.0, -46535170.0, -46535172.0, -46535176.0, -46535180.0, -46535184.0, -46535188.0, -46535196.0, -46535210.0, -46536604.0, -46536636.0, -46536640.0, -46536644.0, -46536656.0, -46536660.0, -46536668.0, -46536670.0, -46536676.0, -46536680.0, -46536684.0, -46536690.0, -46536692.0, -46536696.0, -46536700.0, -46536704.0, -46536708.0, -46536710.0, -46536716.0, -46536720.0, -46536724.0, -46536730.0, -46536732.0, -46536736.0, -46536740.0, -46536744.0, -46536748.0, -46536750.0, -46536756.0, -46536760.0, -46536764.0, -46536770.0, -46536772.0, -46536776.0, -46536780.0, -46536784.0, -46536788.0, -46536790.0, -46536796.0, -46536800.0, -46536804.0, -46536810.0, -46536812.0, -46536816.0, -46536820.0, -46536824.0, -46536828.0, -46536830.0, -46536836.0, -46536840.0, -46536844.0, -46536850.0, -46536852.0, -46536856.0, -46536860.0, -46536864.0, -46536880.0, -46536900.0, -46537070.0, -46540364.0, -46540370.0, -46540372.0, -46540376.0, -46540380.0, -46540384.0, -46540388.0, -46540390.0, -46540396.0, -46540400.0, -46540404.0, -46540410.0, -46540412.0, -46540416.0, -46540420.0, -46540424.0, -46540428.0, -46540430.0, -46540436.0, -46540440.0, -46540444.0, -46540450.0, -46540452.0, -46540456.0, -46540460.0, -46540464.0, -46540468.0, -46540470.0, -46540476.0, -46540480.0, -46540484.0, -46540490.0, -46540492.0, -46540496.0, -46540508.0, -46540510.0, -46540540.0, -46540550.0, -46540560.0, -46540564.0, -46540570.0, -46540572.0, -46540576.0, -46540580.0, -46540584.0, -46540588.0, -46540590.0, -46540596.0, -46540600.0, -46540610.0, -46540612.0, -46540624.0, -46540628.0, -46540630.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8195420539560191\n",
      "Hamming Loss: 0.09215597370210836\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      3679\n",
      "           1       0.00      0.00      0.00       732\n",
      "\n",
      "    accuracy                           0.83      4411\n",
      "   macro avg       0.42      0.50      0.45      4411\n",
      "weighted avg       0.70      0.83      0.76      4411\n",
      "\n",
      "Train on 17642 samples\n",
      "Epoch 1/100\n",
      "17642/17642 [==============================] - 8s 434us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 2/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 3/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 4/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 5/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 6/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 7/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.1376 - binary_accuracy: 0.4076s - \n",
      "Epoch 8/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 9/100\n",
      "17642/17642 [==============================] - 2s 109us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 10/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 11/100\n",
      "17642/17642 [==============================] - 2s 103us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 12/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 13/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 14/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 44/100\n",
      "17642/17642 [==============================] - 2s 134us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 45/100\n",
      "17642/17642 [==============================] - 2s 133us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 46/100\n",
      "17642/17642 [==============================] - 2s 134us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 47/100\n",
      "17642/17642 [==============================] - 2s 130us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 48/100\n",
      "17642/17642 [==============================] - 2s 127us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 49/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 50/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 51/100\n",
      "17642/17642 [==============================] - 2s 99us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 52/100\n",
      "17642/17642 [==============================] - 2s 99us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 53/100\n",
      "17642/17642 [==============================] - 2s 95us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 54/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 55/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 56/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 57/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 58/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 59/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 60/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 61/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 62/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 63/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 64/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 65/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 66/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 67/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 68/100\n",
      "17642/17642 [==============================] - 2s 118us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 69/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 70/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 71/100\n",
      "17642/17642 [==============================] - 3s 160us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 72/100\n",
      "17642/17642 [==============================] - 2s 118us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 73/100\n",
      "17642/17642 [==============================] - 2s 123us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 74/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 75/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 76/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 77/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 78/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 79/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 80/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 81/100\n",
      "17642/17642 [==============================] - 2s 99us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 82/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 83/100\n",
      "17642/17642 [==============================] - 2s 102us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 84/100\n",
      "17642/17642 [==============================] - 2s 132us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 85/100\n",
      "17642/17642 [==============================] - 2s 133us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 86/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 87/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 88/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 89/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 90/100\n",
      "17642/17642 [==============================] - 2s 102us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 91/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 92/100\n",
      "17642/17642 [==============================] - 2s 124us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 93/100\n",
      "17642/17642 [==============================] - 2s 124us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 94/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 95/100\n",
      "17642/17642 [==============================] - 2s 125us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 96/100\n",
      "17642/17642 [==============================] - 2s 125us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 97/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 98/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 99/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n",
      "Epoch 100/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 9.1376 - binary_accuracy: 0.4076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-130608260.0, -130608270.0, -130608280.0, -130608290.0, -130608296.0, -130608300.0, -130608310.0, -130608320.0, -130608330.0, -130608340.0, -130608344.0, -130608350.0, -130608360.0, -130608370.0, -130608376.0, -130608380.0, -130608390.0, -130608400.0, -130608410.0, -130608420.0, -130608424.0, -130608430.0, -130608440.0, -130608450.0, -130608456.0, -130608460.0, -130608470.0, -130608480.0, -130608490.0, -130608500.0, -130608510.0, -130608540.0, -130608880.0, -130608900.0, -130608910.0, -130608930.0, -130608940.0, -130608950.0, -130608960.0, -130608970.0, -130608980.0, -130608984.0, -130608990.0, -130609000.0, -130609010.0, -130609016.0, -130609020.0, -130609030.0, -130609040.0, -130609050.0, -130609060.0, -130609064.0, -130609070.0, -130609080.0, -130609090.0, -130609096.0, -130609100.0, -130609110.0, -130609120.0, -130609130.0, -130609140.0, -130609144.0, -130609150.0, -130609160.0, -130609170.0, -130609176.0, -130609180.0, -130609190.0, -130609200.0, -130609210.0, -130609220.0, -130609224.0, -130609230.0, -130609240.0, -130609250.0, -130609256.0, -130609260.0, -130609270.0, -130609280.0, -130609290.0, -130609300.0, -130609304.0, -130609310.0, -130609330.0, -130609336.0, -130609340.0, -130609350.0, -130609370.0, -130609380.0, -130609384.0, -130609390.0, -130609400.0, -130609410.0, -130609416.0, -130609420.0, -130609430.0, -130609440.0, -130609680.0, -130609700.0, -130609730.0, -130609736.0, -130609740.0, -130609750.0, -130609760.0, -130609770.0, -130609780.0, -130609784.0, -130609790.0, -130609820.0, -130609864.0, -130609940.0, -130609944.0, -130609950.0, -130609970.0, -130609976.0, -130609980.0, -130609990.0, -130610000.0, -130610010.0, -130610020.0, -130610024.0, -130610030.0, -130610040.0, -130610050.0, -130610056.0, -130610060.0, -130610070.0, -130610080.0, -130610090.0, -130610100.0, -130610104.0, -130610110.0, -130610120.0, -130610130.0, -130610136.0, -42533724.0, -42533732.0, -42533736.0, -42533740.0, -42533744.0, -42533748.0, -42533750.0, -42533756.0, -42533760.0, -42533764.0, -42533770.0, -42533772.0, -42533776.0, -42533780.0, -42533784.0, -42533788.0, -42533790.0, -42533796.0, -42533800.0, -42533804.0, -42533810.0, -42533812.0, -42533816.0, -42533820.0, -42533824.0, -42533828.0, -42533830.0, -42533836.0, -42533840.0, -42533844.0, -42533850.0, -42533852.0, -42533856.0, -42533860.0, -42533864.0, -42533868.0, -42533870.0, -42533876.0, -42533880.0, -42533884.0, -42533890.0, -42533892.0, -42533896.0, -42533900.0, -42533904.0, -42533908.0, -42533910.0, -42533916.0, -42533920.0, -42533924.0, -42533930.0, -42533932.0, -42533936.0, -42533940.0, -42533944.0, -42533948.0, -42533950.0, -42533956.0, -42533960.0, -42533964.0, -42533970.0, -42533972.0, -42533976.0, -42533980.0, -42533984.0, -42538576.0, -42538588.0, -42538600.0, -42538610.0, -42538612.0, -42538616.0, -42538620.0, -42538624.0, -42538628.0, -42538630.0, -42538640.0, -42538644.0, -42538650.0, -42538652.0, -42538656.0, -42538660.0, -42538664.0, -42538668.0, -42538670.0, -42538676.0, -42538680.0, -42538684.0, -42538690.0, -42538692.0, -42538696.0, -42538700.0, -42538704.0, -42538708.0, -42538710.0, -42538716.0, -42538720.0, -42538724.0, -42538730.0, -42538732.0, -42538736.0, -42538740.0, -42538744.0, -42538748.0, -42538750.0, -42538756.0, -42538760.0, -42538764.0, -42538770.0, -42538772.0, -42538776.0, -42538780.0, -42538784.0, -42538788.0, -42538790.0, -42538796.0, -42538800.0, -42538804.0, -42538810.0, -42538812.0, -42538816.0, -42538820.0, -42538824.0, -42538828.0, -42538830.0, -42538836.0, -42538840.0, -42538844.0, -42538850.0, -42538852.0, -42538856.0, -42538860.0, -42538864.0, -42538868.0, -42538870.0, -42538876.0, -42538880.0, -42538884.0, -42538890.0, -42538892.0, -42538896.0, -42538900.0, -42538904.0, -42538908.0, -42538910.0, -42538916.0, -42538920.0, -42538924.0, -42538930.0, -42538932.0, -42538936.0, -42538940.0, -42538944.0, -42538948.0, -42538950.0, -42538956.0, -42538960.0, -42538970.0, -42538972.0, -42538984.0, -42539110.0, -42539116.0, -42539124.0, -42539130.0, -42539132.0, -42539136.0, -42539140.0, -42539144.0, -42539150.0, -42539156.0, -42539160.0, -42539170.0, -42539176.0, -42539184.0, -42539188.0, -42539190.0, -42539210.0, -42539212.0, -42539216.0, -42539220.0, -42539224.0, -42539228.0, -42539230.0, -42539236.0, -42539240.0, -42539244.0, -42539250.0, -42539252.0, -42539256.0, -42539260.0, -42539264.0, -42539268.0, -42539270.0, -42539276.0, -42539280.0, -42539284.0, -42539290.0, -42539292.0, -42539296.0, -42539300.0, -42539304.0, -42539308.0, -42539310.0, -42539316.0, -42539320.0, -42539324.0, -42539330.0, -42539332.0, -42539336.0, -42539340.0, -42539344.0, -42539348.0, -42539350.0, -42539356.0, -42539360.0, -42539364.0, -42539370.0, -42539372.0, -42539376.0, -42539380.0, -42539384.0, -42539388.0, -42539390.0, -42539396.0, -42539400.0, -42539404.0, -42539410.0, -42539412.0, -42539416.0, -42539420.0, -42539424.0, -42539428.0, -42539430.0, -42539436.0, -42539440.0, -42539444.0, -42539450.0, -42539452.0, -42539456.0, -42539460.0, -42539464.0, -42539468.0, -42539470.0, -42539476.0, -42539480.0, -42539484.0, -42539490.0, -42539492.0, -42539496.0, -42539500.0, -42539504.0, -42539508.0, -42539510.0, -42539516.0, -42539520.0, -42539524.0, -42539530.0, -42539532.0, -42539536.0, -42539540.0, -42539544.0, -42539548.0, -42539550.0, -42539556.0, -42539560.0, -42539564.0, -42539570.0, -42539572.0, -42539576.0, -42539580.0, -42539584.0, -42539588.0, -42539590.0, -42539596.0, -42539600.0, -42539604.0, -42539610.0, -42539612.0, -42539616.0, -42539620.0, -42539624.0, -42539628.0, -42539630.0, -42539636.0, -42539640.0, -42539644.0, -42539650.0, -42539652.0, -42539656.0, -42539660.0, -42539664.0, -42539668.0, -42539670.0, -42539676.0, -42539680.0, -42539684.0, -42539690.0, -42539692.0, -42539696.0, -42539700.0, -42539704.0, -42539708.0, -42539710.0, -42539716.0, -42539720.0, -42539724.0, -42539730.0, -42539732.0, -42539736.0, -42539740.0, -42539744.0, -42539748.0, -42539750.0, -42539756.0, -42539760.0, -42539764.0, -42539770.0, -42539772.0, -42539776.0, -42539780.0, -42539784.0, -42539788.0, -42539790.0, -42539800.0, -42539804.0, -42540900.0, -42540904.0, -42540910.0, -42540920.0, -42540930.0, -42540936.0, -42540940.0, -42540948.0, -42540950.0, -42540956.0, -42540960.0, -42540964.0, -42540970.0, -42540972.0, -42540976.0, -42540980.0, -42540984.0, -42540988.0, -42540990.0, -42540996.0, -42541000.0, -42541004.0, -42541010.0, -42541012.0, -42541016.0, -42541020.0, -42541024.0, -42541028.0, -42541030.0, -42541036.0, -42541040.0, -42541044.0, -42541050.0, -42541052.0, -42541056.0, -42541060.0, -42541064.0, -42541068.0, -42541070.0, -42541076.0, -42541080.0, -42541084.0, -42541090.0, -42541092.0, -42541096.0, -42541100.0, -42541104.0, -42541108.0, -42541110.0, -42541116.0, -42541120.0, -42541124.0, -42541130.0, -42541132.0, -42541136.0, -42541140.0, -42541144.0, -42541148.0, -42541150.0, -42541156.0, -42541160.0, -42541164.0, -42541170.0, -42541172.0, -42541176.0, -42541180.0, -42541184.0, -42541188.0, -42541190.0, -42541196.0, -42541200.0, -42541204.0, -42541210.0, -42541212.0, -42541216.0, -42541220.0, -42541224.0, -42541228.0, -42541230.0, -42541236.0, -42541240.0, -42541244.0, -42541250.0, -42541256.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7250056676490592\n",
      "Hamming Loss: 0.14883246429381092\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      3717\n",
      "           1       0.00      0.00      0.00       694\n",
      "\n",
      "    accuracy                           0.84      4411\n",
      "   macro avg       0.42      0.50      0.46      4411\n",
      "weighted avg       0.71      0.84      0.77      4411\n",
      "\n",
      "Train on 17642 samples\n",
      "Epoch 1/100\n",
      "17642/17642 [==============================] - 3s 165us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 2/100\n",
      "17642/17642 [==============================] - 2s 109us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 3/100\n",
      "17642/17642 [==============================] - 2s 109us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 4/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 5/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 6/100\n",
      "17642/17642 [==============================] - 2s 107us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 7/100\n",
      "17642/17642 [==============================] - 3s 143us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 8/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 9/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 10/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 11/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 12/100\n",
      "17642/17642 [==============================] - 2s 124us/sample - loss: 1.6748 - binary_accuracy: 0.8914s \n",
      "Epoch 13/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 14/100\n",
      "17642/17642 [==============================] - 2s 109us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 15/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 16/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 17/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 18/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 19/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 20/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 21/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 22/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 23/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 24/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 25/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 26/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 27/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 28/100\n",
      "17642/17642 [==============================] - ETA: 0s - loss: 1.6738 - binary_accuracy: 0.891 - 2s 99us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 29/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 30/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 31/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 32/100\n",
      "17642/17642 [==============================] - 2s 107us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 33/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 34/100\n",
      "17642/17642 [==============================] - 2s 97us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 35/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 36/100\n",
      "17642/17642 [==============================] - 2s 99us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 37/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 38/100\n",
      "17642/17642 [==============================] - 2s 125us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 39/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 40/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 41/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 42/100\n",
      "17642/17642 [==============================] - 2s 106us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 43/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 44/100\n",
      "17642/17642 [==============================] - 2s 103us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 45/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 46/100\n",
      "17642/17642 [==============================] - 2s 136us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 47/100\n",
      "17642/17642 [==============================] - 2s 134us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 48/100\n",
      "17642/17642 [==============================] - 2s 123us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 49/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 50/100\n",
      "17642/17642 [==============================] - 2s 128us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 51/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 52/100\n",
      "17642/17642 [==============================] - 2s 118us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 53/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 54/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 55/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 56/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 57/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 58/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 59/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 60/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 61/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 62/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 63/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 64/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 65/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 66/100\n",
      "17642/17642 [==============================] - 2s 106us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 67/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 68/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 69/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 70/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 71/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 72/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 73/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 74/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 75/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 76/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 77/100\n",
      "17642/17642 [==============================] - 2s 106us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 78/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 79/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 80/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 81/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 82/100\n",
      "17642/17642 [==============================] - 2s 103us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 83/100\n",
      "17642/17642 [==============================] - 2s 107us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 84/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 85/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 86/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 87/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 88/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 89/100\n",
      "17642/17642 [==============================] - 2s 106us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 90/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 91/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 92/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 93/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 94/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 95/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 96/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 97/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 98/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 99/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n",
      "Epoch 100/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 1.6748 - binary_accuracy: 0.8914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-77110060.0, -77110100.0, -77110180.0, -77110190.0, -77110200.0, -77110210.0, -77110216.0, -77110220.0, -77110230.0, -77110240.0, -77110250.0, -77110260.0, -77110264.0, -77110270.0, -77110280.0, -77110290.0, -77110296.0, -77110300.0, -77110310.0, -77110320.0, -77110330.0, -77110340.0, -77110344.0, -77110350.0, -77110360.0, -77110370.0, -77110376.0, -77110380.0, -77110390.0, -77110400.0, -77110410.0, -77110420.0, -77110424.0, -77110430.0, -77110440.0, -77110450.0, -77110456.0, -77110460.0, -77110470.0, -77110480.0, -77110490.0, -77110500.0, -77110504.0, -77110510.0, -77110520.0, -77110530.0, -77110536.0, -77110540.0, -77110550.0, -77110560.0, -77110570.0, -77110580.0, -77110584.0, -77110590.0, -77110600.0, -77110610.0, -77110616.0, -77110620.0, -77110630.0, -77110640.0, -77110650.0, -77110660.0, -77110664.0, -77110670.0, -77110680.0, -77110690.0, -77110696.0, -77110700.0, -77110710.0, -77110720.0, -77110730.0, -77110740.0, -77110744.0, -77110750.0, -77110760.0, -77110770.0, -77110776.0, -77110780.0, -77110790.0, -77110800.0, -77110810.0, -77110820.0, -77110824.0, -77110830.0, -77110840.0, -77110850.0, -77110856.0, -77110860.0, -77110870.0, -77111090.0, -77111100.0, -77111110.0, -77111120.0, -77111140.0, -77111144.0, -77111150.0, -77111160.0, -77111170.0, -77111176.0, -77111180.0, -77111190.0, -77111200.0, -77111210.0, -77111220.0, -77111224.0, -77111230.0, -77111240.0, -77111256.0, 55203024.0, 55203110.0, 55203130.0, 55203236.0, 55203250.0, 55203260.0, 55203264.0, 55203270.0, 55203276.0, 55203280.0, 55203284.0, 55203290.0, 55203292.0, 55203296.0, 55203300.0, 55203304.0, 55203308.0, 55203310.0, 55203316.0, 55203320.0, 55203324.0, 55203330.0, 55203332.0, 55203336.0, 55203340.0, 55203344.0, 55203348.0, 55203350.0, 55203356.0, 55203360.0, 55203364.0, 55203370.0, 55203372.0, 55203376.0, 55203380.0, 55203384.0, 55203388.0, 55203390.0, 55203396.0, 55203400.0, 55203404.0, 55203410.0, 55203412.0, 55203416.0, 55203420.0, 55203424.0, 55203428.0, 55203430.0, 55203436.0, 55203440.0, 55203444.0, 55203450.0, 55203452.0, 55203456.0, 55203460.0, 55203464.0, 55203468.0, 55203470.0, 55203476.0, 55203480.0, 55203484.0, 55203490.0, 55203492.0, 55203496.0, 55203500.0, 55203504.0, 55203508.0, 55203510.0, 55203516.0, 55203520.0, 55203524.0, 55203530.0, 55203532.0, 55203540.0, 55203548.0, 55204296.0, 55204300.0, 55204308.0, 55204310.0, 55204316.0, 55204320.0, 55204324.0, 55204330.0, 55204332.0, 55204336.0, 55204340.0, 55204344.0, 55204348.0, 55204350.0, 55204356.0, 55204360.0, 55204364.0, 55204370.0, 55204372.0, 55204376.0, 55204380.0, 55204384.0, 55204388.0, 55204390.0, 55204396.0, 55204400.0, 55204404.0, 55204410.0, 55204412.0, 55204416.0, 55204420.0, 55204424.0, 55204428.0, 55204430.0, 55204436.0, 55204440.0, 55204444.0, 55204450.0, 55204452.0, 55204456.0, 55204460.0, 55204464.0, 55204468.0, 55204470.0, 55204476.0, 55204480.0, 55204484.0, 55204490.0, 55204492.0, 55204496.0, 55204500.0, 55204504.0, 55204508.0, 55204510.0, 55204516.0, 55204520.0, 55204524.0, 55204530.0, 55204532.0, 55204536.0, 55204540.0, 55204544.0, 55204548.0, 55204556.0, 55204560.0, 55204564.0, 55204572.0, 55204580.0, 55204670.0, 55204680.0, 55204690.0, 55204692.0, 55204696.0, 55204700.0, 55204704.0, 55204708.0, 55204710.0, 55204716.0, 55204720.0, 55204724.0, 55204730.0, 55204732.0, 55204736.0, 55204740.0, 55204744.0, 55204748.0, 55204750.0, 55204756.0, 55204760.0, 55204764.0, 55204770.0, 55204772.0, 55204776.0, 55204780.0, 55204784.0, 55204788.0, 55204790.0, 55204796.0, 55204800.0, 55204804.0, 55204810.0, 55204812.0, 55204816.0, 55204820.0, 55204824.0, 55204828.0, 55204830.0, 55204836.0, 55204840.0, 55204844.0, 55204852.0, 55204856.0, 55204860.0, 55204864.0, 55204868.0, 55204870.0, 55204876.0, 55204880.0, 55204884.0, 55204890.0, 55204892.0, 55204896.0, 55204900.0, 55204904.0, 55204908.0, 55204910.0, 55204916.0, 55204920.0, 55204924.0, 55204930.0, 55204932.0, 55204936.0, 55204940.0, 55204944.0, 55204948.0, 55204950.0, 55204956.0, 55204960.0, 55204964.0, 55204970.0, 55204972.0, 55204976.0, 55204980.0, 55204984.0, 55204988.0, 55204990.0, 55204996.0, 55205000.0, 55205004.0, 55205010.0, 55205012.0, 55205016.0, 55205020.0, 55205024.0, 55205028.0, 55205030.0, 55205036.0, 55205040.0, 55205044.0, 55205050.0, 55205052.0, 55205056.0, 55205060.0, 55205064.0, 55205068.0, 55205076.0, 55205080.0, 55205092.0, 55205200.0, 55205210.0, 55205216.0, 55205220.0, 55205224.0, 55205228.0, 55205230.0, 55205236.0, 55205240.0, 55205244.0, 55205250.0, 55205252.0, 55205256.0, 55205260.0, 55205264.0, 55205268.0, 55205270.0, 55205276.0, 55205280.0, 55205284.0, 55205290.0, 55205292.0, 55205296.0, 55205300.0, 55205304.0, 55205308.0, 55205316.0, 55205320.0, 55205340.0, 55205348.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7975515756064384\n",
      "Hamming Loss: 0.1062117433688506\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3740\n",
      "           1       0.00      0.00      0.00       671\n",
      "\n",
      "    accuracy                           0.85      4411\n",
      "   macro avg       0.42      0.50      0.46      4411\n",
      "weighted avg       0.72      0.85      0.78      4411\n",
      "\n",
      "Train on 17643 samples\n",
      "Epoch 1/100\n",
      "17643/17643 [==============================] - 2s 134us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 2/100\n",
      "17643/17643 [==============================] - 2s 105us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 3/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 4/100\n",
      "17643/17643 [==============================] - 2s 106us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 5/100\n",
      "17643/17643 [==============================] - 2s 121us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 6/100\n",
      "17643/17643 [==============================] - 2s 115us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 7/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 8/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 9/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 10/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 11/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 12/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 13/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 14/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 15/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 16/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 17/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 18/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 19/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 20/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 21/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 22/100\n",
      "17643/17643 [==============================] - 3s 148us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 23/100\n",
      "17643/17643 [==============================] - 2s 106us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 24/100\n",
      "17643/17643 [==============================] - 2s 102us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 25/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 26/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 27/100\n",
      "17643/17643 [==============================] - 2s 103us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 28/100\n",
      "17643/17643 [==============================] - 2s 106us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 29/100\n",
      "17643/17643 [==============================] - 2s 103us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 30/100\n",
      "17643/17643 [==============================] - 2s 102us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 31/100\n",
      "17643/17643 [==============================] - 3s 143us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 32/100\n",
      "17643/17643 [==============================] - 2s 124us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 33/100\n",
      "17643/17643 [==============================] - 3s 194us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 34/100\n",
      "17643/17643 [==============================] - 2s 101us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 35/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 36/100\n",
      "17643/17643 [==============================] - 4s 232us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 37/100\n",
      "17643/17643 [==============================] - 3s 186us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 38/100\n",
      "17643/17643 [==============================] - 2s 138us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 39/100\n",
      "17643/17643 [==============================] - 2s 136us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 40/100\n",
      "17643/17643 [==============================] - 3s 146us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 41/100\n",
      "17643/17643 [==============================] - 3s 151us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 42/100\n",
      "17643/17643 [==============================] - 2s 134us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 82/100\n",
      "17643/17643 [==============================] - 2s 130us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 83/100\n",
      "17643/17643 [==============================] - 2s 133us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 84/100\n",
      "17643/17643 [==============================] - 2s 121us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 85/100\n",
      "17643/17643 [==============================] - 3s 152us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 86/100\n",
      "17643/17643 [==============================] - 3s 147us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 87/100\n",
      "17643/17643 [==============================] - 2s 106us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 88/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 89/100\n",
      "17643/17643 [==============================] - 3s 146us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 90/100\n",
      "17643/17643 [==============================] - 2s 126us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 91/100\n",
      "17643/17643 [==============================] - 2s 101us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 92/100\n",
      "17643/17643 [==============================] - 2s 106us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 93/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 94/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 9.2713 - binary_accuracy: 0.3989s - loss: 9.2743 - binary_accuracy: 0\n",
      "Epoch 95/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 96/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 97/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 98/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 99/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n",
      "Epoch 100/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 9.2713 - binary_accuracy: 0.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-40140864.0, -40140870.0, -40140880.0, -40140890.0, -40140896.0, -40140904.0, -40140910.0, -40140920.0, -40140930.0, -40140936.0, -40140944.0, -40140950.0, -40140960.0, -40140970.0, -40140976.0, -40140984.0, -40140990.0, -40141000.0, -40141010.0, -40141016.0, -40141024.0, -40141030.0, -40141040.0, -40141056.0, -40142810.0, -40142816.0, -40142824.0, -40142830.0, -40142850.0, -40142856.0, -40142864.0, -40142870.0, -40142880.0, -40142890.0, -40142896.0, -40142904.0, -40142910.0, -40142920.0, -40142930.0, -40142936.0, -40142944.0, -40142950.0, -40142960.0, -40142970.0, -40142976.0, -40142984.0, -40142990.0, -40143000.0, -40143010.0, -40143016.0, -40143024.0, -40143030.0, -40143040.0, -40143050.0, -40143056.0, -40144704.0, -40144710.0, -40144720.0, -40144730.0, -40144736.0, -40144744.0, -40144750.0, -40144760.0, -40144770.0, -40144776.0, -40144784.0, -40144800.0, -40144810.0, -40144816.0, -40144824.0, -40144830.0, -40144840.0, -40144850.0, -40144856.0, -40144864.0, -40144870.0, -40144880.0, -40144890.0, -40144896.0, -40144904.0, -40144910.0, -40144920.0, -40144930.0, -40144936.0, -40144944.0, -40144950.0, -40144960.0, -40144970.0, -40144976.0, -40144984.0, -40144990.0, -40145000.0, -40145010.0, -40145016.0, -40145024.0, -40145030.0, -40145040.0, -40145050.0, -40145056.0, -40145064.0, -40145070.0, -40149256.0, -40149264.0, -40149270.0, -40149280.0, -40149290.0, -40149296.0, -40149304.0, -40149310.0, -40149320.0, -40149330.0, -40149336.0, -40149344.0, -40149350.0, -40149360.0, -40149370.0, -40149376.0, -40149384.0, -40149390.0, -40149400.0, -40149410.0, -40149416.0, -40149424.0, -40149430.0, -40149440.0, -40149450.0, -6805128.0, -6805129.0, -6805136.0, -6805137.0, -6805140.0, -6805141.0, -6805142.0, -6805144.0, -6805145.0, -6805146.0, -6805147.0, -6805148.0, -6805149.0, -6805150.0, -6805151.0, -6805152.0, -6805153.0, -6805154.0, -6805155.0, -6805156.0, -6805157.0, -6805158.0, -6805159.0, -6805160.0, -6805161.0, -6805162.0, -6805163.0, -6805164.0, -6805165.0, -6805166.0, -6805167.0, -6805168.0, -6805169.0, -6805170.0, -6805171.0, -6805172.0, -6805173.0, -6805174.0, -6805175.0, -6805176.0, -6805177.0, -6805178.0, -6805179.0, -6805180.0, -6805181.0, -6805182.0, -6805183.0, -6805184.0, -6805185.0, -6805186.0, -6805187.0, -6805188.0, -6805189.0, -6805190.0, -6805191.0, -6805192.0, -6805193.0, -6805194.0, -6805195.0, -6805196.0, -6805197.0, -6805198.0, -6805199.0, -6805200.0, -6805201.0, -6805202.0, -6805203.0, -6805204.0, -6805205.0, -6805206.0, -6805207.0, -6805208.0, -6805209.0, -6805210.0, -6805211.0, -6805212.0, -6805213.0, -6805214.0, -6805215.0, -6805216.0, -6805217.0, -6805218.0, -6805219.0, -6805220.0, -6805221.0, -6805222.0, -6805223.0, -6805224.0, -6805225.0, -6805226.0, -6805227.0, -6805228.0, -6805229.0, -6805230.0, -6805231.0, -6805232.0, -6805233.0, -6805234.0, -6805235.0, -6805236.0, -6805237.0, -6805238.0, -6805239.0, -6805240.0, -6805242.0, -6805243.0, -6805244.0, -6805245.0, -6805246.0, -6805247.0, -6805248.0, -6805249.0, -6805250.0, -6805251.0, -6805253.0, -6805254.0, -6805256.0, -6805257.0, -6805259.0, -6805260.0, -6805262.0, -6805265.0, -6805267.0, -6805271.0, -6805277.0, -6805279.0, -6805284.0, -6805286.0, -6805292.0, -6805319.0, -6805584.0, -6805586.0, -6805588.0, -6805590.0, -6805591.0, -6805593.0, -6805594.0, -6805595.0, -6805596.0, -6805597.0, -6805598.0, -6805600.0, -6805601.0, -6805603.0, -6805604.0, -6805605.0, -6805606.0, -6805607.0, -6805608.0, -6805609.0, -6805610.0, -6805611.0, -6805612.0, -6805614.0, -6805616.0, -6805617.0, -6805618.0, -6805619.0, -6805620.0, -6805621.0, -6805622.0, -6805623.0, -6805624.0, -6805625.0, -6805627.0, -6805628.0, -6805629.0, -6805630.0, -6805631.0, -6805632.0, -6805633.0, -6805634.0, -6805635.0, -6805636.0, -6805637.0, -6805638.0, -6805639.0, -6805640.0, -6805641.0, -6805642.0, -6805643.0, -6805644.0, -6805645.0, -6805646.0, -6805647.0, -6805648.0, -6805649.0, -6805650.0, -6805651.0, -6805652.0, -6805653.0, -6805654.0, -6805655.0, -6805656.0, -6805657.0, -6805658.0, -6805659.0, -6805660.0, -6805661.0, -6805662.0, -6805663.0, -6805664.0, -6805665.0, -6805666.0, -6805667.0, -6805668.0, -6805669.0, -6805670.0, -6805671.0, -6805672.0, -6805673.0, -6805674.0, -6805675.0, -6805676.0, -6805677.0, -6805678.0, -6805679.0, -6805680.0, -6805681.0, -6805682.0, -6805683.0, -6805684.0, -6805685.0, -6805686.0, -6805687.0, -6805688.0, -6805689.0, -6805690.0, -6805691.0, -6805692.0, -6805693.0, -6805694.0, -6805695.0, -6805696.0, -6805697.0, -6805698.0, -6805699.0, -6805700.0, -6805701.0, -6805702.0, -6805703.0, -6805704.0, -6805705.0, -6805706.0, -6805707.0, -6805708.0, -6805710.0, -6805711.0, -6805712.0, -6805713.0, -6805714.0, -6805715.0, -6805716.0, -6805717.0, -6805718.0, -6805719.0, -6805720.0, -6805722.0, -6805723.0, -6805724.0, -6805725.0, -6805726.0, -6805727.0, -6805728.0, -6805730.0, -6805731.0, -6805732.0, -6805733.0, -6805734.0, -6805735.0, -6805736.0, -6805737.0, -6805740.0, -6805741.0, -6805742.0, -6805743.0, -6805744.0, -6805745.0, -6805746.0, -6805747.0, -6805748.0, -6805749.0, -6805750.0, -6805751.0, -6805752.0, -6805753.0, -6805754.0, -6805755.0, -6805756.0, -6805757.0, -6805758.0, -6805759.0, -6805760.0, -6805761.0, -6805762.0, -6805763.0, -6805764.0, -6805765.0, -6805766.0, -6805767.0, -6805769.0, -6805770.0, -6805771.0, -6805772.0, -6805773.0, -6805774.0, -6805775.0, -6805776.0, -6805777.0, -6805779.0, -6805780.0, -6805781.0, -6805782.0, -6805783.0, -6805784.0, -6805785.0, -6805786.0, -6805787.0, -6805788.0, -6805789.0, -6805790.0, -6805792.0, -6805793.0, -6805794.0, -6805795.0, -6805796.0, -6805797.0, -6805798.0, -6805799.0, -6805800.0, -6805801.0, -6805802.0, -6805803.0, -6805805.0, -6805806.0, -6805807.0, -6805808.0, -6805810.0, -6805811.0, -6805812.0, -6805815.0, -6805816.0, -6805817.0, -6805818.0, -6805819.0, -6805820.0, -6805821.0, -6805823.0, -6805824.0, -6805826.0, -6805828.0, -6805830.0, -6805832.0, -6805833.0, -6805834.0, -6805835.0, -6805838.0, -6805839.0, -6805840.0, -6805842.0, -6805843.0, -6805846.0, -6805847.0, -6805848.0, -6805849.0, -6805850.0, -6805851.0, -6805852.0, -6805853.0, -6805854.0, -6805855.0, -6805857.0, -6805860.0, -6805861.0, -6805864.0, -6805866.0, -6805868.0, -6805871.0, -6805872.0, -6805874.0, -6805875.0, -6805876.0, -6805877.0, -6805878.0, -6805879.0, -6805880.0, -6805888.0, -6805890.0, -6805892.0, -6805894.0, -6805896.0, -6805898.0, -6805899.0, -6805900.0, -6805901.0, -6805906.0, -6805907.0, -6805910.0, -6805912.0, -6805914.0, -6805915.0, -6805918.0, -6805919.0, -6805920.0, -6805926.0, -6805928.0, -6805932.0, -6805933.0, -6805936.0, -6805938.0, -6805940.0, -6805941.0, -6805948.0, -6805949.0, -6805952.0, -6805954.0, -6805962.0, -6805968.0, -6805973.0, -6805977.0, -6805978.0, -6805980.0, -6805981.0, -6805995.0, -6805997.0, -6805998.0, -6806000.0, -6806009.0, -6806040.0, -6806060.0, -6806063.0, -6806064.0, -6806065.0, -6806067.0, -6806070.0, -6806079.0, -6806086.0, -6806108.0, -6806116.0, -6806126.0, -6806131.0, -6806152.0, -6806161.0, -6806190.0, -6806194.0, -6806219.0, -6806220.0, -6806229.0, -6806290.0, -6806293.0, -6806390.0, -6806391.0, -6806392.0, -6806394.0, -6806395.0, -6806396.0, -6806397.0, -6806398.0, -6806399.0, -6806400.0, -6806401.0, -6806402.0, -6806404.0, -6806405.0, -6806406.0, -6806407.0, -6806408.0, -6806409.0, -6806410.0, -6806411.0, -6806412.0, -6806413.0, -6806414.0, -6806415.0, -6806416.0, -6806417.0, -6806418.0, -6806419.0, -6806420.0, -6806421.0, -6806422.0, -6806423.0, -6806424.0, -6806425.0, -6806426.0, -6806427.0, -6806428.0, -6806429.0, -6806430.0, -6806431.0, -6806432.0, -6806433.0, -6806434.0, -6806435.0, -6806436.0, -6806437.0, -6806438.0, -6806439.0, -6806440.0, -6806441.0, -6806442.0, -6806443.0, -6806444.0, -6806445.0, -6806446.0, -6806447.0, -6806448.0, -6806449.0, -6806450.0, -6806451.0, -6806452.0, -6806453.0, -6806454.0, -6806455.0, -6806456.0, -6806457.0, -6806458.0, -6806459.0, -6806460.0, -6806461.0, -6806462.0, -6806463.0, -6806464.0, -6806465.0, -6806466.0, -6806467.0, -6806468.0, -6806469.0, -6806470.0, -6806471.0, -6806472.0, -6806473.0, -6806474.0, -6806475.0, -6806476.0, -6806477.0, -6806478.0, -6806479.0, -6806480.0, -6806481.0, -6806482.0, -6806483.0, -6806484.0, -6806485.0, -6806486.0, -6806487.0, -6806488.0, -6806489.0, -6806490.0, -6806491.0, -6806492.0, -6806493.0, -6806494.0, -6806495.0, -6806496.0, -6806497.0, -6806498.0, -6806499.0, -6806500.0, -6806501.0, -6806502.0, -6806503.0, -6806504.0, -6806505.0, -6806506.0, -6806507.0, -6806508.0, -6806509.0, -6806510.0, -6806511.0, -6806512.0, -6806513.0, -6806514.0, -6806515.0, -6806516.0, -6806517.0, -6806518.0, -6806519.0, -6806520.0, -6806521.0, -6806522.0, -6806523.0, -6806524.0, -6806525.0, -6806526.0, -6806527.0, -6806528.0, -6806529.0, -6806530.0, -6806531.0, -6806532.0, -6806533.0, -6806534.0, -6806535.0, -6806536.0, -6806537.0, -6806538.0, -6806539.0, -6806540.0, -6806541.0, -6806542.0, -6806543.0, -6806544.0, -6806546.0, -6806547.0, -6806548.0, -6806549.0, -6806550.0, -6806551.0, -6806552.0, -6806553.0, -6806554.0, -6806555.0, -6806556.0, -6806557.0, -6806558.0, -6806559.0, -6806560.0, -6806561.0, -6806562.0, -6806563.0, -6806564.0, -6806565.0, -6806566.0, -6806567.0, -6806568.0, -6806569.0, -6806570.0, -6806571.0, -6806572.0, -6806573.0, -6806574.0, -6806575.0, -6806576.0, -6806577.0, -6806578.0, -6806579.0, -6806580.0, -6806581.0, -6806582.0, -6806583.0, -6806584.0, -6806586.0, -6806588.0, -6806589.0, -6806590.0, -6806591.0, -6806592.0, -6806593.0, -6806595.0, -6806596.0, -6806597.0, -6806598.0, -6806599.0, -6806601.0, -6806602.0, -6806603.0, -6806604.0, -6806606.0, -6806607.0, -6806609.0, -6806610.0, -6806611.0, -6806612.0, -6806613.0, -6806614.0, -6806618.0, -6806619.0, -6806620.0, -6806622.0, -6806623.0, -6806624.0, -6806625.0, -6806627.0, -6806628.0, -6806630.0, -6806631.0, -6806632.0, -6806633.0, -6806634.0, -6806635.0, -6806636.0, -6806639.0, -6806641.0, -6806642.0, -6806646.0, -6806648.0, -6806649.0, -6806653.0, -6806654.0, -6806655.0, -6806656.0, -6806658.0, -6806662.0, -6806663.0, -6806665.0, -6806666.0, -6806670.0, -6806671.0, -6806672.0, -6806673.0, -6806676.0, -6806679.0, -6806689.0, -6806695.0, -6806696.0, -6806697.0, -6806699.0, -6806703.0, -6806707.0, -6806712.0, -6806721.0, -6806732.0, -6806747.0, -6806750.0, -6806754.0, -6806764.0, -6806778.0, -6806788.0, -6806868.0, -6808702.0, -6808704.0, -6808708.0, -6808709.0, -6808714.0, -6808715.0, -6808718.0, -6808719.0, -6808720.0, -6808721.0, -6808722.0, -6808723.0, -6808724.0, -6808725.0, -6808726.0, -6808727.0, -6808728.0, -6808729.0, -6808730.0, -6808731.0, -6808732.0, -6808733.0, -6808734.0, -6808735.0, -6808736.0, -6808737.0, -6808738.0, -6808739.0, -6808740.0, -6808741.0, -6808742.0, -6808743.0, -6808744.0, -6808745.0, -6808746.0, -6808747.0, -6808748.0, -6808749.0, -6808750.0, -6808751.0, -6808752.0, -6808753.0, -6808754.0, -6808755.0, -6808756.0, -6808757.0, -6808758.0, -6808759.0, -6808760.0, -6808761.0, -6808762.0, -6808763.0, -6808764.0, -6808765.0, -6808766.0, -6808767.0, -6808768.0, -6808769.0, -6808770.0, -6808771.0, -6808772.0, -6808773.0, -6808774.0, -6808775.0, -6808776.0, -6808777.0, -6808778.0, -6808779.0, -6808780.0, -6808781.0, -6808782.0, -6808783.0, -6808784.0, -6808785.0, -6808786.0, -6808787.0, -6808788.0, -6808789.0, -6808790.0, -6808791.0, -6808792.0, -6808793.0, -6808794.0, -6808795.0, -6808798.0, -6808799.0, -6808800.0, -6808801.0, -6808802.0, -6808803.0, -6808804.0, -6808805.0, -6808806.0, -6808807.0, -6808812.0, -6808813.0, -6808826.0, -6808838.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8170068027210884\n",
      "Hamming Loss: 0.09149659863945578\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3603\n",
      "           1       0.18      1.00      0.31       807\n",
      "\n",
      "    accuracy                           0.18      4410\n",
      "   macro avg       0.09      0.50      0.15      4410\n",
      "weighted avg       0.03      0.18      0.06      4410\n",
      "\n",
      "Train on 17643 samples\n",
      "Epoch 1/100\n",
      "17643/17643 [==============================] - 2s 127us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 2/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 3/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 4/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 5/100\n",
      "17643/17643 [==============================] - 2s 100us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 6/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 7/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 8/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 9/100\n",
      "17643/17643 [==============================] - 2s 100us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 10/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 11/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 12/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 13/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 14/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 15/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 16/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 17/100\n",
      "17643/17643 [==============================] - 2s 100us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 18/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 19/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 20/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 21/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 22/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 23/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 24/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 25/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 26/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 27/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 28/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 29/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 30/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 31/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 32/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 33/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 34/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 35/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 36/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 37/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 38/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 39/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 40/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 41/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 42/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 43/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 44/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 45/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 46/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 47/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 48/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 49/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 50/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 51/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 52/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 53/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 54/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 55/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 56/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 57/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 58/100\n",
      "17643/17643 [==============================] - 2s 96us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 59/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 60/100\n",
      "17643/17643 [==============================] - 2s 96us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 61/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 62/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 63/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 64/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 65/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 66/100\n",
      "17643/17643 [==============================] - 2s 101us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "17643/17643 [==============================] - 2s 132us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 68/100\n",
      "17643/17643 [==============================] - 3s 148us/sample - loss: 13.6970 - binary_accuracy: 0.1097 - los\n",
      "Epoch 69/100\n",
      "17643/17643 [==============================] - 2s 137us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 70/100\n",
      "17643/17643 [==============================] - 2s 122us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 71/100\n",
      "17643/17643 [==============================] - 2s 125us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 72/100\n",
      "17643/17643 [==============================] - 3s 149us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 73/100\n",
      "17643/17643 [==============================] - 2s 103us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 74/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 75/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 76/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 77/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 78/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 79/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 80/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 81/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 82/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 83/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 84/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 85/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 86/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 87/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 88/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 89/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 90/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 91/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 92/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 93/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 94/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 95/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 96/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 97/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 98/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 99/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n",
      "Epoch 100/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 13.6970 - binary_accuracy: 0.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-30195276.0, -30195280.0, -30195296.0, -30195308.0, -30195320.0, -30195324.0, -30195332.0, -30195336.0, -30195340.0, -30195344.0, -30195348.0, -30195352.0, -30195356.0, -30195360.0, -30195364.0, -30195368.0, -30195372.0, -30195376.0, -30195380.0, -30195384.0, -30195388.0, -30195392.0, -30195396.0, -30195400.0, -30195404.0, -30195408.0, -30195412.0, -30195416.0, -30195420.0, -30195424.0, -30195428.0, -30195432.0, -30195436.0, -30195440.0, -30195444.0, -30195448.0, -30195452.0, -30195456.0, -30195460.0, -30195464.0, -30195468.0, -30195472.0, -30195476.0, -30195480.0, -30195484.0, -30195488.0, -30195492.0, -30195496.0, -30195500.0, -30195504.0, -30195508.0, -30195512.0, -30195516.0, -30195520.0, -30195524.0, -30195528.0, -30195532.0, -30195536.0, -30195540.0, -30195544.0, -30195548.0, -30195804.0, -30195812.0, -30195816.0, -30195824.0, -30195828.0, -30195832.0, -30195836.0, -30195840.0, -30195844.0, -30195848.0, -30195852.0, -30195856.0, -30195860.0, -30195864.0, -30195868.0, -30195872.0, -30195876.0, -30195880.0, -30195884.0, -30195888.0, -30195892.0, -30195896.0, -30195900.0, -30195904.0, -30195908.0, -30195912.0, -30195916.0, -30195920.0, -30195924.0, -30195928.0, -30195932.0, -30195936.0, -30195940.0, -30195944.0, -30195948.0, -30195952.0, -30195956.0, -30195960.0, -30195964.0, -30195968.0, -30195972.0, -30195976.0, -30195980.0, -30195984.0, -30195988.0, -30195992.0, -30195996.0, -30196000.0, -30196004.0, -30196008.0, -30196012.0, -30196016.0, -30196020.0, -30196024.0, -30196028.0, -30196032.0, -30196036.0, -30196040.0, -30196044.0, -30196048.0, -30196052.0, -30196056.0, -30196060.0, -30196064.0, -30196068.0, -30196072.0, -30196076.0, -30196080.0, -30196084.0, -30196088.0, -30196092.0, -30196096.0, -30196100.0, -30196104.0, -30196108.0, -30196112.0, -30196116.0, -30196120.0, -30196124.0, -30196128.0, -30196132.0, -30196136.0, -30196140.0, -30196144.0, -30196148.0, -30196152.0, -30196156.0, -30196160.0, -30196164.0, -30196168.0, -30196172.0, -30196176.0, -30196180.0, -30196184.0, -30196188.0, -30196192.0, -30196196.0, -30196200.0, -30196204.0, -30196208.0, -30196212.0, -30196216.0, -30196220.0, -30196224.0, -30196228.0, -30196232.0, -30196236.0, -30196240.0, -30196244.0, -30196248.0, -30196252.0, -30196256.0, -30196260.0, -30196264.0, -30196268.0, -30196272.0, -30196276.0, -30196280.0, -30196284.0, -30196288.0, -30196292.0, -30196296.0, -30196300.0, -30196304.0, -30196308.0, -30196312.0, -30196316.0, -30196320.0, -30196324.0, -30196328.0, -30196332.0, -30196336.0, -30196340.0, -30196344.0, -30196348.0, -30196352.0, -30196356.0, -30196360.0, -30196364.0, -30196368.0, -30196372.0, -30196376.0, -30196380.0, 74547790.0, 74547800.0, 74547810.0, 74547816.0, 74547820.0, 74547830.0, 74547840.0, 74547850.0, 74547860.0, 74547864.0, 74547870.0, 74547880.0, 74547890.0, 74547896.0, 74547900.0, 74547910.0, 74547920.0, 74547930.0, 74547940.0, 74547944.0, 74547950.0, 74547960.0, 74547970.0, 74547976.0, 74547980.0, 74547990.0, 74548000.0, 74548010.0, 74548020.0, 74548024.0, 74548030.0, 74548040.0, 74548050.0, 74548056.0, 74548060.0, 74548070.0, 74548080.0, 74548090.0, 74548100.0, 74548104.0, 74548110.0, 74548120.0, 74548130.0, 74548136.0, 74548140.0, 74551250.0, 74551256.0, 74551260.0, 74551280.0, 74551336.0, 74551340.0, 74551350.0, 74551360.0, 74551370.0, 74551380.0, 74551384.0, 74551390.0, 74551400.0, 74551410.0, 74551416.0, 74551420.0, 74551430.0, 74551440.0, 74551450.0, 74551460.0, 74551464.0, 74551470.0, 74551480.0, 74551490.0, 74551496.0, 74551500.0, 74551510.0, 74551520.0, 74551530.0, 74551540.0, 74551544.0, 74551550.0, 74551560.0, 74551570.0, 74551576.0, 74551580.0, 74551590.0, 74551600.0, 74551610.0, 74551620.0, 74551624.0, 74551630.0, 74551640.0, 74551650.0, 74551656.0, 74551660.0, 74551680.0, 74551700.0, 74551704.0, 74551710.0, 74551720.0, 74551730.0, 74551740.0, 74551750.0, 74551780.0, 74551790.0, 74551800.0, 74551810.0, 74551816.0, 74551820.0, 74551830.0, 74551840.0, 74551850.0, 74551860.0, 74551864.0, 74551870.0, 74551880.0, 74551890.0, 74551896.0, 74551900.0, 74551910.0, 74551920.0, 74551930.0, 74551940.0, 74551944.0, 74551950.0, 74551960.0, 74551970.0, 74551976.0, 74551980.0, 74551990.0, 74552000.0, 74552010.0, 74552020.0, 74552024.0, 74552030.0, 74552040.0, 74552050.0, 74552056.0, 74552060.0, 74552070.0, 74552080.0, 74552090.0, 74552100.0, 74552104.0, 74552110.0, 74552120.0, 74552130.0, 74552136.0, 74552140.0, 74552150.0, 74552160.0, 74552170.0, 74552180.0, 74552184.0, 74552190.0, 74552200.0, 74552210.0, 74552216.0, 74552220.0, 74552230.0, 74552240.0, 74552250.0, 74552260.0, 74552264.0, 74552270.0, 74552280.0, 74552290.0, 74552296.0, 74552300.0, 74552310.0, 74552320.0, 74552330.0, 74552340.0, 74552344.0, 74552350.0, 74552360.0, 74552370.0, 74552376.0, 74552380.0, 74552390.0, 74552400.0, 74552410.0, 74552420.0, 74552424.0, 74552430.0, 74552440.0, 74552450.0, 74552456.0, 74552460.0, 74552470.0, 74552480.0, 74552490.0, 74552500.0, 74553540.0, 74553544.0, 74553550.0, 74553560.0, 74553570.0, 74553576.0, 74553580.0, 74553810.0, 74553816.0, 74553820.0, 74553830.0, 74553840.0, 74553850.0, 74553860.0, 74553864.0, 74553870.0, 74553880.0, 74553890.0, 74553896.0, 74553900.0, 74553910.0, 74553920.0, 74553930.0, 74553940.0, 74553944.0, 74553950.0, 74553960.0, 74553970.0, 74553976.0, 74553980.0, 74553990.0, 74554000.0, 74554010.0, 74554020.0, 74554024.0, 74554030.0, 74554040.0, 74554050.0, 74554056.0, 74554060.0, 74554070.0, 74554080.0, 74554090.0, 74554100.0, 74554104.0, 74554110.0, 74554120.0, 74554130.0, 74554136.0, 74555170.0, 74555176.0, 74555180.0, 74555190.0, 74555200.0, 74555210.0, 74555220.0, 74555224.0, 74555230.0, 74555240.0, 74555250.0, 74555256.0, 74555260.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8045351473922903\n",
      "Hamming Loss: 0.10192743764172335\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3723\n",
      "           1       0.16      1.00      0.27       687\n",
      "\n",
      "    accuracy                           0.16      4410\n",
      "   macro avg       0.08      0.50      0.13      4410\n",
      "weighted avg       0.02      0.16      0.04      4410\n",
      "\n",
      "y_pred shape: \n",
      "(4410, 2)\n",
      "y_pred2 shape:\n",
      "(4410, 2)\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      1.00      0.75         3\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.30      0.50      0.37         5\n",
      "weighted avg       0.36      0.60      0.45         5\n",
      "\n",
      "Hamming Loss: 0.1081248435291898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "y_pred3, pred3, y_test3, hl3 = list(),list(),list(),list()\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(1)\n",
    "  #]) \n",
    "    \n",
    "  #inputs = tf.keras.layers.Input(shape=(X_train.shape))\n",
    "  #outputs = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "  #model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "\n",
    "  #outputs = keras.layers.Dense(tf.keras.layers.Dense(number_of_classes)) #(x) \n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(2)\n",
    "  #])   \n",
    "  \n",
    "  #model = tf.contrib.learn.DNNClassifier(hidden_units=[5,10,5],\n",
    "  #                                          n_classes=4)    \n",
    "    \n",
    "\n",
    "  inputs = tf.keras.Input(shape=(X_train.shape))\n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    \n",
    "  outputs = tf.keras.layers.Dense(number_of_classes)  #, activation=activation)\n",
    "                             #kernel_initializer=initializer,\n",
    "                             #activation=activation) #(x) \n",
    "  #model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(2)\n",
    "  ]) \n",
    "    \n",
    "  #model.add(layers.Activation(activations.relu))\n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  #model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.001))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  #print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "    \n",
    "  #y_pred2.append(y_pred)\n",
    "  y_pred3 = np.append(y_pred3, y_pred2)\n",
    "  #pred2.append(pred)\n",
    "  pred3 = np.append(pred3, pred2)\n",
    "  #y_test2.append(y_test)\n",
    "  y_test3 = np.append(y_test3, y_test2)\n",
    "  #hl2.append(hl)\n",
    "  hl3 = np.append(hl3, hl)\n",
    "    \n",
    "print(\"y_pred shape: \")\n",
    "print(np.shape(y_pred))\n",
    "\n",
    "print(\"y_pred2 shape:\")\n",
    "print(np.shape(y_pred2))\n",
    "\n",
    "y_pred3 = np.concatenate((y_pred3[0], y_pred3[1], y_pred3[2], y_pred3[3], y_pred3[4]), axis=None)\n",
    "pred3 = np.concatenate((pred3[0], pred3[1], pred3[2], pred3[3], pred3[4]), axis=None)\n",
    "y_test3 = np.concatenate((y_test3[0], y_test3[1], y_test3[2], y_test3[3], y_test3[4]), axis=None)\n",
    "hl3 = np.concatenate((hl3[0], hl3[1], hl3[2], hl3[3], hl3[4]), axis=None)\n",
    "\n",
    "print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\")\n",
    "#print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "print(classification_report(y_test3, pred3))\n",
    "hl3_avg = sum(hl3) / len(hl3)\n",
    "print(\"Hamming Loss: \" + str(hl3_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107235, 15)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_KGFS10.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 21s 248us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 17s 195us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 18s 207us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 27s 311us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - 16s 183us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 15s 174us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 17s 194us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 15s 173us/sample - loss: 9.0267 - binary_accuracy: 0.4148  - ETA: 0s - loss: 9.0239 - binary_accuracy\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 18s 207us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 17s 202us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 16s 188us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - 15s 172us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 18s 209us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 16s 184us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 15/100\n",
      "85788/85788 [==============================] - 17s 197us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0254 - binary_acc\n",
      "Epoch 16/100\n",
      "85788/85788 [==============================] - 16s 192us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 17/100\n",
      "85788/85788 [==============================] - 16s 186us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 18/100\n",
      "85788/85788 [==============================] - 17s 195us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 19/100\n",
      "85788/85788 [==============================] - 25s 286us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 20/100\n",
      "85788/85788 [==============================] - 17s 201us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 21/100\n",
      "85788/85788 [==============================] - 14s 166us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 22/100\n",
      "85788/85788 [==============================] - 16s 187us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - los\n",
      "Epoch 23/100\n",
      "85788/85788 [==============================] - 17s 198us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.\n",
      "Epoch 24/100\n",
      "85788/85788 [==============================] - 16s 186us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 16s 186us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0304 - binary_\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 17s 202us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 17s 201us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 18s 205us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 16s 187us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 16s 187us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0312 - binar\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 15s 179us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 17s 199us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 33/100\n",
      "85788/85788 [==============================] - 16s 192us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 34/100\n",
      "85788/85788 [==============================] - 17s 201us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 17s 198us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 36/100\n",
      "85788/85788 [==============================] - 15s 177us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 17s 193us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0269 -\n",
      "Epoch 38/100\n",
      "85788/85788 [==============================] - 16s 192us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9 - ETA: 1s \n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 18s 207us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0291 - binary_\n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 18s 207us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0\n",
      "Epoch 41/100\n",
      "85788/85788 [==============================] - 17s 203us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.022\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 17s 199us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - los\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 16s 192us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss:  - ETA: 0s - loss: 9.0269 - binary_accuracy: 0.41\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 17s 196us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.024 - ETA:\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 18s 210us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 20s 229us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 23s 266us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 18s 214us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 16s 189us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9 - ETA: 0s - loss: 9.0255 - binary_ac\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 17s 201us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 17s 202us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 17s 198us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0234 - binary_accu - E\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 23s 268us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 54/100\n",
      "85788/85788 [==============================] - 17s 203us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - lo - E\n",
      "Epoch 55/100\n",
      "85788/85788 [==============================] - 22s 259us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 56/100\n",
      "85788/85788 [==============================] - 21s 241us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - los\n",
      "Epoch 57/100\n",
      "85788/85788 [==============================] - 19s 222us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 58/100\n",
      "85788/85788 [==============================] - 34s 396us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 59/100\n",
      "85788/85788 [==============================] - 33s 381us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 60/100\n",
      "85788/85788 [==============================] - 25s 297us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0256 - binary_accuracy:  - ETA: 1s - loss: 9.0270 - binary\n",
      "Epoch 61/100\n",
      "85788/85788 [==============================] - 18s 208us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 62/100\n",
      "85788/85788 [==============================] - 25s 288us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 63/100\n",
      "85788/85788 [==============================] - 23s 267us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - l\n",
      "Epoch 64/100\n",
      "85788/85788 [==============================] - 23s 271us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 30s 352us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0289 - binary_ac\n",
      "Epoch 66/100\n",
      "85788/85788 [==============================] - 29s 341us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.027\n",
      "Epoch 67/100\n",
      "85788/85788 [==============================] - 27s 309us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 68/100\n",
      "85788/85788 [==============================] - 17s 199us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0256 - binary_accuracy: 0.41\n",
      "Epoch 69/100\n",
      "85788/85788 [==============================] - 19s 220us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 70/100\n",
      "85788/85788 [==============================] - 39s 452us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 71/100\n",
      "85788/85788 [==============================] - 35s 405us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 72/100\n",
      "85788/85788 [==============================] - 23s 274us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 73/100\n",
      "85788/85788 [==============================] - 25s 291us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 74/100\n",
      "85788/85788 [==============================] - 26s 307us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 75/100\n",
      "85788/85788 [==============================] - 24s 283us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0260 - bin\n",
      "Epoch 76/100\n",
      "85788/85788 [==============================] - 17s 199us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.028 - ETA: 1s - loss: 9.0258 - binary_accuracy: 0.414 - ETA: 1s - loss: 9.0260 - b\n",
      "Epoch 77/100\n",
      "85788/85788 [==============================] - 17s 197us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 15s 176us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0243 - binary_accuracy: - ETA: 1s - loss: 9.0229 -\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 17s 200us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 17s 195us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 18s 204us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 32s 368us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 33s 390us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 37s 428us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 18s 206us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 29s 336us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 34s 394us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 41s 474us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 31s 366us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.027\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 28s 325us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 26s 308us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 25s 288us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - los\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 14s 166us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 14s 164us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0293 - binary_accuracy: 0.41 - ETA: 0s - loss: 9.0288 - binary_accura\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 17s 203us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 19s 219us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 20s 235us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0331 - binary_ac\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 19s 224us/sample - loss: 9.0267 - binary_accuracy: 0.4148\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 14s 168us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - lo\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 14s 158us/sample - loss: 9.0267 - binary_accuracy: 0.4148 - loss: 9.0296 - binary_accuracy: 0.414 - ETA: 0s - loss: 9.0284 - binary_accuracy: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-136136540.0, -136136560.0, -136136580.0, -136136590.0, -136136600.0, -136136620.0, -136136640.0, -136136700.0, -136136720.0, -136136740.0, -136136750.0, -136136770.0, -136136780.0, -136136800.0, -136136820.0, -136136830.0, -136136850.0, -136136860.0, -136136880.0, -136136900.0, -136136910.0, -136136930.0, -136136940.0, -136136960.0, -136136980.0, -136137000.0, -136137010.0, -136137020.0, -136137730.0, -136137740.0, -136137760.0, -136137780.0, -136137800.0, -136137810.0, -136137820.0, -136137840.0, -136137860.0, -136137870.0, -136137890.0, -136137900.0, -136137920.0, -136137940.0, -136137950.0, -136137970.0, -136137980.0, -136138190.0, -136138200.0, -136138220.0, -136138240.0, -136138260.0, -136138270.0, -136138290.0, -136138300.0, -136138320.0, -136138340.0, -136138350.0, -136138370.0, -136138380.0, -136138400.0, -136138420.0, -136138430.0, -136138450.0, -136138460.0, -136138480.0, -136138500.0, -136138510.0, -136138530.0, -136138660.0, -136138690.0, -136138700.0, -136138720.0, -136138740.0, -136138750.0, -136139580.0, -136139600.0, -136139650.0, -136139660.0, -136139680.0, -136139700.0, -136139710.0, -136139730.0, -136139740.0, -136139760.0, -136139780.0, -136139790.0, -136139800.0, -136139820.0, -136139840.0, -136139860.0, -136139870.0, -136139890.0, -136139900.0, -136139920.0, -136140030.0, -136140050.0, -136140060.0, -136140080.0, -136140100.0, -136140110.0, -136140130.0, -136140140.0, -136140160.0, -136140180.0, -136140200.0, -136140210.0, -136140220.0, -136140240.0, -136140260.0, -136140270.0, -136140290.0, -39632464.0, -39632480.0, -39632490.0, -39632496.0, -39632504.0, -39632510.0, -39632520.0, -39632530.0, -39632536.0, -39632544.0, -39632550.0, -39632560.0, -39632570.0, -39632576.0, -39632584.0, -39632590.0, -39632600.0, -39632610.0, -39632616.0, -39632624.0, -39632630.0, -39632640.0, -39632650.0, -39632656.0, -39632664.0, -39632670.0, -39632680.0, -39632690.0, -39632696.0, -39632704.0, -39632710.0, -39632720.0, -39632730.0, -39632736.0, -39632744.0, -39632750.0, -39632760.0, -39632770.0, -39632776.0, -39632784.0, -39632790.0, -39632800.0, -39632810.0, -39632816.0, -39632824.0, -39632830.0, -39632840.0, -39632850.0, -39632856.0, -39632864.0, -39632870.0, -39632880.0, -39632890.0, -39632896.0, -39632904.0, -39632910.0, -39632920.0, -39632930.0, -39632936.0, -39632944.0, -39632950.0, -39632960.0, -39632970.0, -39632976.0, -39632984.0, -39632990.0, -39633000.0, -39633010.0, -39633016.0, -39633024.0, -39633030.0, -39633040.0, -39633050.0, -39633056.0, -39633064.0, -39633070.0, -39633080.0, -39633090.0, -39633096.0, -39633104.0, -39633110.0, -39633120.0, -39633130.0, -39633136.0, -39633144.0, -39633150.0, -39633160.0, -39633170.0, -39633176.0, -39633184.0, -39633190.0, -39633200.0, -39633210.0, -39633216.0, -39633224.0, -39633230.0, -39633240.0, -39633250.0, -39633256.0, -39633264.0, -39633270.0, -39633280.0, -39633290.0, -39633296.0, -39633304.0, -39633310.0, -39633320.0, -39633330.0, -39633336.0, -39633344.0, -39633350.0, -39633360.0, -39633370.0, -39633376.0, -39633384.0, -39633390.0, -39633400.0, -39633410.0, -39633416.0, -39633424.0, -39633430.0, -39633440.0, -39633450.0, -39633456.0, -39633464.0, -39633470.0, -39633480.0, -39633490.0, -39633496.0, -39633504.0, -39633510.0, -39633520.0, -39633530.0, -39633536.0, -39633544.0, -39633550.0, -39633560.0, -39633570.0, -39633576.0, -39633584.0, -39633590.0, -39633600.0, -39633610.0, -39633616.0, -39633624.0, -39633630.0, -39633640.0, -39633650.0, -39633656.0, -39633664.0, -39633670.0, -39633680.0, -39633690.0, -39633696.0, -39633704.0, -39633710.0, -39633720.0, -39633730.0, -39633736.0, -39633744.0, -39633750.0, -39633760.0, -39633770.0, -39633776.0, -39633784.0, -39633790.0, -39633800.0, -39633810.0, -39633816.0, -39633824.0, -39633830.0, -39633840.0, -39633850.0, -39633856.0, -39633864.0, -39633870.0, -39633880.0, -39633890.0, -39633896.0, -39633904.0, -39633910.0, -39633920.0, -39633930.0, -39633936.0, -39633944.0, -39633950.0, -39633960.0, -39633970.0, -39633976.0, -39633984.0, -39633990.0, -39634000.0, -39634010.0, -39634016.0, -39634024.0, -39634030.0, -39634040.0, -39634050.0, -39634056.0, -39634064.0, -39634070.0, -39634080.0, -39634090.0, -39634096.0, -39634104.0, -39634110.0, -39634120.0, -39634130.0, -39634136.0, -39634144.0, -39634150.0, -39634160.0, -39634170.0, -39634176.0, -39634184.0, -39634190.0, -39634200.0, -39634210.0, -39634216.0, -39634224.0, -39634230.0, -39634240.0, -39634250.0, -39634256.0, -39634264.0, -39634270.0, -39634280.0, -39634290.0, -39634296.0, -39634304.0, -39634310.0, -39634320.0, -39634330.0, -39634336.0, -39634344.0, -39634350.0, -39634360.0, -39634370.0, -39634376.0, -39634384.0, -39634390.0, -39634400.0, -39634410.0, -39634416.0, -39634424.0, -39634430.0, -39634440.0, -39634450.0, -39634456.0, -39634464.0, -39634470.0, -39634480.0, -39634490.0, -39634550.0, -39634560.0, -39634570.0, -39634576.0, -39634584.0, -39634590.0, -39634600.0, -39634610.0, -39634616.0, -39634624.0, -39634630.0, -39634640.0, -39634650.0, -39634656.0, -39634664.0, -39634670.0, -39634680.0, -39634690.0, -39634696.0, -39634704.0, -39634710.0, -39634720.0, -39634730.0, -39634736.0, -39634744.0, -39634750.0, -39634760.0, -39634770.0, -39634776.0, -39634784.0, -39634790.0, -39634800.0, -39634810.0, -39634816.0, -39634824.0, -39634830.0, -39634840.0, -39634850.0, -39634856.0, -39634864.0, -39634870.0, -39634880.0, -39634890.0, -39634896.0, -39634904.0, -39634910.0, -39634920.0, -39634930.0, -39634936.0, -39634944.0, -39634950.0, -39634960.0, -39634970.0, -39634976.0, -39634984.0, -39634990.0, -39635000.0, -39635010.0, -39635016.0, -39635024.0, -39635030.0, -39635040.0, -39635050.0, -39635056.0, -39635064.0, -39635070.0, -39635080.0, -39635090.0, -39635096.0, -39635104.0, -39635110.0, -39635120.0, -39635130.0, -39635136.0, -39635144.0, -39635150.0, -39635160.0, -39635170.0, -39635176.0, -39635184.0, -39635190.0, -39635200.0, -39635210.0, -39635216.0, -39635224.0, -39635230.0, -39635240.0, -39635250.0, -39635256.0, -39635264.0, -39635270.0, -39635280.0, -39635290.0, -39635296.0, -39635304.0, -39635310.0, -39635320.0, -39635330.0, -39635336.0, -39635344.0, -39635350.0, -39635360.0, -39635370.0, -39635376.0, -39635384.0, -39635390.0, -39635400.0, -39635410.0, -39635416.0, -39635424.0, -39635430.0, -39635440.0, -39635450.0, -39635456.0, -39635464.0, -39635470.0, -39635480.0, -39635490.0, -39635496.0, -39635504.0, -39635510.0, -39639480.0, -39639490.0, -39639496.0, -39639504.0, -39639510.0, -39639520.0, -39639530.0, -39639536.0, -39639544.0, -39639550.0, -39639560.0, -39639570.0, -39639576.0, -39639584.0, -39639590.0, -39639600.0, -39639610.0, -39639616.0, -39639624.0, -39639630.0, -39639640.0, -39639650.0, -39639656.0, -39639664.0, -39639670.0, -39639680.0, -39639690.0, -39639696.0, -39639704.0, -39639710.0, -39639720.0, -39639730.0, -39639736.0, -39639744.0, -39639750.0, -39639760.0, -39639770.0, -39639776.0, -39639784.0, -39639790.0, -39639800.0, -39639810.0, -39639816.0, -39639824.0, -39639830.0, -39639840.0, -39639850.0, -39639856.0, -39639864.0, -39639870.0, -39639880.0, -39639890.0, -39639896.0, -39639904.0, -39639910.0, -39639920.0, -39639930.0, -39639936.0, -39639944.0, -39639950.0, -39639960.0, -39639970.0, -39639976.0, -39639984.0, -39639990.0, -39640000.0, -39640010.0, -39640016.0, -39640024.0, -39640030.0, -39640040.0, -39640050.0, -39640056.0, -39640064.0, -39640070.0, -39640080.0, -39640090.0, -39640096.0, -39640104.0, -39640110.0, -39640120.0, -39640130.0, -39640136.0, -39640144.0, -39640150.0, -39640160.0, -39640170.0, -39640176.0, -39640184.0, -39640190.0, -39640200.0, -39640210.0, -39640216.0, -39640224.0, -39640230.0, -39640240.0, -39640250.0, -39640256.0, -39640264.0, -39640270.0, -39640280.0, -39640290.0, -39640296.0, -39640304.0, -39640310.0, -39640320.0, -39640330.0, -39640336.0, -39640344.0, -39640350.0, -39640360.0, -39640370.0, -39640376.0, -39640384.0, -39640390.0, -39640400.0, -39640410.0, -39640416.0, -39640424.0, -39640430.0, -39640440.0, -39640450.0, -39640456.0, -39640464.0, -39640470.0, -39640480.0, -39640490.0, -39640496.0, -39640504.0, -39640510.0, -39640520.0, -39640530.0, -39640544.0, -39640550.0, -39640560.0, -39640570.0, -39640576.0, -39640584.0, -39640590.0, -39640600.0, -39640610.0, -39640616.0, -39640624.0, -39640630.0, -39640640.0, -39640650.0, -39640656.0, -39640664.0, -39640670.0, -39640680.0, -39640690.0, -39640696.0, -39640704.0, -39640710.0, -39640720.0, -39640730.0, -39640736.0, -39640744.0, -39640750.0, -39640760.0, -39640770.0, -39640776.0, -39640784.0, -39640790.0, -39640800.0, -39640810.0, -39640816.0, -39640824.0, -39640830.0, -39640840.0, -39640850.0, -39640856.0, -39640864.0, -39640870.0, -39640880.0, -39640890.0, -39640896.0, -39640904.0, -39640910.0, -39640920.0, -39640930.0, -39640936.0, -39640944.0, -39640950.0, -39640960.0, -39640970.0, -39640976.0, -39640984.0, -39640990.0, -39641000.0, -39641010.0, -39641016.0, -39641024.0, -39641030.0, -39641040.0, -39641050.0, -39641056.0, -39641064.0, -39641070.0, -39641080.0, -39641090.0, -39641096.0, -39641104.0, -39641110.0, -39641120.0, -39641130.0, -39641136.0, -39641144.0, -39641150.0, -39641160.0, -39641170.0, -39641176.0, -39641184.0, -39641190.0, -39641200.0, -39641210.0, -39641216.0, -39641224.0, -39641230.0, -39641240.0, -39641250.0, -39641256.0, -39641264.0, -39641270.0, -39641280.0, -39641290.0, -39641296.0, -39641304.0, -39641310.0, -39641320.0, -39641330.0, -39641336.0, -39641344.0, -39641350.0, -39641360.0, -39641370.0, -39641376.0, -39641384.0, -39641390.0, -39641400.0, -39641410.0, -39641416.0, -39641424.0, -39641430.0, -39641440.0, -39641450.0, -39641456.0, -39641464.0, -39641470.0, -39641480.0, -39641490.0, -39641496.0, -39641504.0, -39641510.0, -39641520.0, -39641530.0, -39641536.0, -39641544.0, -39641550.0, -39641560.0, -39641570.0, -39641576.0, -39641584.0, -39641590.0, -39641600.0, -39641610.0, -39641616.0, -39641624.0, -39641630.0, -39641640.0, -39641650.0, -39641656.0, -39641664.0, -39641670.0, -39641680.0, -39641690.0, -39641696.0, -39641704.0, -39641710.0, -39641720.0, -39641730.0, -39641736.0, -39641744.0, -39641750.0, -39641760.0, -39641770.0, -39641776.0, -39641784.0, -39641790.0, -39641800.0, -39641810.0, -39641816.0, -39641824.0, -39641830.0, -39641840.0, -39641850.0, -39641856.0, -39641864.0, -39641870.0, -39641880.0, -39641890.0, -39641896.0, -39641904.0, -39641910.0, -39641920.0, -39641930.0, -39641936.0, -39641944.0, -39641950.0, -39641970.0, -39641976.0, -39641984.0, -39641990.0, -39642000.0, -39642010.0, -39642016.0, -39642024.0, -39642030.0, -39642040.0, -39642050.0, -39642056.0, -39642064.0, -39642070.0, -39642080.0, -39642090.0, -39642096.0, -39642104.0, -39642110.0, -39642120.0, -39642130.0, -39642136.0, -39642144.0, -39642150.0, -39642160.0, -39642170.0, -39642176.0, -39642184.0, -39642190.0, -39642210.0, -39642224.0, -39645230.0, -39645250.0, -39645256.0, -39645264.0, -39645270.0, -39645280.0, -39645290.0, -39645296.0, -39645304.0, -39645310.0, -39645320.0, -39645330.0, -39645336.0, -39645344.0, -39645350.0, -39645360.0, -39645370.0, -39645376.0, -39645384.0, -39645390.0, -39645400.0, -39645410.0, -39645416.0, -39645424.0, -39645430.0, -39645440.0, -39645450.0, -39645456.0, -39645464.0, -39645470.0, -39645480.0, -39645490.0, -39645496.0, -39645504.0, -39645510.0, -39645520.0, -39645530.0, -39645536.0, -39645544.0, -39645550.0, -39645560.0, -39645570.0, -39645576.0, -39645584.0, -39645590.0, -39645600.0, -39645610.0, -39645616.0, -39645624.0, -39645630.0, -39645640.0, -39645650.0, -39645656.0, -39645664.0, -39645670.0, -39645680.0, -39645690.0, -39645696.0, -39645704.0, -39645710.0, -39645720.0, -39645730.0, -39645736.0, -39645744.0, -39645750.0, -39645760.0, -39645770.0, -39645776.0, -39645784.0, -39645790.0, -39645800.0, -39645810.0, -39645816.0, -39645824.0, -39645830.0, -39645840.0, -39645850.0, -39645856.0, -39645864.0, -39645870.0, -39645880.0, -39645890.0, -39645896.0, -39645904.0, -39645910.0, -39645920.0, -39645930.0, -39645936.0, -39645944.0, -39645950.0, -39645960.0, -39645970.0, -39645976.0, -39645984.0, -39645990.0, -39646000.0, -39646010.0, -39646016.0, -39646024.0, -39646030.0, -39646040.0, -39646050.0, -39646056.0, -39646064.0, -39646070.0, -39646080.0, -39646090.0, -39646096.0, -39646104.0, -39646110.0, -39646120.0, -39646130.0, -39646136.0, -39646144.0, -39646150.0, -39646160.0, -39646170.0, -39646176.0, -39646184.0, -39646190.0, -39646200.0, -39646210.0, -39646216.0, -39646224.0, -39646230.0, -39646240.0, -39646250.0, -39646256.0, -39646264.0, -39646270.0, -39646280.0, -39646290.0, -39646296.0, -39646304.0, -39646310.0, -39646320.0, -39646330.0, -39646336.0, -39646344.0, -39646350.0, -39646360.0, -39646370.0, -39646376.0, -39646384.0, -39646390.0, -39646400.0, -39646410.0, -39646416.0, -39646424.0, -39646430.0, -39646440.0, -39646450.0, -39646456.0, -39646464.0, -39646470.0, -39646480.0, -39646490.0, -39646496.0, -39646504.0, -39646510.0, -39646520.0, -39646530.0, -39646536.0, -39646544.0, -39646550.0, -39646560.0, -39646570.0, -39646576.0, -39646584.0, -39646590.0, -39646600.0, -39646610.0, -39646616.0, -39646624.0, -39646630.0, -39646640.0, -39646650.0, -39646656.0, -39646664.0, -39646670.0, -39646680.0, -39646690.0, -39646696.0, -39646704.0, -39646710.0, -39646730.0, -39646736.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8269688068261295\n",
      "Hamming Loss: 0.08849722571921481\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     18221\n",
      "           1       0.15      1.00      0.26      3226\n",
      "\n",
      "    accuracy                           0.15     21447\n",
      "   macro avg       0.08      0.50      0.13     21447\n",
      "weighted avg       0.02      0.15      0.04     21447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 16s 182us/sample - loss: 13.9922 - binary_accuracy: 0.09046s - l - ETA: 10s - loss: 1 - ETA: 2s - los - ETA: 1s - loss:\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 14s 162us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 14s 160us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 14s 162us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - 17s 195us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 17s 194us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 14s 160us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9900 - bin\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 13.9922 - binary_accuracy: 0.0904 - ETA: 9s\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 22s 254us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - 16s 189us/sample - loss: 13.9922 - binary_accuracy: 0.0904- l\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 15s 172us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 14s 162us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 15/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 14.0005 - bin - ETA: 5s - loss: 1 - ETA: 2s - l\n",
      "Epoch 16/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9955 - binar - ETA: 1s - loss: 13.9945 - binary_accura\n",
      "Epoch 17/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 18/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 19/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 20/100\n",
      "85788/85788 [==============================] - 13s 155us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 21/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 22/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9754 - binary_accuracy: 0. - E - ETA: 1s - loss: 13.9859 - binary_accuracy: 0.09 - ETA: 1s - loss: 13.9866 - binary_accura - ETA: 0s - loss: 13.9881 -\n",
      "Epoch 23/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 24/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9921 - binary_accura\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904ETA: 0s - loss: 13.9998 - binar - ETA: 0s - loss: 13.9952 - binary_accuracy: 0.\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9973 - ETA: 2s - loss: 13.9939 - binary_accuracy: 0. - ETA: 1s - loss: 13.99 - ETA: 1s - loss:\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904 - loss: 13.9792 - - ETA: 0s - loss: 13.9913 - binary_accuracy: \n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 33/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 1 - ETA: 2s - loss: 1 - ETA: 1s - loss: 13.99 - ETA: 0s - loss: 13.9906 - binary_accuracy: \n",
      "Epoch 34/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 13.9922 - binary_accuracy: 0.0904s - loss: 13.9905 -\n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 13.9922 - binary_accuracy: 0.09040s - loss: 13.98\n",
      "Epoch 36/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.09040s - loss: 13.9473 - binary_accuracy: 0.09 - ETA: 10s - loss: 13.9570 - binary_accuracy: 0. - ETA: 10s - loss: 13.9509 - b - ETA: 9s - loss: 13.9516 - binary_accura - ETA: 9s - ETA: 7s - loss: 13.9503 - b - ETA: 2s - loss: 13.9834 - b\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 38/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 13.9922 - binary_accuracy: 0.09041s - loss: 13.9811 - binary_accurac - ETA: 11s - loss - ETA: \n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 14s 161us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.09041s - l - ETA: 2s - ETA: 1s\n",
      "Epoch 41/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 12s 144us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 14s 161us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9930 -\n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9933 - binary_accura\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 54/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9912 - binary_accuracy - ETA: 0s - loss: 13.9914 - binary_accuracy: \n",
      "Epoch 55/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 56/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.99\n",
      "Epoch 57/100\n",
      "85788/85788 [==============================] - 14s 157us/sample - loss: 13.9922 - binary_accuracy: 0.0904- - ETA: 1s - loss:\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 13s 152us/sample - loss: 13.9922 - binary_accuracy: 0.0904- los - ETA: 3s - ETA: 0s - loss: 13.9870 - b\n",
      "Epoch 59/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 60/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 61/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.\n",
      "Epoch 62/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9957\n",
      "Epoch 63/100\n",
      "85788/85788 [==============================] - 13s 154us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 64/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9921 - binary_accuracy: 0.09\n",
      "Epoch 65/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 66/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: - ETA: 1s - - ETA: 0s - loss: 13.9920 - binary_accu\n",
      "Epoch 67/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 68/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9974 - bin - ETA: 5s - loss: 14.0002 - ETA: 1s - los\n",
      "Epoch 69/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 70/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 71/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.\n",
      "Epoch 72/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9913 - bin\n",
      "Epoch 73/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9921 - binary_\n",
      "Epoch 74/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 75/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 76/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.09040s - loss: 13.9766 - binary_a - ETA: 5s - loss: 13.9861 - binary_accuracy: 0.09 - ETA: 5s - loss: 13.9842 - bin - ETA: 5s - loss: 13.9858 - binary_accu - ETA: 4s - loss: 13.9876 - binary_accuracy - ETA: 4s - loss: 13.9882 - binary_ac - ETA: 4s - loss: 13. - ETA: 1s - loss: 13.9916 - binary_accu - ETA: 1s - los\n",
      "Epoch 77/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9846 - ETA: 4s - loss: 13. - ETA: 1s - loss: 13.9905 - binary_accuracy: 0.09 - ETA\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.0904ETA: 1s - loss: 13.9987 - binar - ETA: 1s - l\n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 13.9922 - binary_accuracy: 0.09041s - loss: 14.0031 - binary - ETA: 9 - ETA: 2s - loss: 13.9936 - binar - E\n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13. - ETA: 1s - los - ETA: 0s - loss: 13.9921 - binary_accuracy: 0.\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9886 - binary_accuracy\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 12s 144us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 13.9922 - binary_accuracy: 0.0904- los - ETA: 4s - loss: 13.9791 - binary_accura - ETA: 4s - loss: 13. - ETA: 3s - loss: 13.9853 - binary_accuracy:  - ETA: 2s - - ETA - ETA: 0s - loss: 13.9911 - binary_accuracy: 0.\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9913 - binary_accuracy\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.9867\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 13.9922 - binary_accuracy: 0.0904\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 13.9922 - binary_accuracy: 0.0904- loss: 13.98 - E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-67612984.0, -67612990.0, -67613000.0, -67613010.0, -67613016.0, -67613020.0, -67613030.0, -67613040.0, -67613050.0, -67613060.0, -67613064.0, -67613070.0, -67613080.0, -67613090.0, -67613096.0, -67613100.0, -67613110.0, -67613120.0, -67613130.0, -67613140.0, -67613144.0, -67613150.0, -67613160.0, -67613170.0, -67613176.0, -67613180.0, -67613190.0, -67613200.0, -67613210.0, -67613220.0, -67613224.0, -67613230.0, -67613240.0, -67613250.0, -67613256.0, -67613260.0, -67613270.0, -67613280.0, -67613290.0, -67613300.0, -67613304.0, -67613310.0, -67613320.0, -67613330.0, -67613336.0, -67613470.0, -67613480.0, -67613490.0, -67613496.0, -67613500.0, -67613510.0, -67613520.0, -67613530.0, -67613540.0, -67613544.0, -67613550.0, -67613560.0, -67613570.0, -67613576.0, -67613580.0, -67613590.0, -67613600.0, -67613610.0, -67613620.0, -67613624.0, -67613630.0, -67613640.0, -67613650.0, -67613656.0, -67613660.0, -67613670.0, -67613680.0, -67613690.0, -67613760.0, -67613770.0, -67613780.0, -67613784.0, -67613790.0, -67613800.0, -67613810.0, -67613816.0, -67613820.0, -67613830.0, -67613840.0, -67613850.0, -67613860.0, -67613864.0, -67613870.0, -67613880.0, -67613890.0, -67613896.0, -67613900.0, -67613910.0, -67613920.0, -67613930.0, -67613940.0, -67613944.0, -67613950.0, -67613960.0, -67613970.0, -67613976.0, -67613980.0, -67613990.0, -67614000.0, -67614010.0, -67614020.0, -67614024.0, -67614030.0, -67614040.0, -67614050.0, -67614056.0, -67614060.0, -67614070.0, -67614080.0, -67614090.0, -67614100.0, -67614104.0, -67614110.0, -67614120.0, -67614130.0, -67614136.0, -67614140.0, -67614150.0, -67614160.0, -67614170.0, -67614180.0, -67614184.0, -67614190.0, -67614200.0, -67614210.0, -67614216.0, -67614220.0, -67614230.0, -67614240.0, -67614250.0, -67614260.0, -67614264.0, -67614270.0, -67614280.0, -67614290.0, -67614296.0, -67614300.0, -67614310.0, -67614320.0, -67614330.0, -67614344.0, -67614350.0, -67614360.0, -67614370.0, -67614376.0, -67614380.0, -67614390.0, -67614400.0, -67614410.0, -67614420.0, -67614424.0, -67614430.0, -67614440.0, -67614450.0, -67614456.0, -67614460.0, 114424460.0, 114424470.0, 114424480.0, 114424490.0, 114424500.0, 114424504.0, 114424510.0, 114424520.0, 114424530.0, 114424536.0, 114424540.0, 114424550.0, 114424560.0, 114424570.0, 114424580.0, 114424584.0, 114424590.0, 114424600.0, 114424610.0, 114424616.0, 114424620.0, 114424630.0, 114424640.0, 114424650.0, 114424660.0, 114424664.0, 114424670.0, 114424680.0, 114424690.0, 114424696.0, 114424700.0, 114424710.0, 114424720.0, 114424730.0, 114424740.0, 114424744.0, 114424750.0, 114424760.0, 114424770.0, 114424776.0, 114424780.0, 114424790.0, 114424800.0, 114424810.0, 114424820.0, 114424824.0, 114424830.0, 114424840.0, 114424850.0, 114424856.0, 114424860.0, 114424870.0, 114424880.0, 114424890.0, 114424900.0, 114424904.0, 114424910.0, 114424920.0, 114424930.0, 114424936.0, 114424940.0, 114424950.0, 114424960.0, 114424970.0, 114424980.0, 114424984.0, 114424990.0, 114425000.0, 114425010.0, 114425016.0, 114425020.0, 114425030.0, 114425040.0, 114425050.0, 114425060.0, 114425064.0, 114425070.0, 114425080.0, 114425090.0, 114425096.0, 114425100.0, 114425110.0, 114425120.0, 114425130.0, 114425140.0, 114425144.0, 114425150.0, 114425160.0, 114425170.0, 114425176.0, 114425180.0, 114425190.0, 114425200.0, 114425210.0, 114425220.0, 114425224.0, 114425230.0, 114425240.0, 114425250.0, 114425256.0, 114425260.0, 114425270.0, 114425280.0, 114425290.0, 114425300.0, 114425304.0, 114425310.0, 114425320.0, 114425330.0, 114425336.0, 114425340.0, 114425350.0, 114425360.0, 114425370.0, 114425380.0, 114425384.0, 114425390.0, 114425400.0, 114425410.0, 114425416.0, 114425420.0, 114425430.0, 114425440.0, 114425450.0, 114425460.0, 114425464.0, 114425470.0, 114425480.0, 114425490.0, 114425496.0, 114425500.0, 114425510.0, 114425520.0, 114425530.0, 114425540.0, 114425544.0, 114425550.0, 114425560.0, 114425570.0, 114425576.0, 114425580.0, 114425590.0, 114425600.0, 114425610.0, 114425620.0, 114425624.0, 114425630.0, 114425640.0, 114425650.0, 114425656.0, 114425660.0, 114425670.0, 114425680.0, 114425690.0, 114425700.0, 114425704.0, 114425710.0, 114425720.0, 114425730.0, 114425736.0, 114425740.0, 114425750.0, 114425760.0, 114425770.0, 114425780.0, 114425784.0, 114425790.0, 114425800.0, 114425810.0, 114425816.0, 114425820.0, 114425830.0, 114425840.0, 114425850.0, 114425860.0, 114425864.0, 114425870.0, 114425880.0, 114425890.0, 114425896.0, 114425900.0, 114425910.0, 114425920.0, 114425930.0, 114425940.0, 114425944.0, 114425950.0, 114425960.0, 114425970.0, 114425976.0, 114425980.0, 114425990.0, 114426000.0, 114426010.0, 114426020.0, 114426024.0, 114426030.0, 114426040.0, 114426050.0, 114426056.0, 114426060.0, 114426070.0, 114426080.0, 114426090.0, 114426100.0, 114426104.0, 114426110.0, 114426120.0, 114426130.0, 114426136.0, 114426140.0, 114426150.0, 114426160.0, 114426170.0, 114426180.0, 114426184.0, 114426190.0, 114426200.0, 114426210.0, 114426216.0, 114426220.0, 114426230.0, 114426240.0, 114426250.0, 114426260.0, 114426264.0, 114426270.0, 114426280.0, 114426290.0, 114426296.0, 114426300.0, 114426310.0, 114426320.0, 114426330.0, 114426340.0, 114426344.0, 114426350.0, 114426360.0, 114426370.0, 114426376.0, 114426380.0, 114426390.0, 114426400.0, 114426410.0, 114426420.0, 114426424.0, 114426430.0, 114426440.0, 114426450.0, 114426456.0, 114426460.0, 114426470.0, 114426480.0, 114426490.0, 114426500.0, 114426504.0, 114426510.0, 114426520.0, 114426530.0, 114426536.0, 114426540.0, 114426550.0, 114426560.0, 114426570.0, 114426580.0, 114426584.0, 114426590.0, 114426600.0, 114426610.0, 114426616.0, 114426620.0, 114426630.0, 114426640.0, 114426650.0, 114426660.0, 114426670.0, 114427000.0, 114427010.0, 114427016.0, 114427020.0, 114427030.0, 114427040.0, 114427050.0, 114427060.0, 114427064.0, 114427070.0, 114427080.0, 114427090.0, 114427096.0, 114427100.0, 114427110.0, 114427120.0, 114427130.0, 114427140.0, 114427144.0, 114427150.0, 114427160.0, 114427170.0, 114427176.0, 114427180.0, 114427190.0, 114427200.0, 114427210.0, 114427220.0, 114427224.0, 114427230.0, 114427240.0, 114427250.0, 114427256.0, 114427260.0, 114427270.0, 114427280.0, 114427290.0, 114427300.0, 114427304.0, 114427310.0, 114427320.0, 114427330.0, 114427336.0, 114427340.0, 114427350.0, 114427360.0, 114427370.0, 114427380.0, 114427384.0, 114427390.0, 114427400.0, 114427410.0, 114427416.0, 114427420.0, 114427430.0, 114427440.0, 114427450.0, 114427460.0, 114427464.0, 114427470.0, 114427480.0, 114427490.0, 114427496.0, 114427500.0, 114427510.0, 114427520.0, 114427530.0, 114427540.0, 114427544.0, 114427550.0, 114427560.0, 114427570.0, 114427576.0, 114427580.0, 114427590.0, 114427600.0, 114427610.0, 114427620.0, 114428700.0, 114428710.0, 114428720.0, 114428730.0, 114428740.0, 114428744.0, 114428750.0, 114428760.0, 114428770.0, 114428776.0, 114428780.0, 114428790.0, 114428800.0, 114428810.0, 114428820.0, 114428824.0, 114428830.0, 114428840.0, 114428850.0, 114428856.0, 114428860.0, 114428870.0, 114428880.0, 114428890.0, 114428900.0, 114428904.0, 114428910.0, 114428920.0, 114428930.0, 114428936.0, 114428940.0, 114428950.0, 114428960.0, 114428970.0, 114428980.0, 114428984.0, 114428990.0, 114429000.0, 114429010.0, 114429016.0, 114429020.0, 114429030.0, 114429040.0, 114429050.0, 114429060.0, 114429064.0, 114429070.0, 114429080.0, 114429090.0, 114429096.0, 114429100.0, 114429110.0, 114429120.0, 114429130.0, 114429140.0, 114429144.0, 114429150.0, 114429160.0, 114429170.0, 114429176.0, 114429180.0, 114429190.0, 114429200.0, 114429210.0, 114429220.0, 114429224.0, 114429230.0, 114429240.0, 114429250.0, 114429256.0, 114429260.0, 114429270.0, 114429280.0, 114429290.0, 114429300.0, 114429304.0, 114429310.0, 114429320.0, 114429330.0, 114429336.0, 114429340.0, 114429350.0, 114429360.0, 114429370.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8170373478808225\n",
      "Hamming Loss: 0.09635380239660558\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     18537\n",
      "           1       0.14      1.00      0.24      2910\n",
      "\n",
      "    accuracy                           0.14     21447\n",
      "   macro avg       0.07      0.50      0.12     21447\n",
      "weighted avg       0.02      0.14      0.03     21447\n",
      "\n",
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 15s 172us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9891 - b - ETA: 0s - loss: 8.9889 - binary_acc\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 13s 155us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 13s 155us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss - ETA: 4s - loss: 8.9897 - binary_a\n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9886 - binary_accura\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9878 - binary_accur - ETA: - ETA: 2s - loss: 8.990 - ETA: 0s - loss: 8.9903 - binary_ac\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 14s 161us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9970 - binary_accuracy: 0.416 - ETA: 2s - loss: 8.9980 - binary_accuracy:  - ETA: 2s - l - ETA: 0s - loss: 8.9891 - binary_accuracy:\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss - ETA: 2s - loss: 8.987 - ETA: 0s - loss: 8.9843 - binary_accuracy - ETA: 0s - loss: 8.9867 - binary_accuracy: 0.417 - ETA: 0s - loss: 8.9876 - binary_accuracy: 0\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 4s - loss: 8.9880 - binary_accuracy: 0.4 - ETA: 4s - loss: 8.9889 - bi - ETA: 0s - loss: 8.9897 - binary_accuracy: 0.417\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: \n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8 - ETA - ETA: 2s - loss: 8 - ETA: 0s - loss: 8.9912 - binary_acc\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9921 - bin\n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 15/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - - ETA: 3s - loss: 8.9951 - binary - ETA\n",
      "Epoch 16/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9794 - binary_accuracy: 0 - ETA: 7s - loss: 8.9845 -  - ETA: 6s - loss: - ETA: 4s - loss: 8.9858 - ETA: 2s - loss: 8 - ETA: 0s - loss: 8.9928 - binary_accura\n",
      "Epoch 17/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9697 - - ETA - ETA: 6 - ETA: 3s - loss: 8.9850 - binary_accuracy: 0.417 - ETA: 3s - loss: 8.9847 - binary_acc - ETA: 2s - loss: 8.9901 - binary_acc - ETA: 1s - loss: 8.\n",
      "Epoch 18/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9875 - binary_accuracy: 0.4\n",
      "Epoch 19/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 ETA: 5s - loss: 9.0 - ETA: 3s - loss: 8.9874 - ETA: 2s - loss: 8.9917 - binary_accuracy: 0.41 - ETA: 2s - loss:\n",
      "Epoch 20/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 8.9897 - binary_accuracy: 0.4172TA: 3s - loss: 8.9917 - binary_accuracy: 0.417 - ETA: 3s - loss: 8.9910 - binary_accuracy:  - ETA: 2s - loss: 8.9905 - binary_accuracy: 0.41 - ETA:  - ETA: 0s - loss: 8.9890 - binary_accu\n",
      "Epoch 21/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9843  - ETA: 2s - loss: 8.9826 - binary_accuracy: 0 - ETA: 1s - loss: 8.9856 - binary_accurac - ETA: 1s - loss: 8.9882 - bin\n",
      "Epoch 22/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9953 - binary_accuracy: - ETA - ETA: 2s - loss: 8.9918 - binary_accura - ETA: 1s - loss: 8.9926 - binary_accuracy: 0.417 - ETA: 1s - loss: 8.991 - ETA: 0s - loss: 8.9893 - binary_accuracy: 0.4\n",
      "Epoch 23/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - ETA: 7s - loss: 8.9871 - ETA: 5s - loss: 8.9940 - binary_accuracy: - ETA: 5s - loss: 8.9953 - binary_accurac - ETA: 4s - loss: 8.9941 - binary_accura - ETA: 0s - loss: 8.9875 - binary_accuracy\n",
      "Epoch 24/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9897 - binary_accurac\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 13s 155us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 9.0 - ETA: 7s - loss: 8.9781 - binary_accuracy: - ETA: 7 - ETA: 0s - loss: 8.9890 - binary_ac\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 9.0274 - E - ETA: 2s - loss: 8.9854 - binary_accuracy: - ETA: 1s - loss: 8.9\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9903 -  - ETA: 0s - loss: 8.9905 - binary_accuracy: 0.4\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9990 - binary_accuracy: 0.41 - ETA: 1s - loss: 8.9988 - binary_acc - ETA: 0s - loss: 8.9873 - binary_accuracy: 0 - ETA: 0s - loss: 8.9881 - binary_accuracy: 0.\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9871 - binary_accur\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9730 - binary_ac - ETA: 10s - loss: 8.95 - ETA: 9s - loss: 8.97 - ETA: 7s - loss: 8.9768 - binary_accuracy: 0.4 - ETA: 7s - loss: 8.9799 - binary_accuracy - ETA: 6 - ETA: 4s - loss: 8.9924 - binary_accuracy: 0.417 - ETA: 3s - l - ETA: 1s - loss: 8.9900 - binary_accuracy:  - ETA: 1s - loss: 8.9899 - bi\n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8\n",
      "Epoch 33/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.98 - ETA: 3s - loss: 8.9924 - binary_accuracy: - ETA: 3s - loss: 8.9936 - binary_a - ETA: 2s - lo - ETA: 0s - loss: 8.9888 - binary_accuracy: 0.417\n",
      "Epoch 34/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9924 - b - ETA: 5s - loss: 8.9962 - binar - ETA: 4s - loss: 8.9950  - ETA: 3s - loss: 8.9912 - binary_acc - ETA: 2s - loss: 8.9904 - binary_accuracy:  - ETA: 1s - loss: 8.9905 - binary - ETA: 0s - loss: 8.9902 - binary_a\n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9835 - binary_accuracy: 0.417 - ETA: 7s - - ETA: 1s - loss: 8.9881\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: - ETA: 9s - loss: 8.9952 - binary_accuracy: 0.4 - ETA: 9s - loss: 8.9992 - binary_acc  - ETA: 1s - loss: 8.9950 - bina - ETA: 0s - loss: 8.9894 - binary_accuracy\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 14s 159us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 38/100\n",
      "85788/85788 [==============================] - 14s 163us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 14s 165us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9878 - bin\n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 16s 182us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9930 - binary_accuracy: 0.417  - ETA: 0s - loss: 8.9898 - binary_accurac\n",
      "Epoch 41/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9900 - binary_a\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9641 - binary_accuracy:  - ETA: 9s - loss: 8.9732 -  - ETA: 8s - loss: 8.9734 - bina - ETA: 4s - loss: 8.9 - ETA: 2s - lo\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - l - ETA: 4s - loss: 8.9884 - ETA: 2s - loss: 8.9935 -  - ETA: 1s - loss: 8.9877 - bi\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9335 - binary_accuracy: 0. - ETA: 10s - loss: 8 - ETA: 6s - loss: 8.9967 - binary_accuracy - ETA: 6s - - ETA: 3s - loss: - ETA: 1s - loss: 8.9\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9843 - binary_acc - ETA: 4s - lo - ETA: 2s - loss: 8.9828 - binary - ETA: 1s - loss: 8.9822 - binary_accuracy: 0.4 - ETA: 0s - loss: 8.9835 - binary_\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9774 - binary_accuracy: - ETA: 7s - loss: 8.9810 - binary_ - ETA: 7s - loss: 8.9925 - - ETA: 5s - loss: 8. - ETA: 0s - loss: 8.9911 - binary_accuracy: 0.\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 9.0050 - binary_accuracy: 0.4 - ETA: 4s - loss: 9.0035 - binary_accuracy - ETA: 4s - loss: 9.0006 - binary - \n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9993 - binary_a - ETA: 6s - loss: 8.9844 - binary_accur - ETA: 5s - los  - ETA: 0s - loss: 8.9913 - binary_accuracy\n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9 - ETA: 2s - loss: 8.9916 - binary_accura - ETA: 1s - loss: 8.\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9900 - binary_accurac - ETA: 2s - loss: 8.9908 - b - ETA: 1s - loss: 8.9853\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss:\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9908 - binar - ETA: 4s -  - ETA: 1s - loss: \n",
      "Epoch 54/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8\n",
      "Epoch 55/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 9.0100 - binary_accura - ETA: - ETA: 8s - loss: 9.0003 - bina - ETA: 6s - loss: 9.0008 - binary_ - ETA: 2s - loss: - ETA: 0s - loss: 8.9906 - binary_acc\n",
      "Epoch 56/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 9.0010 - binary_ - ETA: 4s - loss: 9.0039 - bi\n",
      "Epoch 57/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 9.0084 - binary_accu - ETA: 5s - loss: - ETA: 3s - loss: 8.9 - ETA: 1s - loss: 8.9913 - bin\n",
      "Epoch 58/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9880 - binary_accurac - ETA:  - ETA: 3s - loss: 8.9775 - binary_accur - ET\n",
      "Epoch 59/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9991 - bin - ETA: 10s - los - ETA: 7s - loss: 8.9943 - binary_accuracy: 0.416 - ETA: 7s - loss:  - ETA\n",
      "Epoch 60/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9845 - bin - ETA: 0s - loss: 8.9907 - binary_\n",
      "Epoch 61/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 62/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9716 -  - ETA: 7s - loss: 8.9733 - bina - ET - ETA: 3s - loss: 8.9773 - bina - ETA: 2s \n",
      "Epoch 63/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9879 - b - ETA: 5s - loss: 8\n",
      "Epoch 64/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9748 - bi - ETA: 5s  - ETA\n",
      "Epoch 65/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9704 - b - ETA:  - ETA: 4s - loss: 8.9757 - binary_accurac - ETA: 4s - loss: 8.9765 - - ETA: 2s - loss: 8.9797 - binary_acc - ETA: 1s - loss: 8.9757 - bina - ETA: 0s - loss: 8.9843 - binary_accura\n",
      "Epoch 66/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - lo - ETA: 4s - loss: 8.9922 - binary_\n",
      "Epoch 67/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9767 - binary_accuracy: - ETA:  - ETA: 2s\n",
      "Epoch 68/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss\n",
      "Epoch 69/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9993 -  - ETA: 7s - loss: 9.0041 - binary_accuracy - ETA: 6s - loss: 9.0046 - binary_accura - ETA: 6s - loss: 9.00 - ETA: 4s - loss: 8.9961 - binary_accu - ETA: 4s - loss: 8.9932 - bina - ETA: 2s - loss: 8.9913 - ETA: 1s - loss: 8.9884 - bina\n",
      "Epoch 70/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9917 - binary_accuracy: 0\n",
      "Epoch 71/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9796 - binary_accuracy: 0 - ETA: 9s - loss: 8.9807 - binary_accuracy: 0.4 - ETA: 9s - loss: 8. - ETA: 0s - loss: 8.9905 - binary_accuracy:  - ETA: 0s - loss: 8.9894 - binary_accuracy: \n",
      "Epoch 72/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9854 - binary_ - ETA: 10s - loss: 8. - ETA: 9s - loss: 8.9948 - binary_accur - ET - ETA:  - ETA: 3s - loss: 8.9915 - bina - ETA: 2s - loss: 8.9875 - b - ETA: 0s - loss: 8.9946 - binary_accu\n",
      "Epoch 73/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9942 - binary_accuracy: 0.4 - ETA: 0s - loss: 8.9921 - binary_\n",
      "Epoch 74/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9862 - binary_accuracy: 0 - ETA: 9s - loss: 8.9984 - binar - ETA: 2s - loss:  - ETA: 0s - loss: 8.9887 - binary_accuracy:\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9529 - bin - ETA - ETA: 5s - loss: 8.97 - ETA: 3s - loss: 8.9830 - binary_accuracy: 0.4 - ETA: 3s - loss: 8.9855  - ETA: 1s - loss: 8.9862 - binary_a - ETA: 0s - loss: 8.9892 - binary_accur\n",
      "Epoch 76/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss:  - ETA: 8s -  - ETA: 2s - loss: 8.9842 - binary_accuracy: 0. - ETA: 2s - loss: 8.9846 - binary_accuracy: 0.417 - ETA: 2s - loss: 8.9852 - binary_accurac - ETA: 2s - loss\n",
      "Epoch 77/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.9651 - binary_a - ETA: 8s - loss: 8.9809 - binary_accuracy: 0. - ETA: 7s - loss: 8.9773 \n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9967 - binary_accuracy: 0.4 - ETA: 4s - loss: 8.9962 - binary_acc - ETA: 0s - loss: 8.9892 - binary_accu\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 9.0011 - binar\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 9.016 - ETA: 5s - loss\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9 - ETA: 0s - loss: 8.9895 - binary_accuracy: 0.41\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9908 - binary_accuracy: 0.417 - ETA: 9s - loss: 8.9963 -   - ETA: 2s - loss:\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 9.0117 - binary_accurac - ETA: \n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss - ETA: 6s - loss: 9.0 - ETA: 1s - loss: 8.9961 - bi\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 12s 138us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9974 - binary - ETA: - ETA: 5s - loss: 8.9814 - binary_ac - ETA: 4s - loss: 8 - ETA: 2s - loss: 8. - ETA: 0s - loss: 8.9878 - binary_accura - ETA: 0s - loss: 8.9887 - binary_accuracy: 0.41\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9941 - binary_accu - ETA: 4s - loss: 8.9953 - binary_accuracy:  - ETA: 4s - loss: 8.9906 - bi - ETA: 2s - loss: 8.9915 - binary_accura - ETA: 2s - loss: 8.9954 - binary_acc - ETA: 1s - loss: 8.9909 - \n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9828 - binary_a - ETA: 0s - loss: 8.9860 - binary_\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 -  - ETA: 5s - loss: 8.9893 - binary_accuracy - ETA: 4s -  - ETA: 2s - loss: 8. - ETA: 0s - loss: 8.9907 - binary_accu\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - los - ETA: 7s  - ETA: 4s - loss: 8.9780 - binary_accuracy - ETA: 4s - loss: 8.9814 - binary_accurac - ETA - ETA: 1s - loss: 8.9892 - bina\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 8.9897 - binary_accuracy: 0.4172s - loss: 8.97 - ETA: 9s - loss: 8.9838 - binary_accu - ETA: 8s - loss: 8.9843 - binary_accuracy: 0.4 - ETA: 8s - loss: 8.9827 - binary_accurac - ETA: 1s - loss: 8.9859 - bi\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9901 - binary_accuracy: 0\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9979 - binary_ac - ETA: 5s - loss: 8.9950 - binary_accuracy - ETA: 5s - loss: 8.9909 - b - ETA: 3s - loss: 8. - ETA: 2s - loss: 8.9897 -  - ETA: 0s - loss: 8.9860 - binary_accura\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.95 - ETA: 1s - loss: 8.9847 - binar\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 8.9897 - binary_accuracy: 0.4172 - loss: 8.9766 - - ETA: 1s - loss: 8.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-11205544.0, -11205552.0, -11205560.0, -11205568.0, -11205576.0, -11205584.0, -11205592.0, -11205600.0, -11205608.0, -11205616.0, -11205624.0, -11205632.0, -11205640.0, -11205648.0, -11205656.0, -11207032.0, -11207040.0, -11207048.0, -11207056.0, -11207064.0, -11207072.0, -11207080.0, -11207088.0, -11207096.0, -11207104.0, -11207112.0, -11207120.0, -11207128.0, -11207136.0, -11207144.0, -11207152.0, -11207160.0, -11207168.0, -11207176.0, -11207184.0, -11207192.0, -11207200.0, -11207208.0, -11207216.0, -11207224.0, -11207232.0, -11207240.0, -11207248.0, -11207256.0, -11207264.0, -11207272.0, -11207280.0, -11207288.0, -11207296.0, -11207304.0, -11207312.0, -11207320.0, -11207328.0, -11207336.0, -11207344.0, -11207352.0, -11207360.0, -11207368.0, -11207376.0, -11207384.0, -11207392.0, -11207400.0, -11207408.0, -11207416.0, -11207424.0, -11207432.0, -11207440.0, -11207448.0, -11207456.0, -11207464.0, -11207472.0, -11207480.0, -11207488.0, -11207496.0, -11207504.0, -11207512.0, -11207520.0, -11207528.0, -11207536.0, -11207544.0, -11207552.0, -11207560.0, -11207568.0, -11207576.0, -11207584.0, -11207592.0, -11207600.0, -11207608.0, -11207616.0, -11207624.0, -11207632.0, -11207640.0, -11207648.0, -11207656.0, -11207664.0, -11207672.0, -11207680.0, -11207688.0, -11207696.0, -11207704.0, -11207712.0, -11207720.0, -11207728.0, -11207736.0, -11207744.0, -11207752.0, -11207760.0, -11207768.0, -11207776.0, -11207784.0, -11207792.0, -11207800.0, -11207808.0, -11207816.0, -11207824.0, -11207832.0, -11207840.0, -11207848.0, -11207856.0, -11207864.0, -11207872.0, -11207880.0, -11207888.0, -11207896.0, -11207904.0, -11207912.0, -11207920.0, -11207928.0, -11207936.0, -11207944.0, -11207952.0, -11207960.0, -11207968.0, -11207976.0, -11207984.0, -11207992.0, -11208000.0, -11208008.0, -11208016.0, -11208024.0, -11208032.0, -11208040.0, -11208048.0, -11208056.0, -11208064.0, -11208072.0, -11208080.0, -11208088.0, -11208096.0, -11208104.0, -11208112.0, -11208120.0, -11208128.0, -11208136.0, -11208144.0, -11208152.0, -11208160.0, -11208168.0, -11208176.0, -11208184.0, -11208192.0, -11208200.0, -11208208.0, -11208216.0, -11208224.0, -11208232.0, -11208240.0, -11208248.0, -11208256.0, -11208264.0, -11208272.0, -11208280.0, -11208288.0, -11208296.0, -11208304.0, -11208312.0, -11208320.0, -11208328.0, -11208336.0, -11208344.0, -11208352.0, -11208360.0, -11208368.0, -11208376.0, -11208384.0, -11208392.0, -11208400.0, -11208408.0, -11208416.0, -11208424.0, -11208432.0, -11208440.0, -11208448.0, -11208456.0, -11208464.0, -11208472.0, -11208480.0, -11208488.0, -11208496.0, -11208504.0, -11208512.0, -11208520.0, -11208528.0, -11208536.0, -11208544.0, -11208552.0, -11208560.0, -11208568.0, -11208576.0, -11208584.0, -11208592.0, -11208600.0, -11208608.0, -11208616.0, -11208624.0, -11208632.0, -11208640.0, -11208648.0, -11208656.0, -11208664.0, -11208672.0, -11208680.0, -11208688.0, -11208696.0, -11208704.0, -11208712.0, -11208720.0, -11208728.0, -11208736.0, -11208744.0, -11208752.0, -11208760.0, -11208768.0, -11208776.0, -11208784.0, -11208792.0, -11208800.0, -11208808.0, -11208816.0, -11208824.0, -11208832.0, -11208840.0, -11208848.0, -11208856.0, -11208864.0, -11208872.0, -11208880.0, -11208888.0, -11208896.0, -11208904.0, -11208912.0, -11208920.0, -11208928.0, -11208936.0, -11208944.0, -11208952.0, -11208960.0, -11208968.0, -11208976.0, -11208984.0, -11208992.0, -11209000.0, -11209008.0, -11209016.0, -11209024.0, -11209032.0, -11209040.0, -11209048.0, -11209056.0, -11209064.0, -11209072.0, -11209080.0, -11209088.0, -11209096.0, -11209104.0, -11209112.0, -11209120.0, -11209128.0, -11209136.0, -11209144.0, -11209152.0, -11209160.0, -11209168.0, -11209176.0, -11209184.0, -11209192.0, -11209200.0, -11209208.0, -11209216.0, -11209224.0, -11209232.0, -11209240.0, -11209248.0, -11209256.0, -11209264.0, -11209272.0, -11209280.0, -11209288.0, -11209296.0, -11209304.0, -11209312.0, -11209320.0, -11209328.0, -11209336.0, -11209344.0, -11209352.0, -11209360.0, -11209368.0, -11209376.0, -11209384.0, -11209392.0, -11209400.0, -11209408.0, -11209416.0, -11209424.0, -11209432.0, -11209440.0, -11209448.0, -11209456.0, -11209464.0, -11209472.0, -11209480.0, -11209488.0, -11209496.0, -11209504.0, -11209512.0, -11209520.0, -11209528.0, -11209536.0, -11209544.0, -11209552.0, -11209560.0, -11209568.0, -11209576.0, -11209584.0, -11209592.0, -11209600.0, -11209608.0, -11209616.0, -11209624.0, -11209632.0, -11209640.0, -11209648.0, -11209656.0, -11209664.0, -11209672.0, -11209680.0, -11209688.0, -11209696.0, -11209704.0, -11209712.0, -11209720.0, -11209728.0, -11209736.0, -11209744.0, -11209752.0, -11209760.0, -11209768.0, -11209776.0, -11209784.0, -11209792.0, -11209800.0, -11209808.0, -11209816.0, -11209824.0, -11209832.0, -11209840.0, -11209848.0, -11209856.0, -11209864.0, -11209872.0, -11209880.0, -11209888.0, -11209896.0, -11209904.0, -11209912.0, -11209920.0, -11209928.0, -11209936.0, -11209944.0, -11209952.0, -11209960.0, -11209968.0, -11209976.0, -11209984.0, -11209992.0, -11210000.0, -11210008.0, -11210016.0, -11210024.0, -11210032.0, -11210040.0, -11210048.0, -11210056.0, -11210064.0, -11210072.0, -11210080.0, -11210088.0, -11210096.0, -11210104.0, -11210112.0, -11210120.0, -11210128.0, -11210136.0, -11210144.0, -11210152.0, -11210160.0, -11210168.0, -11210176.0, -11210184.0, -11210192.0, -11210200.0, -11210208.0, -11210216.0, -11210224.0, -11210232.0, -11210240.0, -11210248.0, -11210256.0, -11210264.0, -11210272.0, -11210280.0, -11210288.0, -11210296.0, -11210304.0, -11210312.0, -11210320.0, -11210328.0, -11210336.0, -11210344.0, -11210352.0, -11210360.0, -11210368.0, -11210376.0, -11210384.0, -11210392.0, -11210400.0, -11210408.0, -11210416.0, -11210424.0, -11210432.0, -11210440.0, -11210448.0, -11210456.0, -11210464.0, -11210472.0, -11210480.0, -11210488.0, -11210496.0, -11210504.0, -11210512.0, -11210520.0, -11210528.0, -11210536.0, -11210544.0, -11210552.0, -11210560.0, -11210568.0, -11210576.0, -11210584.0, -11210592.0, -11210600.0, -11210608.0, -11210616.0, -11210624.0, -11210632.0, -11210640.0, -11210648.0, -11210656.0, -11210664.0, -11210672.0, -11210680.0, -11210688.0, -11210696.0, -11210704.0, -11210712.0, -11210720.0, -11210728.0, -11210736.0, -11210744.0, -11210752.0, -11210760.0, -11210768.0, -11210776.0, -11210784.0, -11210792.0, -11210800.0, -11210808.0, -11210816.0, -11210824.0, -11210832.0, -11210840.0, -11210848.0, -11210856.0, -11210864.0, -11210872.0, -11210880.0, -11210888.0, -11210896.0, -11210904.0, -11210912.0, -11210920.0, -11210928.0, -11210936.0, -11210944.0, -11210952.0, -11210960.0, -11210968.0, -11210976.0, -11210984.0, -11210992.0, -11211000.0, -11211008.0, -11211016.0, -11211024.0, -11211032.0, -11211040.0, -11211048.0, -11211056.0, -11211064.0, -11211072.0, -11211080.0, -11211088.0, -11211096.0, -11211104.0, -11211112.0, -11211120.0, -11211128.0, -11211136.0, -11211144.0, -11211152.0, -11211160.0, -11211168.0, -11211176.0, -11211184.0, -11211192.0, -11211200.0, -11211208.0, -11211216.0, -11211224.0, -11211232.0, -11211240.0, -11211248.0, -11211256.0, -11211264.0, -11211272.0, -11211280.0, -11211288.0, -11211296.0, -11211304.0, -11211312.0, -11211320.0, -11211328.0, -11211336.0, -11211344.0, -11211352.0, -11211360.0, -11211368.0, -11211376.0, -11211384.0, -11211392.0, -11211400.0, -11211408.0, -11211416.0, -11211424.0, -11211432.0, -11211440.0, -11211448.0, -11211456.0, -11211464.0, -11211472.0, -11211480.0, -11211488.0, -11211496.0, -11211504.0, -11211512.0, -11211520.0, -11211528.0, -11211536.0, -11211544.0, -11211552.0, -11211560.0, -11211568.0, -11211576.0, -11211584.0, -11211592.0, -11211600.0, -11211608.0, -11211616.0, -11211624.0, -11211632.0, -11211640.0, -11211648.0, -11211656.0, -11211664.0, -11211672.0, -11211680.0, -11211688.0, -11211696.0, -11211704.0, -11211712.0, -11211720.0, -11211728.0, -11211736.0, -11211744.0, -11211752.0, -11211760.0, -11211768.0, -11211776.0, -11211784.0, -11211792.0, -11211800.0, -11211808.0, -11211816.0, -11211824.0, -11211832.0, -11211840.0, -11211848.0, -11211856.0, -11211864.0, -11211872.0, -11214456.0, -11214464.0, -11214472.0, -11214480.0, -11214488.0, -11214496.0, -11214504.0, -11214512.0, -11214520.0, -11214528.0, -11214536.0, -11214544.0, -11214552.0, -11214560.0, -11214568.0, -11214576.0, -11214584.0, -11214592.0, -11214600.0, -11214608.0, -11214616.0, -11214624.0, -11214632.0, -11214640.0, -11214648.0, -11214656.0, -11214664.0, -11214672.0, -11214680.0, -11214688.0, -11214696.0, -11214704.0, -11214712.0, -11214720.0, -11214728.0, -11214736.0, -11214744.0, -11214752.0, -11214760.0, -11214768.0, -11214776.0, -11214784.0, -11214792.0, -11214800.0, -11214808.0, -11214816.0, -11214824.0, -11214832.0, -11214840.0, -11214848.0, -11214856.0, -11214864.0, -11214872.0, -11214880.0, -11214888.0, -11214896.0, -11214904.0, -11214912.0, -11214920.0, -11214928.0, -11214936.0, -11214944.0, -11214952.0, -11214960.0, -11214968.0, -11214976.0, -11214984.0, -11214992.0, -11215000.0, -11215008.0, -11215016.0, -11215024.0, -11215032.0, -11215040.0, -11215048.0, -11215056.0, -11215064.0, -11215072.0, -11215080.0, -11215088.0, -11215096.0, -11215104.0, -11215112.0, -11215120.0, -11215128.0, -11215136.0, -11215144.0, -11215152.0, -11215160.0, -11215168.0, -11215176.0, -11215184.0, -11215192.0, -11215200.0, -11215208.0, -11215216.0, -11215224.0, -11215232.0, -11215240.0, -11215248.0, -11215256.0, -11215264.0, -11215272.0, -11215280.0, -11215288.0, -11215296.0, -11215304.0, -11215312.0, -11215320.0, -11215328.0, -11215336.0, -11215344.0, -11215352.0, -11215360.0, -11215368.0, -11215376.0, -11215384.0, -11215392.0, -11215400.0, -11215408.0, -11215416.0, -11215424.0, -11215432.0, -11215440.0, -11215448.0, -11215456.0, -11215464.0, -11215472.0, -11215480.0, -11215488.0, -11215496.0, -11215504.0, -11215512.0, -11215520.0, -11215528.0, -11215536.0, -11215544.0, -11215552.0, -11215560.0, -11215568.0, -11215576.0, -11215584.0, -11215592.0, -11215600.0, -11215608.0, -11215616.0, -11215624.0, -11215632.0, -11215640.0, -11215648.0, -11215656.0, -11215664.0, -11215672.0, -11215680.0, -11215688.0, -11215696.0, -11215704.0, -11215712.0, -11215720.0, -11215728.0, -11215736.0, -11215744.0, -11215752.0, -11215760.0, -11215768.0, -11215776.0, -11215784.0, -11215792.0, -11215800.0, -11215808.0, -96853890.0, -96853896.0, -96853900.0, -96853910.0, -96853920.0, -96853930.0, -96853940.0, -96853944.0, -96853950.0, -96853960.0, -96853970.0, -96853976.0, -96853980.0, -96853990.0, -96854000.0, -96854010.0, -96854020.0, -96854024.0, -96854030.0, -96854040.0, -96854050.0, -96854056.0, -96854060.0, -96854070.0, -96854080.0, -96854090.0, -96854100.0, -96854104.0, -96854110.0, -96854120.0, -96854130.0, -96854136.0, -96854140.0, -96854150.0, -96854160.0, -96854170.0, -96854180.0, -96854184.0, -96854190.0, -96854200.0, -96854210.0, -96854216.0, -96854220.0, -96854230.0, -96854240.0, -96854250.0, -96854260.0, -96854264.0, -96854270.0, -96854280.0, -96854290.0, -96854296.0, -96854300.0, -96854310.0, -96854320.0, -96854330.0, -96854340.0, -96854344.0, -96854350.0, -96854360.0, -96854370.0, -96854376.0, -96854380.0, -96854390.0, -96854400.0, -96854410.0, -96854420.0, -96854424.0, -96854430.0, -96854440.0, -96854450.0, -96854456.0, -96854460.0, -96854470.0, -96854480.0, -96854490.0, -96854500.0, -96854504.0, -96854510.0, -96854520.0, -96854530.0, -96854536.0, -96854540.0, -96854550.0, -96854560.0, -96854570.0, -96854580.0, -96854584.0, -96854590.0, -96854600.0, -96854610.0, -96854616.0, -96854620.0, -96854630.0, -96854640.0, -96854650.0, -96854660.0, -96854670.0, -96854690.0, -96854696.0, -96854700.0, -96854710.0, -96854720.0, -96854730.0, -96854740.0, -96854744.0, -96854750.0, -96854760.0, -96854770.0, -96854776.0, -96854780.0, -96854790.0, -96854800.0, -96854810.0, -96854820.0, -96854824.0, -96854830.0, -96854840.0, -96854850.0, -96854856.0, -96854860.0, -96854870.0, -96854880.0, -96854890.0, -96854900.0, -96854904.0, -96854910.0, -96854920.0, -96854930.0, -96854936.0, -96854940.0, -96854950.0, -96854960.0, -96854970.0, -96854980.0, -96854984.0, -96854990.0, -96855000.0, -96855010.0, -96855016.0, -96855020.0, -96855030.0, -96855040.0, -96855050.0, -96855060.0, -96855064.0, -96855070.0, -96855080.0, -96855090.0, -96855096.0, -96855100.0, -96855110.0, -96855120.0, -96855130.0, -96855140.0, -96855144.0, -96855150.0, -96855160.0, -96855170.0, -96855176.0, -96855180.0, -96855190.0, -96855200.0, -96855210.0, -96855220.0, -96855224.0, -96855230.0, -96855240.0, -96855250.0, -96855256.0, -96855260.0, -96855270.0, -96855280.0, -96855290.0, -96855300.0, -96855304.0, -96855310.0, -96855320.0, -96855330.0, -96855336.0, -96855340.0, -96855350.0, -96855360.0, -96855370.0, -96855380.0, -96855384.0, -96855390.0, -96855400.0, -96855410.0, -96855416.0, -96855420.0, -96855430.0, -96855440.0, -96855450.0, -96855460.0, -96855464.0, -96855470.0, -96855480.0, -96855490.0, -96855496.0, -96855500.0, -96855510.0, -96855520.0, -96855530.0, -96855540.0, -96855544.0, -96855550.0, -96855560.0, -96855570.0, -96855576.0, -96855580.0, -96855590.0, -96855600.0, -96855610.0, -96855620.0, -96855624.0, -96855630.0, -96855640.0, -96855650.0, -96855656.0, -96855660.0, -96855670.0, -96855680.0, -96855690.0, -96855700.0, -96855704.0, -96855710.0, -96855720.0, -96855730.0, -96855736.0, -96855740.0, -96855750.0, -96855760.0, -96855770.0, -96855780.0, -96855784.0, -96855790.0, -96855800.0, -96855810.0, -96855816.0, -96855820.0, -96855830.0, -96855840.0, -96855850.0, -96855860.0, -96855864.0, -96855870.0, -96855880.0, -96855890.0, -96855896.0, -96855900.0, -96855910.0, -96855920.0, -96855930.0, -96855940.0, -96855944.0, -96855950.0, -96855960.0, -96855970.0, -96855976.0, -96855980.0, -96855990.0, -96856000.0, -96856010.0, -96856020.0, -96856024.0, -96856030.0, -96856040.0, -96856050.0, -96856056.0, -96856060.0, -96856070.0, -96856080.0, -96856090.0, -96856100.0, -96856104.0, -96856110.0, -96856120.0, -96856130.0, -96856136.0, -96856140.0, -96856150.0, -96856160.0, -96856170.0, -96856180.0, -96856184.0, -96856190.0, -96856200.0, -96856210.0, -96856216.0, -96856220.0, -96856230.0, -96856240.0, -96856250.0, -96856260.0, -96856264.0, -96856270.0, -96856280.0, -96856290.0, -96856296.0, -96856300.0, -96856310.0, -96856320.0, -96856330.0, -96856340.0, -96856344.0, -96856350.0, -96856360.0, -96856370.0, -96856376.0, -96856380.0, -96856390.0, -96856400.0, -96856410.0, -96856420.0, -96856424.0, -96856430.0, -96856440.0, -96856450.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8002517834662191\n",
      "Hamming Loss: 0.10560917610854664\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     18474\n",
      "           1       0.14      1.00      0.24      2973\n",
      "\n",
      "    accuracy                           0.14     21447\n",
      "   macro avg       0.07      0.50      0.12     21447\n",
      "weighted avg       0.02      0.14      0.03     21447\n",
      "\n",
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 14s 166us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3612 - binary_accu - ETA:  - ETA: 11s - loss: 6.3652 - binary_accuracy:  - ETA: 10s - loss\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3835 - binary_accuracy: 0.583 - ETA: 5s - loss: 6.3848 - - ETA: 4s - loss - ETA: 2s - loss: 6.3838  - ETA: 0s - loss: 6.3798 - binary_accuracy\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - lo - ETA: 1s - loss: 6.3716 - bin\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3768 - binary_acc - ETA: 4s - loss: 6.3 - ETA: 2s - loss: 6.3836 - binary_accuracy: 0. - ETA: 2s - loss: 6.383 - ETA: 0s - loss: 6.3807 - binary_accurac\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.379 - ETA: 4s - los - ETA: 2s - los\n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss:  - ETA: 8s - loss - ETA: 6s - loss: 6.3920 - binary_accuracy: 0.583 -  - ETA: 3s - loss: 6.3872 - binary_accuracy:\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.4304 - binary_accuracy - ETA: 10s - los - ETA: 8s - l\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839A: 0s - loss: 6.3804 - binary_accuracy: 0.583\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3764 - b\n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 \n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - lo\n",
      "Epoch 15/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3757  - ETA: 8s - loss: 6.3842 - binar - ETA: 7s - loss: 6.3902 - binar - ETA: 5s - loss: 6.3923 - binar - ETA: 4s - loss: 6.38\n",
      "Epoch 16/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6. - ETA: 6s -  - ETA: 4s - loss: 6.3724 - binary_accur - ETA: 3 - ETA: 1s - loss: 6.3724 - b\n",
      "Epoch 17/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.4060 - bi - ETA: 7s - loss: 6.3951 - binary_ac - ETA: 6s - loss: 6.3938 - binary_ - ETA: 5s - l - ETA: 3s - loss: 6.3854 - bin - ETA: 1s - loss: 6\n",
      "Epoch 18/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3900 - binary_ - ETA: 8s - loss: 6.3917 - bin - ETA: 7s - loss: 6.3892 - binary_accuracy: 0 - ETA: 6s - loss: - ETA: 4s -  - ETA: 2s - l\n",
      "Epoch 19/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 20/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3801 - binary_accuracy:  - ETA: 7s - loss:  - ETA: 5s -  - ETA: - ETA: 0s - loss: 6.3827 - binary_accuracy: \n",
      "Epoch 21/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 22/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3641 - binar - ETA: 9s - loss: 6.3739 - binary_accurac - ETA: 8s - loss: 6.3684 -  - ETA: 7s - loss: 6. - ETA: 5s - loss: 6.3749 - binary_accur - ETA: 4s - loss: 6.3765 - bi\n",
      "Epoch 23/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 6.3808 - binary_accuracy: 0.5839: 7s - loss: 6.3868 - binary_accurac - ETA: 7s - loss: 6.3902  - ETA: 5s - loss: 6.3983 - bin - ETA: 4s - loss: 6.3933 - binary_accura - ETA: 3s - loss: 6.3888 - binary_accuracy: 0.58 - ETA: 3s - loss: 6.3869 - bin - ETA: 2\n",
      "Epoch 24/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - ETA:\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839- ETA: 1s - loss: 6.3777 - binar\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss:  - ETA: 2s - loss: 6.3840 - binar - ETA: 1s - loss: 6.3809 - bin\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3848 - binary_accuracy - ETA: 1s - loss: 6.3849 - bina\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3847 - binary_accur - ETA: - ETA: 0s - loss: 6.3810 - binary_accuracy\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3960 - binary_ - ETA: 5s - loss: 6.3931 - binary_accuracy:  - ETA: 5s  - ETA: 2s - loss: 6.3805 - binary_accuracy: 0.58 - ETA: 2s -\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3810 - binary_accuracy: 0.583\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: - ETA: 5s - loss:  - - ETA: 0s - loss: 6.3793 - binary_accura\n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3797 - b - ETA: 0s - loss: 6.3800 - binary_accuracy: \n",
      "Epoch 33/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3829 -\n",
      "Epoch 34/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.4031 - binary_accuracy: 0. - ETA: 8s - loss: 6.4050 - binary_ac - ETA: 4s - loss: 6.3824 - binary_a - ETA: 3s - loss: 6.3830 - binary_accuracy: - ETA: 3s - loss: 6.38 - ETA: 1s - loss: 6.3794 - binary_accuracy: 0.5 - ETA: 1s - loss: 6.3807 \n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 13s 157us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3835 - binary_acc\n",
      "Epoch 36/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3787 - b - ETA: 0s - loss: 6.3803 - binary_acc\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - los\n",
      "Epoch 38/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3877 - ETA: 1s - loss: 6.3861 - binary_accuracy - ETA: 1s - loss: 6.3843 - binary\n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3809 - binary_accu\n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3501 - ETA: 11s - loss: 6. - ETA: 10s - loss: 6.364 - ETA: 2s - loss: 6.3902 - binary_accuracy - ETA: 2s - l\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss - ETA: 1s - loss: 6.3811 - bi - ETA: 0s - loss: 6.3815 - binary_accuracy: 0.583\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss  - ETA: 1s - loss: 6.3782 - binary_accur - ETA: 0s - loss: 6.3817 - binary_accura\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.4084 - binar - ETA: 9s - - ETA: 6s  - ETA: 4s - loss: 6.3838 - binary_accuracy: 0.58 - ETA: 4s - loss: 6.3832 - binary_acc - ETA: 0s - loss: 6.3809 - binary_accuracy: \n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3855\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3880 -  - ETA: 0s - loss: 6.3792 - binary_acc\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3510 - binary_accura - ETA: 5 - ETA: 3s - loss: 6.3633 - binary_accura - ETA: 2s - lo - ETA: 0s - loss: 6.3785 - binary_accuracy:\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 14s 162us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3 - E - ETA: 5s - loss: 6.3918 - binary_accuracy - ETA: 5s - loss\n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839  - ETA: 6s - loss:  - ETA: 4s - lo - ETA: 2s - loss: 6.3816 - binary_accuracy: 0.58 - ETA: 1s - loss: 6.3813 - binary_accuracy - ETA: 1s - loss: 6.3839 - binary_accu - ETA: 0s - loss: 6.3799 - binary_accur - ETA: 0s - loss: 6.3811 - binary_accuracy: 0.583\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3802 - binary_accuracy - ETA: 0s - loss: 6.3801 - binary_accuracy: 0.58\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3886 - binary_accu - ETA: 5s - loss: 6.3815 - binary_accuracy: 0.583 - ETA: 5s - loss: 6.3827 - binary_accuracy: 0 - ETA: 4s - loss - ETA: 2s - loss: 6.3848 - binary_accu - ETA: 2s - loss\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3712 -  - ETA: 0s - loss: 6.3813 - binary_accu\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3985 - bin - ETA: 7s - loss: 6.3893 - binary_accur - ETA: 7s - loss: 6.3893 - binary_\n",
      "Epoch 54/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3725  - ETA: 6s - loss: 6.3723 - binary_ac - ETA: 2s - loss\n",
      "Epoch 55/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.382\n",
      "Epoch 56/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3923 - binar\n",
      "Epoch 57/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 58/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6. - ETA: 10s - loss: 6.360 - ETA: 6s - loss: 6 - ETA: 0s - loss: 6.3787 - binary_accuracy: 0 - ETA: 0s - loss: 6.3795 - binary_accura\n",
      "Epoch 59/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 60/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 61/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 62/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 63/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3796 - binary_accuracy: 0 - ETA: 1s - loss: 6.3812 - binar\n",
      "Epoch 64/100\n",
      "85788/85788 [==============================] - 14s 159us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3329 -  - ETA: 9s - loss: 6.3727 - bi - ETA: 8s - loss: 6.3683 - binary_accuracy: 0.584 - ETA: 8s - loss: 6.36 - ETA: 7s - loss: 6.3607 - binary_accuracy: 0.5 - ETA: 6s - loss: 6 - ETA: 2s - loss: 6.3816 - binary_accuracy: 0.58 - ETA: 2s - loss: 6.3822 - ETA: 0s - loss: 6.3793 - binary_accur\n",
      "Epoch 65/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 66/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839TA: 6s - loss: 6.3777 - binary_accurac - ETA: 6s - loss: 6.3756 - binary_ - ETA: 5s - loss:\n",
      "Epoch 67/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - ETA: 0s - loss: 6.3813 - binary_accuracy: 0\n",
      "Epoch 68/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3  - ETA: 6s - loss: 6.3795 - binary_accuracy: 0. - - ETA: 0s - loss: 6.3804 - binary_accuracy: \n",
      "Epoch 69/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839ETA - ETA: 1s - loss: 6.3852 - binary_accur - ETA: 1s - loss: 6.3866 - binary_ - ETA: 0s - loss: 6.3815 - binary_accuracy: 0.58\n",
      "Epoch 70/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: - ETA: 5s - loss: 6.3758 - binary - ETA: 1s - loss: 6.3775\n",
      "Epoch 71/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3674 - binary_ - ET - ETA: 8s - loss: 6.382 - ETA: 3s - loss: 6.3906 - binary_accuracy - E\n",
      "Epoch 72/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6 - ETA: 2s - l\n",
      "Epoch 73/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 74/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - lo - ETA:  - ETA: 1s - loss: 6.3772 -\n",
      "Epoch 75/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - los\n",
      "Epoch 76/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3607 - binar - ETA: 3s - loss: 6.37 - ETA: 2s - los\n",
      "Epoch 77/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839A: 7s - loss: 6.3884 - binary_accuracy: 0.583 - ETA: 7s - loss: 6.3892 - binary_accu - ETA: 3s - loss: 6.3958 - binary_accuracy: 0.58 - ETA: 2s - loss: 6.3966 - b - ETA: 1s - loss: 6.3913 \n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3608 - binary_ac - ETA: 6s - loss: 6.3688  - ETA:  - ETA: 2s - loss:\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - ETA: 8s - lo - ETA: 0s - loss: 6.3824 - binary_accuracy: 0.58\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3717 - binary_accuracy - ETA: 8 - ETA: 5s - loss: 6.3  - ETA: 0s - loss: 6.3800 - binary_acc\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3861 - binary_accuracy:  - ETA: 6s - loss: 6.3841 - binary_a - ETA: 5s - ETA: 3s - loss: 6.3 - ETA: 1s - loss: 6.3807 - bi\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3858 -  - ETA: 1s - loss: 6.3\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss - ETA: 1s - loss: 6.3774 - binary_ac - ETA: 0s - loss: 6.3810 - binary_accura\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.4060 - binary_ac - ETA: 10s - loss: 6.3810 - binary_accura - ETA:  - ETA: 6s - loss: 6.3821 - binary_ac - \n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 6.3808 - binary_accuracy: 0.5839ETA: 4s - loss: 6.3877 - binary_\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 13s 155us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.3920 - binary_acc - ETA: 9s - ETA: 6s - \n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss - ETA: 1s - loss: 6.3779 - binary_accur - ETA: 0s - loss: 6.3791 - binary_accuracy:\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839: 9s - lo - ETA: 7s - loss - ETA: 5s - loss: 6.3778 - binary_accuracy - ETA: 5s - loss: 6.3737 - binary_accuracy: 0 - ETA: 4s - los - ETA: 2s - loss: 6.3793 - binary_a - ETA: 1s - loss: 6.3\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3901 - binary - ETA: 1s - loss: 6.\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839 - loss: 6.3  - ETA: 0s - loss: 6.3775 - binary_accurac\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - ETA: 8s - loss: 6.3948 - bina - ETA: 7s - loss: 6.3900 - binary_acc\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6 - ETA: 7s - loss: 6.3918 - binary_acc - ETA: 6s - loss: 6.389 - ETA - ETA: 1s - loss: 6.3854 - binary_accuracy:  - ETA: 1s - loss: 6.3816 - binary_accur - ETA: 0s - loss: 6.3805 - binary_ac\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - lo - ETA: 9s - loss: 6.3771 - binary - ETA: 8s - loss: 6.3679 - binary_ - ETA: 7s - loss: 6.3648 - binary_accuracy: 0.5 - ETA: 7s - loss: 6.3635 - binary_accuracy: 0.585 - ETA: 7s - loss: 6.3643 - binary_accu - ETA: 3s - loss: 6.3771 - - ETA: 1s - loss: 6.3760 -  - ETA: 0s - loss: 6.3786 - binary_accuracy: 0\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 6.3808 - binary_accuracy: 0.5839s - loss: 6.37 - ETA: 6s - loss: 6.3670 - - ETA: 4s - loss: 6.3711 - binary_accur - ETA: 4s - loss: 6.3737 - ETA: 2s - loss: 6.3750 - bi - ETA: 1s - loss: 6.3755 - binary_accuracy: - ETA: 0s - loss: 6.3779 - binary_accu - ETA: 0s - loss: 6.3774 - binary_accuracy: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [43162984.0, 43162990.0, 43163000.0, 43163010.0, 43163016.0, 43163024.0, 43163030.0, 43163040.0, 43163050.0, 43163056.0, 43163064.0, 43163070.0, 43163080.0, 43163090.0, 43163096.0, 43163104.0, 43163110.0, 43163120.0, 43163130.0, 43163136.0, 43163144.0, 43163150.0, 43163160.0, 43163170.0, 43163816.0, 43163830.0, 43163840.0, 43163850.0, 43163856.0, 43163864.0, 43163870.0, 43163880.0, 43163890.0, 43163896.0, 43163904.0, 43163910.0, 43163920.0, 43163930.0, 43163936.0, 43163944.0, 43163950.0, 43163960.0, 43163970.0, 43163976.0, 43163984.0, 43163990.0, 43164000.0, 43164010.0, 43164016.0, 43164024.0, 43164030.0, 43164040.0, 43164050.0, 43164056.0, 43164064.0, 43164070.0, 43164760.0, 43164770.0, 43164776.0, 43164784.0, 43164790.0, 43164800.0, 43164810.0, 43164816.0, 43164824.0, 43164830.0, 43164840.0, 43164850.0, 43164856.0, 43164864.0, 43164870.0, 43164880.0, 43164890.0, 43164896.0, 43164904.0, 43164910.0, 43164920.0, 43164930.0, 43164936.0, 43164944.0, 43164950.0, 43164960.0, 43164970.0, 43164976.0, 43164984.0, 43164990.0, 43165000.0, 43165010.0, 43165016.0, 43165024.0, 43165030.0, 43165040.0, 43165050.0, 43165230.0, 43165250.0, 43165256.0, 43165264.0, 43165270.0, 43165280.0, 43165290.0, 43165296.0, 43165304.0, 43165310.0, 43165320.0, 43165330.0, 43165336.0, 43165344.0, 43165350.0, 43165360.0, 43165370.0, 43165376.0, 43165384.0, 43165390.0, 43165400.0, 43165410.0, 43165416.0, 43165424.0, 43165430.0, 43165440.0, 43165450.0, 43165456.0, 43165464.0, 43165470.0, 43165480.0, 43165490.0, 43165496.0, 43165504.0, 43165510.0, 43165520.0, 43165530.0, 43165536.0, 43165544.0, 43165550.0, 43165560.0, 43165570.0, 43165576.0, 43165584.0, 43165590.0, 43165600.0, 43165610.0, 43165616.0, 43165624.0, 43165630.0, 43165640.0, 43165650.0, 43165656.0, 43165664.0, 43165670.0, 43165680.0, 43165690.0, 43165696.0, 43165704.0, 43165710.0, 43165720.0, 43165730.0, 43165736.0, 43165744.0, 43165750.0, 43165760.0, 43166096.0, 43166104.0, 43166110.0, 43166120.0, 43166130.0, 43166136.0, 43166144.0, 43166150.0, 43166160.0, 43166170.0, 43166176.0, 43166184.0, 43166190.0, 43166200.0, 43166210.0, 43166216.0, 43166224.0, 43166230.0, 43166240.0, 43166250.0, 43166256.0, 43166264.0, 43166270.0, 43166280.0, 43166290.0, 43166296.0, 43166304.0, 43166310.0, 43166320.0, 43166330.0, 43166336.0, 43168344.0, 43168370.0, 43168376.0, 43168384.0, 43168390.0, 43168400.0, 43168410.0, 43168416.0, 43168424.0, 43168430.0, 43168440.0, 43168450.0, 43168456.0, 43168464.0, 43168470.0, 43168480.0, 43168490.0, 43168496.0, 43168504.0, 43168510.0, 43168520.0, 43168530.0, 43168536.0, 43168544.0, 43168550.0, 43168560.0, 43168570.0, 43168576.0, 43168584.0, 43168590.0, 43168600.0, 43168610.0, 43168616.0, 43168624.0, 43168630.0, 43168640.0, 43168650.0, 43168656.0, 43168664.0, 43168670.0, 43168680.0, 43168690.0, 43168696.0, 43168704.0, 43168710.0, 43168720.0, 43168730.0, 43168736.0, 43168744.0, 43168750.0, 43168760.0, 43168770.0, 43168776.0, 43168784.0, 43168790.0, 43168800.0, 43168810.0, 43168816.0, 43168824.0, 43168830.0, 43168840.0, 43168850.0, 43168856.0, 43168864.0, 65089304.0, 65089310.0, 65089316.0, 65089320.0, 65089324.0, 65089330.0, 65089332.0, 65089336.0, 65089340.0, 65089344.0, 65089348.0, 65089350.0, 65089356.0, 65089360.0, 65089364.0, 65089370.0, 65089372.0, 65089376.0, 65089380.0, 65089384.0, 65089388.0, 65089390.0, 65089396.0, 65089400.0, 65089404.0, 65089410.0, 65089412.0, 65089416.0, 65089420.0, 65089424.0, 65089428.0, 65089430.0, 65089436.0, 65089440.0, 65089444.0, 65089450.0, 65089452.0, 65089456.0, 65089460.0, 65089464.0, 65089468.0, 65089470.0, 65089476.0, 65089480.0, 65089484.0, 65089490.0, 65089492.0, 65089496.0, 65089500.0, 65089504.0, 65089508.0, 65089510.0, 65089516.0, 65089520.0, 65090340.0, 65090344.0, 65090348.0, 65090350.0, 65090356.0, 65090360.0, 65090364.0, 65090370.0, 65090372.0, 65090376.0, 65090380.0, 65090384.0, 65090388.0, 65090390.0, 65090396.0, 65090400.0, 65090404.0, 65090410.0, 65090412.0, 65090416.0, 65090420.0, 65090424.0, 65090428.0, 65090430.0, 65090436.0, 65090440.0, 65090444.0, 65090450.0, 65090452.0, 65090456.0, 65090460.0, 65090464.0, 65090468.0, 65090470.0, 65090476.0, 65090480.0, 65090484.0, 65090490.0, 65090492.0, 65090496.0, 65090500.0, 65090504.0, 65090508.0, 65090510.0, 65090516.0, 65090520.0, 65090524.0, 65090530.0, 65090532.0, 65090536.0, 65090540.0, 65090544.0, 65090548.0, 65090550.0, 65090556.0, 65090560.0, 65090564.0, 65090570.0, 65090572.0, 65090576.0, 65090580.0, 65090584.0, 65090588.0, 65090590.0, 65090596.0, 65090600.0, 65090604.0, 65090610.0, 65090616.0, 65091572.0, 65091576.0, 65091580.0, 65091584.0, 65091588.0, 65091590.0, 65091596.0, 65091600.0, 65091604.0, 65091610.0, 65091612.0, 65091616.0, 65091620.0, 65091624.0, 65091628.0, 65091630.0, 65091636.0, 65091640.0, 65091644.0, 65091650.0, 65091652.0, 65091656.0, 65091660.0, 65091664.0, 65091668.0, 65091670.0, 65091676.0, 65091680.0, 65091684.0, 65091690.0, 65091692.0, 65091696.0, 65091700.0, 65091704.0, 65091708.0, 65091710.0, 65091716.0, 65091720.0, 65091724.0, 65091730.0, 65091732.0, 65091736.0, 65091740.0, 65091744.0, 65091748.0, 65091750.0, 65091756.0, 65091760.0, 65091764.0, 65091770.0, 65091772.0, 65091776.0, 65091780.0, 65091784.0, 65091788.0, 65091790.0, 65091796.0, 65091800.0, 65091804.0, 65091810.0, 65091812.0, 65091816.0, 65091820.0, 65091824.0, 65091828.0, 65092144.0, 65092150.0, 65092160.0, 65092170.0, 65092172.0, 65092176.0, 65092180.0, 65092184.0, 65092188.0, 65092190.0, 65092196.0, 65092200.0, 65092204.0, 65092210.0, 65092212.0, 65092216.0, 65092220.0, 65092224.0, 65092228.0, 65092230.0, 65092236.0, 65092240.0, 65092244.0, 65092250.0, 65092252.0, 65092256.0, 65092260.0, 65092264.0, 65092268.0, 65092270.0, 65092276.0, 65092280.0, 65092284.0, 65092290.0, 65092292.0, 65092296.0, 65092300.0, 65092304.0, 65092308.0, 65092310.0, 65092316.0, 65092320.0, 65092324.0, 65092330.0, 65092332.0, 65092336.0, 65092340.0, 65092344.0, 65092348.0, 65092350.0, 65092356.0, 65092360.0, 65092364.0, 65092370.0, 65092372.0, 65092376.0, 65092380.0, 65092384.0, 65092388.0, 65092390.0, 65092396.0, 65092400.0, 65092404.0, 65092410.0, 65092412.0, 65092416.0, 65092420.0, 65092424.0, 65092428.0, 65092430.0, 65092436.0, 65092440.0, 65092444.0, 65092450.0, 65092452.0, 65092456.0, 65092460.0, 65092464.0, 65092468.0, 65092470.0, 65092476.0, 65092480.0, 65092484.0, 65092490.0, 65092492.0, 65092496.0, 65092500.0, 65092504.0, 65092508.0, 65092510.0, 65092516.0, 65092520.0, 65092524.0, 65092530.0, 65092532.0, 65092536.0, 65092540.0, 65092544.0, 65092548.0, 65092550.0, 65092556.0, 65092560.0, 65092564.0, 65092570.0, 65092572.0, 65092576.0, 65092580.0, 65092584.0, 65092588.0, 65092590.0, 65092596.0, 65092600.0, 65092604.0, 65092610.0, 65092612.0, 65092616.0, 65092620.0, 65092624.0, 65092628.0, 65092630.0, 65092636.0, 65092640.0, 65092644.0, 65092650.0, 65092652.0, 65092656.0, 65092660.0, 65092664.0, 65092668.0, 65092670.0, 65092676.0, 65092680.0, 65092684.0, 65092690.0, 65092692.0, 65092696.0, 65092700.0, 65092704.0, 65092708.0, 65092710.0, 65092724.0, 65093150.0, 65093156.0, 65093160.0, 65093164.0, 65093170.0, 65093172.0, 65093176.0, 65093180.0, 65093184.0, 65093188.0, 65093190.0, 65093196.0, 65093200.0, 65093204.0, 65093210.0, 65093212.0, 65093216.0, 65093220.0, 65093224.0, 65093228.0, 65093230.0, 65093236.0, 65093240.0, 65093244.0, 65093250.0, 65093252.0, 65093256.0, 65093260.0, 65093264.0, 65093268.0, 65093270.0, 65093276.0, 65093280.0, 65093284.0, 65093290.0, 65093292.0, 65093296.0, 65093300.0, 65093304.0, 65093308.0, 65093310.0, 65093316.0, 65093320.0, 65093324.0, 65093330.0, 65093332.0, 65093336.0, 65093340.0, 65093344.0, 65093348.0, 65093350.0, 65093356.0, 65093360.0, 65093364.0, 65093370.0, 65093372.0, 65093376.0, 65093380.0, 65093384.0, 65093388.0, 65093390.0, 65093396.0, 65093400.0, 65093404.0, 65093410.0, 65093412.0, 65093416.0, 65093420.0, 65093424.0, 65093428.0, 65093430.0, 65093440.0, 65093450.0, 65096000.0, 65096004.0, 65096010.0, 65096012.0, 65096016.0, 65096020.0, 65096024.0, 65096028.0, 65096030.0, 65096036.0, 65096040.0, 65096044.0, 65096050.0, 65096052.0, 65096056.0, 65096060.0, 65096064.0, 65096068.0, 65096070.0, 65096076.0, 65096080.0, 65096084.0, 65096090.0, 65096092.0, 65096096.0, 65096100.0, 65096104.0, 65096108.0, 65096110.0, 65096116.0, 65096120.0, 65096124.0, 65096130.0, 65096132.0, 65096136.0, 65096140.0, 65096144.0, 65096148.0, 65096150.0, 65096156.0, 65096160.0, 65096164.0, 65096170.0, 65096172.0, 65096176.0, 65096180.0, 65096184.0, 65096188.0, 65096190.0, 65096196.0, 65096200.0, 65096204.0, 65096210.0, 65096212.0, 65096216.0, 65096220.0, 65096224.0, 65096228.0, 65096230.0, 65096236.0, 65096240.0, 65096244.0, 65096250.0, 65096252.0, 65096256.0, 65096260.0, 65096264.0, 65096268.0, 65096270.0, 65096276.0, 65096280.0, 65096284.0, 65096290.0, 65096292.0, 65096296.0, 65096300.0, 65096304.0, 65096308.0, 65096310.0, 65096316.0, 65096320.0, 65096340.0, 65096344.0, 65096350.0, 65096356.0, 65096360.0, 65096364.0, 65096370.0, 65096372.0, 65096376.0, 65096380.0, 65096384.0, 65096388.0, 65096390.0, 65096396.0, 65096400.0, 65096404.0, 65096410.0, 65096412.0, 65096416.0, 65096420.0, 65096424.0, 65096428.0, 65096430.0, 65096436.0, 65096440.0, 65096444.0, 65096450.0, 65096452.0, 65096456.0, 65096460.0, 65096464.0, 65096468.0, 65096470.0, 65096476.0, 65096480.0, 65096484.0, 65096490.0, 65096492.0, 65096496.0, 65096500.0, 65096504.0, 65096508.0, 65096510.0, 65096516.0, 65096520.0, 65096524.0, 65096530.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8155452977106356\n",
      "Hamming Loss: 0.09453536625169022\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91     18036\n",
      "           1       0.00      0.00      0.00      3411\n",
      "\n",
      "    accuracy                           0.84     21447\n",
      "   macro avg       0.42      0.50      0.46     21447\n",
      "weighted avg       0.71      0.84      0.77     21447\n",
      "\n",
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 15s 170us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - los - ETA: 7s - ETA: 1s - loss: 9.085\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.1285 - - ETA: 7s - loss: 9.1066 - ETA\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - ETA: 1s - loss: 9.0868 - binary\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112- ETA: 2s - loss: 9.0862 - binary_accuracy: 0.41 - ETA: \n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 9.0822 - binary_accuracy: 0.4112TA: 7s - loss: 9.0976 - - ETA: 5s - loss: 9 - ETA: 4s - ETA: 1s - loss: 9.0882 -\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0492 - binary - ETA: 7s - loss: 9.0564 - binary_ac - ETA: 6s - loss: 9.0557 - bina - ETA:\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 9.0822 - binary_accuracy: 0.4112TA: 1s - loss: 9.0832 - binary_accuracy: 0.41 - ETA: 1s - loss: 9.0823\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.07 - ETA: 10s - loss: 9.0704 - binary_accura - ETA: 10s - loss: 9.0621 - b - ETA: 8s \n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - 12s 138us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112- ETA: 7s - loss: 9.0743 - binary_accuracy: 0.411 - ETA:  - ETA: 1s - loss: 9.0864 -  - ETA: 0s - loss: 9.0826 - binary_accuracy: 0.411\n",
      "Epoch 15/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0835 - binary_accuracy: 0.41 - ETA: 4s - loss: 9.0816 - binary_accu - ETA: 3s - loss: 9.0832 - binary_accu - ETA: 2s - loss: 9.0819 - binary_accur - ETA: 1s - loss: 9.0808 - binary_ac - ETA: 1s - loss: 9.0785 - binary_accur - ETA: 0s - loss: 9.0808 - binary_accuracy:\n",
      "Epoch 16/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0902 - binary_accurac - ETA: 7s - loss: 9.0855 \n",
      "Epoch 17/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 18/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.1038 - binary_accuracy: 0 - ETA: 9s - loss: 9.0982 - binary_accuracy - ETA: 8s - loss: 9.0996 - binary_accuracy: 0.41 - ETA: 5s - loss - ETA: 0s - loss: 9.0839 - binary_accuracy: 0.\n",
      "Epoch 19/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0784 - b - ETA: 5s - loss: 9.0689 - ETA: 4s - loss: 9.07 - ETA: 2\n",
      "Epoch 20/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 21/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.1 - ETA: 8s - loss: 9.1044 - binary_accuracy: 0. - ETA: 7s - l - ETA: 5s - loss: 9.0876 - binary_a - ET - ETA: 1s - loss: 9.0806 - binary_accuracy: 0.411 - ETA: 1s - loss: 9.0802 - binary_ac - ETA: 0s - loss: 9.0805 - binary_ac\n",
      "Epoch 22/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0679 - binary_accuracy: 0. - ETA: 7s - loss: 9.0719  - ETA: 2s - loss: 9.0834 - binary_accuracy: 0.411 - ETA: \n",
      "Epoch 23/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - ETA - - ETA: 0s - loss: 9.0827 - binary_accuracy: 0.4 - ETA: 0s - loss: 9.0835 - binary_accuracy:  - ETA: 0s - loss: 9.0836 - binary_accuracy: 0.41\n",
      "Epoch 24/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 9.0822 - binary_accuracy: 0.41122s - loss: 9.0802 - - ETA: 0s - loss: 9.0821 - binary_accurac\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0749 - binary_accura - ETA: 2s - loss\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.0584 - bina - - ETA: 0s - loss: 9.0802 - binary_accur\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 12s 144us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0682 - binary_ - ETA: 8s - loss: 9.0894 - bi - ETA: 7s - loss: 9.0872 - ETA: 5s - loss: 9.0828 - binary_accu - ETA: 4s - loss: 9.0868 - binary_accuracy:  - ETA: 4s - loss: 9.0847 - b - E\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0846 - binary_accuracy: - ETA: 7s - loss: 9.0806 - binary - ETA: 2s - loss: 9.0895 - bina - ETA: 1s - loss: 9.089\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0999 - ETA: 7s - loss: 9.0900 - b\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 9.0822 - binary_accuracy: 0.4112  - ETA: 6s - loss: 9. - ETA: 1s - loss: 9.0817 -\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.0328 - bin - ETA: 10s - loss: 9 - ETA: 8s - loss: 9.0690 - - ETA: 6s - loss: 9 - ETA: 5s - loss: \n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 12s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0932 - binary_accu - ETA: 9s - loss: 9.0907 - binary_ac - ETA: 8s - loss:  - ETA: 6s - loss: 9.0803 - binary_accu - ETA: 5s - loss:  - ETA: 0s - loss: 9.0814 - binary_accuracy: 0.\n",
      "Epoch 33/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0822 - binary_accuracy: 0.41\n",
      "Epoch 34/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 ETA - ETA: 1s - loss: 9.0883 - \n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 12s 136us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 36/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - ETA: 4s - loss: 9.0787 - binary_accuracy: - ETA: 3s - loss: 9.0809 - binary_accura - ETA: 3s - loss: 9.0797 - binary_accuracy - ETA: 2s - loss: 9.0856 - binary_accur - ETA: 1s - loss: 9. - ETA: 0s - loss: 9.0829 - binary_accuracy: 0.41\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0748 - binary_accu - ETA: 7s - loss: 9.0737 - - ETA: 2s - l - ETA: 0s - loss: 9.0825 - binary_accuracy: 0.\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112ETA: 9s - loss: 9.0753 - binary_accuracy: 0.411 - ETA: 8s - loss: 9.0739 - bina - ETA: 7s - loss: 9.0746 - binary_accuracy:  - ETA: 7s - loss: 9.0765 - binary_accuracy:  - ETA: 7s - loss: 9.0748  - ETA: 5s - loss: 9.0 - ETA: 0s - loss: 9.0859 - binary_accura\n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0849 - \n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0932 - binary_accu\n",
      "Epoch 41/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0844 - binary_ - ETA: 1s - loss: 9.0815\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0962 - binary_accur - ETA: 3s - loss: 9.0922 - b - ETA: 2s - loss: 9.0906 - binary_accuracy: 0. - ETA: 1s - loss: 9.0897 - binar - ETA: 0s - loss: 9.0850 - binary_accuracy: 0.4 - ETA: 0s - loss: 9.0827 - binary_accuracy: 0.411 - ETA: 0s - loss: 9.0830 - binary_accuracy\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 13s 155us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 14s 159us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - lo\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.0932 - binary_ac - ETA: 6s - loss: 9.0914 - binary_accuracy: 0.41 - ETA: 6s - loss: - ETA: 4s - loss: 9.0910 - binar\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - lo - ETA: 11s\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0721 - binary_ - ETA: 3s - loss: 9.0753 - bi - ETA: 2s - loss: 9.0822 - binary_accuracy: 0.4 - ETA: 2s - l - ETA: 0s - loss: 9.0799 - binary_accuracy: \n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0854 - binary\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0918 - b - ETA: 7s - ETA: 5s - loss: 9.0903 - b - ETA: 4s - loss: 9.083 - ETA: 2s\n",
      "Epoch 54/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0881  - ETA: 2s - loss: 9.09 - ETA: 1s - loss: 9.0842 - bin\n",
      "Epoch 55/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 56/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0892 - binary_accuracy: 0.41 - ETA: 4s - loss: 9.0889 - binary_accuracy: 0. - ETA: 4s  - ETA: 1s - loss: 9.0822 - binary_accuracy: 0.4 - ETA: 1s - loss: 9.0799 - b\n",
      "Epoch 57/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.0793 - binary_accuracy: 0\n",
      "Epoch 58/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.08 - ETA: 7s - loss: 9. - ETA: 1s - loss: 9.0846 - binary_accura - ETA: 1s - loss: 9.0830 - bi\n",
      "Epoch 59/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0702 - binary_accuracy: 0.41 - ETA:  - ETA: 1s - loss: 9.0832 - binary_accuracy: 0.41 - ETA: 1s - loss: 9.0818 - bina\n",
      "Epoch 60/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 7s - loss: 9.0798 -  - ETA: 5s - loss: 9.0822 - binary_accuracy: - ETA: 5s - loss: 9.0867 - binar - ETA: 4s - loss: 9.0875 - binary_accuracy: 0 - ETA: 0s - loss: 9.0840 - binary_accuracy:  - ETA: 0s - loss: 9.0826 - binary_accuracy: 0.411 - ETA: 0s - loss: 9.0823 - binary_accuracy: 0.41\n",
      "Epoch 61/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0838 - binary_ - ETA: 0s - loss: 9.0852 - binary_accura\n",
      "Epoch 62/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112s\n",
      "Epoch 63/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 9.0822 - binary_accuracy: 0.4112s -  - ETA: 10s - loss: 9.0394 - binary_accuracy - ETA: 10s - loss: 9.0497 - binary_accuracy - ETA: 10s - loss: 9.0579 - binary_acc - ETA: 9\n",
      "Epoch 64/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 65/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0911 - binary_accuracy: 0. - ETA: 8s - loss: - ETA: 2s - loss: 9.0915 - binary_accuracy: 0.4 - ETA: 2s - loss: 9.0898 - binary_ - ETA: 1s - loss: 9.0863  - ETA: 0s - loss: 9.0816 - binary_accuracy: 0.41\n",
      "Epoch 66/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss:  - ETA: 5s - loss: 9.08 - ETA: 3s - loss: 9.0733 - binary_a - ETA: 2s - loss: 9.0784 - binary_accura - ETA: 1s - loss: 9.0821 - bi - ETA: 0s - loss: 9.0867 - binary_accurac\n",
      "Epoch 67/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0963 - binary_accura - ETA: 5s - loss: - ETA: 0s - loss: 9.0856 - binary_ac\n",
      "Epoch 68/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112: 1s - loss: 9.0851 - binary_accuracy: 0.411 - ETA: 1s - loss: 9.084\n",
      "Epoch 69/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.0758 - binary_accuracy:  - ETA: 11s - loss: 9.0725 - binary_accuracy: 0.41 - ETA: 11s - loss: 9.0702 - ETA: 10s -  - ETA: 8s - loss: 9.0 - E - ETA: 3s - loss: 9.0872 - binary_accura - ETA: 2s - loss: 9.0818 -  - ETA: 1s - loss: 9.0823 - \n",
      "Epoch 70/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - lo - ETA: 7s - loss: 9.0708 - binary_acc\n",
      "Epoch 71/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0816 - binary_ac\n",
      "Epoch 72/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0741 - binary_accur - ETA: 8s - los - ETA: 6s - loss: 9.0826 - binary_accur - ETA: 5s -  - ETA: 3s - loss: 9.0789 - binary_ - ETA: 2s - loss: 9.0820 - binary_accuracy: 0.4 - ETA: 2s - loss: 9.0824 - binary_accur - ETA: 1s - loss: 9.0846 - binary_accuracy: 0.411 - ETA: 1s - loss: 9.0843 -\n",
      "Epoch 73/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0812 - binary_accuracy\n",
      "Epoch 74/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0961 - binary_accuracy: 0.4 - ETA: 9s - loss: 9.0949 - binary - ETA: 8s - loss: 9.0881 - binary_accuracy: 0.4 - ETA: 7s - loss: - ETA: 5\n",
      "Epoch 75/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0824 - binary_accuracy: 0 - ETA: 1s - loss: 9.0820 - bi\n",
      "Epoch 76/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 13s 151us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - l\n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0907 - b - ETA: 7s - loss: 9.0869 - binary_ac - ETA: 6s - loss: 9.0864 - binary_accuracy: 0 - ET\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0751 -  - ETA: 2s - loss: 9.0735 - binar - ETA: 1s - loss: 9.0768 - bin - ETA: 0s - loss: 9.0830 - binary_accura\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.41120s - loss: 9.0870 - binary_accuracy: 0.410 - ETA: 0s - loss: 9.0865 - binary_accuracy: 0.410 - ETA: 0s - loss: 9.0869 - binary_acc\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.0663 - binary_ -  - E\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 14s 163us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.1188 - - ETA: 14s - loss: 9.1210 - ETA: 1s - loss: 9.0797 - \n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0871 - binary_accuracy: - ETA: 3s - loss: 9.0875 - binary_accura - ETA: 3s - loss: 9.085 - ETA: 1s - loss: 9.082\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.1068 - bin - ETA: - ET - ETA: 0s - loss: 9.0829 - binary_accura\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - l - - ETA: 2s - \n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.\n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.1060 - binary_ - E - ETA: 5s - loss: 9.0913 - b - ETA: 4s - loss: 9.0883 - bi - ETA: 2s - - ETA: 0s - loss: 9.0833 - binary_accuracy:\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 13s 154us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0790 - binary_accu\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.10 - ETA: 10s - loss: 9.0852 - binary_accura - E - ETA: 7s - loss: 9.0802 - binary_ac - ETA: 3s - loss: 9.0829 - binary_accuracy - ETA: 3s - loss: 9.0791  - ETA: 1s - loss: 9\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 9.0822 - binary_accuracy: 0.41127s - loss: 9.0724 - binary_accuracy - ETA: 6s - loss: 9.0691 - b - ETA: 5s - loss: 9.0852 - binary_accuracy:  - ETA: 5s - loss: 9.079 - ETA: 3 - ETA: 0s - loss: 9.0799 - binary_accur\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112s -  - ETA: 10s - loss: 9.0956 - - ETA: 8s - loss:  - ETA: 3s - loss: 9.0769 - binary_accuracy - ETA: 0s - loss: 9.0823 - binary_accuracy: 0.411\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0740 - bin - ETA: 1s - loss: 9.0825 - - ETA: 0s - loss: 9.0813 - binary_accuracy: 0.41\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.1137 - binary_accuracy:  - ETA: 7s - loss: 9.1114 - binary - ETA: 6s - loss: 9.0899 - bin - ETA: 4s - loss: 9.08 - ETA: 3s - loss: 9 - ETA: 1s - loss: 9.0841 - binary_accuracy: 0.4 - ETA: 1s - loss: 9.0829 - bi\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0788 - binary_accuracy - ETA: 2s - loss: \n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - loss: 9.0820 - binary_accuracy:  - ETA: 2s - loss: 9.0821 - b - ETA: 1s - loss: 9.0823 \n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 9.0822 - binary_accuracy: 0.4112 - ETA: 5s - loss: 9.0703 - binary_accu - ETA: 5s - loss: \n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 9.0822 - binary_accuracy: 0.4112s - loss: 9.0644 - binary_accuracy - ETA: 9s - loss: 9.0568 - binary_ -  - ETA: 2s - loss: 9.08 - ETA: 0s - loss: 9.0790 - binary_accuracy: - ETA: 0s - loss: 9.0802 - binary_accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-29979092.0, -29979096.0, -29979104.0, -29979108.0, -29979116.0, -29979120.0, -29979124.0, -29979128.0, -29979132.0, -29979136.0, -29979140.0, -29979144.0, -29979148.0, -29979152.0, -29979156.0, -29979160.0, -29979164.0, -29979168.0, -29979172.0, -29979176.0, -29979180.0, -29979184.0, -29979188.0, -29979192.0, -29979196.0, -29979200.0, -29979204.0, -29979208.0, -29979212.0, -29979216.0, -29979220.0, -29979224.0, -29979228.0, -29979232.0, -29979236.0, -29979240.0, -29979244.0, -29979248.0, -29979252.0, -29979256.0, -29979260.0, -29979264.0, -29979268.0, -29979272.0, -29979276.0, -29979280.0, -29979284.0, -29979288.0, -29979292.0, -29979296.0, -29979300.0, -29979304.0, -29979308.0, -29979312.0, -29979320.0, -29979324.0, -29979340.0, -29979704.0, -29979708.0, -29979712.0, -29979716.0, -29979720.0, -29979724.0, -29979728.0, -29979732.0, -29979736.0, -29979740.0, -29979744.0, -29979748.0, -29979752.0, -29979756.0, -29979760.0, -29979764.0, -29979768.0, -29979772.0, -29979776.0, -29979780.0, -29979784.0, -29979788.0, -29979792.0, -29979796.0, -29979800.0, -29979804.0, -29979808.0, -29979812.0, -29979816.0, -29979820.0, -29979824.0, -29979828.0, -29979832.0, -29979836.0, -29979840.0, -29979844.0, -29979848.0, -29979852.0, -29979856.0, -29979860.0, -29979864.0, -29979868.0, -29979872.0, -29979876.0, -29979880.0, -29979884.0, -29979888.0, -29979892.0, -29979896.0, -29979900.0, -29979904.0, -29979908.0, -29979912.0, -29979916.0, -29979920.0, -29979924.0, -29979928.0, -29979932.0, -29979936.0, -29979940.0, -29979944.0, -29979948.0, -29979952.0, -29979956.0, -29979960.0, -29979964.0, -29979968.0, -29979972.0, -29979976.0, -29979980.0, -29979984.0, -29979988.0, -29979992.0, -29979996.0, -29980000.0, -29980004.0, -29980008.0, -29980012.0, -29980016.0, -29980020.0, -29980024.0, -29980028.0, -29980032.0, -29980036.0, -29980040.0, -29980044.0, -29980048.0, -29980052.0, -29980056.0, -29980060.0, -29980064.0, -29980068.0, -29980072.0, -29980076.0, -29980080.0, -29980084.0, -29980088.0, -29980092.0, -29980096.0, -29980100.0, -29980104.0, -29980108.0, -29980112.0, -29980116.0, -29980120.0, -29980124.0, -29980128.0, -29980132.0, -29980136.0, -29980140.0, -29980144.0, -29980148.0, -29980152.0, -29980156.0, -29980160.0, -29980164.0, -29980168.0, -29980172.0, -29980176.0, -29980180.0, -29980184.0, -29980188.0, -29980192.0, -29980196.0, -29980200.0, -29980204.0, -29980208.0, -29980212.0, -29980216.0, -29980220.0, -29980224.0, -29980228.0, -29980232.0, -29980236.0, -29980240.0, -29980244.0, -29980248.0, -29980252.0, -29980256.0, -29980260.0, -29980264.0, -29980268.0, -29980272.0, -29980276.0, -29980280.0, -29980284.0, -29980288.0, -29980292.0, -29980296.0, -29980300.0, -29980304.0, -29980308.0, -29980312.0, -29980316.0, -29980320.0, -29980324.0, -29980328.0, -29980332.0, -29980336.0, -29980340.0, -29980344.0, -29980348.0, -29980352.0, -29980356.0, -29980360.0, -29980364.0, -29980368.0, -29980372.0, -29980376.0, -29980380.0, -29980384.0, -29980388.0, -29980392.0, -29980396.0, -29980400.0, -29980404.0, -29980408.0, -29980412.0, -29980416.0, -29980420.0, -29980424.0, -29980428.0, -29980432.0, -29980436.0, -29980440.0, -29980444.0, -29980448.0, -29980452.0, -29980456.0, -29980460.0, -29980464.0, -29980468.0, -29980472.0, -29980476.0, -29980480.0, -29980484.0, -29980488.0, -29980492.0, -29980496.0, -29980500.0, -29980504.0, -29980508.0, -29980512.0, -29980516.0, -29980520.0, -29980524.0, -29980528.0, -29980532.0, -29980536.0, -29980540.0, -29980544.0, -29980548.0, -29980552.0, -29980556.0, -29980560.0, -29980564.0, -29980568.0, -29980572.0, -29980576.0, -29980580.0, -29980584.0, -29980588.0, -29980592.0, -29980596.0, -29980600.0, -29980604.0, -29980608.0, -29980612.0, -29980616.0, -29980620.0, -29980624.0, -29980628.0, -29980632.0, -29980636.0, -29980640.0, -29980644.0, -29980648.0, -29980652.0, -29980656.0, -29980660.0, -29980664.0, -29980668.0, -29980672.0, -29980676.0, -29980680.0, -29980684.0, -29980688.0, -29980692.0, -29980696.0, -29980700.0, -29980704.0, -29980708.0, -29980712.0, -29980716.0, -29980720.0, -29980724.0, -29980728.0, -29980732.0, -29980740.0, -29980748.0, -29980752.0, -29980756.0, -29981132.0, -29981136.0, -29981140.0, -29981144.0, -29981148.0, -29981152.0, -29981156.0, -29981160.0, -29981164.0, -29981168.0, -29981172.0, -29981176.0, -29981180.0, -29981184.0, -29981188.0, -29981192.0, -29981196.0, -29981200.0, -29981204.0, -29981208.0, -29981212.0, -29981216.0, -29981220.0, -29981224.0, -29981228.0, -29981232.0, -29981236.0, -29981240.0, -29981244.0, -29981248.0, -29981252.0, -29981256.0, -29981260.0, -29981264.0, -29981268.0, -29981272.0, -29981276.0, -29981280.0, -29981284.0, -29981288.0, -29981292.0, -29981296.0, -29981300.0, -29981304.0, -29981308.0, -29981312.0, -29981316.0, -29981320.0, -29981324.0, -29981328.0, -29981332.0, -29981336.0, -29981340.0, -29981344.0, -29981348.0, -29981352.0, -29981356.0, -29981360.0, -29981364.0, -29981368.0, -29981372.0, -29981376.0, -29981380.0, -29981384.0, -29981388.0, -29981392.0, -29981396.0, -29981400.0, -29981404.0, -29981408.0, -29981412.0, -29981416.0, -29981420.0, -29981424.0, -29981428.0, -29981432.0, -29981436.0, -29981440.0, -29981444.0, -29981448.0, -29981452.0, -29981456.0, -29981460.0, -29981464.0, -29981468.0, -29981472.0, -29981476.0, -29981480.0, -29981484.0, -29981488.0, -29981492.0, -29981496.0, -29981500.0, -29981504.0, -29981508.0, -29981516.0, -29981520.0, -29981528.0, -29981536.0, -43012216.0, -43012220.0, -43012224.0, -43012228.0, -43012230.0, -43012236.0, -43012240.0, -43012244.0, -43012250.0, -43012252.0, -43012256.0, -43012260.0, -43012264.0, -43012268.0, -43012270.0, -43012276.0, -43012280.0, -43012284.0, -43012290.0, -43012292.0, -43012296.0, -43012300.0, -43012304.0, -43012308.0, -43012310.0, -43012316.0, -43012320.0, -43012324.0, -43012330.0, -43012332.0, -43012336.0, -43012340.0, -43012344.0, -43012348.0, -43012350.0, -43012356.0, -43012360.0, -43012364.0, -43012370.0, -43012372.0, -43012376.0, -43012380.0, -43012384.0, -43012388.0, -43012390.0, -43012396.0, -43012400.0, -43012404.0, -43012410.0, -43012412.0, -43012416.0, -43012420.0, -43012424.0, -43012428.0, -43012430.0, -43012436.0, -43012440.0, -43012444.0, -43012450.0, -43012452.0, -43012456.0, -43012460.0, -43012464.0, -43012468.0, -43012470.0, -43012476.0, -43012480.0, -43012484.0, -43012490.0, -43012492.0, -43012496.0, -43012500.0, -43012504.0, -43012508.0, -43012510.0, -43012516.0, -43012520.0, -43012524.0, -43012530.0, -43012532.0, -43012536.0, -43012540.0, -43012544.0, -43012548.0, -43012550.0, -43012556.0, -43012560.0, -43012564.0, -43012570.0, -43012572.0, -43012576.0, -43012580.0, -43012584.0, -43012588.0, -43012590.0, -43012596.0, -43012600.0, -43012604.0, -43012610.0, -43012612.0, -43012616.0, -43012620.0, -43012624.0, -43012628.0, -43012630.0, -43012636.0, -43012640.0, -43012644.0, -43012650.0, -43012652.0, -43012656.0, -43012660.0, -43012664.0, -43012668.0, -43012670.0, -43012676.0, -43012680.0, -43012684.0, -43012690.0, -43012692.0, -43012696.0, -43012700.0, -43012704.0, -43012708.0, -43012710.0, -43012716.0, -43012720.0, -43012724.0, -43012730.0, -43012732.0, -43012736.0, -43012740.0, -43012744.0, -43012748.0, -43012750.0, -43012756.0, -43012760.0, -43012764.0, -43012770.0, -43012772.0, -43012776.0, -43012780.0, -43012784.0, -43012788.0, -43012796.0, -43014350.0, -43014356.0, -43014360.0, -43014364.0, -43014370.0, -43014372.0, -43014376.0, -43014380.0, -43014384.0, -43014388.0, -43014390.0, -43014396.0, -43014400.0, -43014404.0, -43014410.0, -43014412.0, -43014416.0, -43014420.0, -43014424.0, -43014428.0, -43014430.0, -43014436.0, -43014440.0, -43014444.0, -43014450.0, -43014452.0, -43014456.0, -43014460.0, -43014464.0, -43014468.0, -43014470.0, -43014476.0, -43014480.0, -43014484.0, -43014490.0, -43014492.0, -43014496.0, -43014500.0, -43014504.0, -43014508.0, -43014510.0, -43014516.0, -43014520.0, -43014524.0, -43014530.0, -43014532.0, -43014536.0, -43014540.0, -43014544.0, -43014548.0, -43014550.0, -43014556.0, -43014560.0, -43014564.0, -43014570.0, -43014572.0, -43014576.0, -43014580.0, -43014584.0, -43014588.0, -43014590.0, -43014596.0, -43014600.0, -43014604.0, -43014610.0, -43014612.0, -43014616.0, -43014620.0, -43014624.0, -43014628.0, -43014630.0, -43014636.0, -43014640.0, -43014644.0, -43014650.0, -43014652.0, -43014656.0, -43014660.0, -43014664.0, -43014668.0, -43014670.0, -43014676.0, -43014680.0, -43014684.0, -43014690.0, -43014692.0, -43014696.0, -43014700.0, -43014704.0, -43014708.0, -43014710.0, -43014716.0, -43014720.0, -43014724.0, -43014730.0, -43014732.0, -43014736.0, -43014740.0, -43014744.0, -43014748.0, -43014750.0, -43014756.0, -43014760.0, -43014764.0, -43014770.0, -43014772.0, -43014776.0, -43014780.0, -43014784.0, -43014788.0, -43014790.0, -43014796.0, -43014800.0, -43014804.0, -43014810.0, -43014812.0, -43014816.0, -43014820.0, -43014824.0, -43014828.0, -43014830.0, -43014836.0, -43014840.0, -43014844.0, -43014850.0, -43014852.0, -43014856.0, -43014860.0, -43014864.0, -43014868.0, -43014870.0, -43014876.0, -43014880.0, -43014884.0, -43014890.0, -43014892.0, -43014896.0, -43014900.0, -43014904.0, -43014908.0, -43014910.0, -43014916.0, -43014920.0, -43014924.0, -43014930.0, -43014932.0, -43014936.0, -43014940.0, -43014944.0, -43014948.0, -43014950.0, -43014956.0, -43014960.0, -43014964.0, -43014970.0, -43014972.0, -43014976.0, -43014980.0, -43014984.0, -43014988.0, -43014990.0, -43014996.0, -43015000.0, -43015004.0, -43015010.0, -43015012.0, -43015016.0, -43015020.0, -43015024.0, -43015028.0, -43015030.0, -43015036.0, -43015040.0, -43015044.0, -43015050.0, -43015052.0, -43015056.0, -43015060.0, -43015064.0, -43015068.0, -43015070.0, -43015076.0, -43015080.0, -43015084.0, -43015090.0, -43015092.0, -43015096.0, -43015100.0, -43015104.0, -43015108.0, -43015110.0, -43015116.0, -43015120.0, -43015124.0, -43015130.0, -43015132.0, -43015136.0, -43015140.0, -43015144.0, -43015148.0, -43015150.0, -43015156.0, -43015160.0, -43015164.0, -43015170.0, -43015172.0, -43015176.0, -43015180.0, -43015184.0, -43015188.0, -43015190.0, -43015196.0, -43015200.0, -43015204.0, -43015210.0, -43015212.0, -43015216.0, -43015220.0, -43015224.0, -43015228.0, -43015230.0, -43015236.0, -43015240.0, -43015244.0, -43015250.0, -43015252.0, -43015256.0, -43015260.0, -43015264.0, -43015268.0, -43015270.0, -43015276.0, -43015280.0, -43015284.0, -43015290.0, -43015292.0, -43015296.0, -43015300.0, -43015304.0, -43015308.0, -43015310.0, -43015316.0, -43015320.0, -43015324.0, -43015330.0, -43015332.0, -43015336.0, -43015340.0, -43015344.0, -43015348.0, -43015350.0, -43015356.0, -43015360.0, -43015364.0, -43015370.0, -43015372.0, -43015376.0, -43015380.0, -43015384.0, -43015388.0, -43015390.0, -43015396.0, -43015400.0, -43015404.0, -43015410.0, -43015412.0, -43015416.0, -43015420.0, -43015424.0, -43015428.0, -43015430.0, -43015436.0, -43015440.0, -43015444.0, -43015450.0, -43015452.0, -43015456.0, -43015460.0, -43015464.0, -43015468.0, -43015470.0, -43015476.0, -43015480.0, -43015484.0, -43015490.0, -43015492.0, -43015496.0, -43015500.0, -43015504.0, -43015508.0, -43015510.0, -43015516.0, -43015520.0, -43015524.0, -43015530.0, -43015532.0, -43015536.0, -43015540.0, -43015544.0, -43015548.0, -43015550.0, -43015556.0, -43015560.0, -43015564.0, -43015570.0, -43015572.0, -43015576.0, -43015580.0, -43015584.0, -43015588.0, -43015590.0, -43015596.0, -43015600.0, -43015604.0, -43015610.0, -43015612.0, -43015616.0, -43015620.0, -43015624.0, -43015628.0, -43015630.0, -43015636.0, -43015640.0, -43015644.0, -43015650.0, -43015652.0, -43015656.0, -43015660.0, -43015664.0, -43015668.0, -43015670.0, -43015676.0, -43015680.0, -43015684.0, -43015690.0, -43015692.0, -43015696.0, -43015700.0, -43015704.0, -43015708.0, -43015710.0, -43015716.0, -43015720.0, -43015724.0, -43015730.0, -43015732.0, -43015736.0, -43015740.0, -43015744.0, -43015748.0, -43015750.0, -43015756.0, -43015760.0, -43015764.0, -43015770.0, -43015772.0, -43015776.0, -43015780.0, -43015784.0, -43015788.0, -43015790.0, -43015796.0, -43015800.0, -43015804.0, -43015810.0, -43015812.0, -43015816.0, -43015820.0, -43015824.0, -43015828.0, -43015830.0, -43015836.0, -43015840.0, -43015844.0, -43015850.0, -43015852.0, -43015856.0, -43015860.0, -43015864.0, -43015868.0, -43015870.0, -43015876.0, -43015880.0, -43015884.0, -43015890.0, -43015892.0, -43015896.0, -43015900.0, -43015904.0, -43015908.0, -43015910.0, -43015916.0, -43015920.0, -43015924.0, -43015930.0, -43015932.0, -43015936.0, -43015940.0, -43015944.0, -43015948.0, -43015950.0, -43015956.0, -43015960.0, -43015964.0, -43015970.0, -43015972.0, -43015976.0, -43015980.0, -43015984.0, -43015988.0, -43015990.0, -43015996.0, -43016000.0, -43016004.0, -43016010.0, -43016012.0, -43016016.0, -43016020.0, -43016024.0, -43016028.0, -43016030.0, -43016036.0, -43016040.0, -43016044.0, -43016050.0, -43016052.0, -43016056.0, -43016060.0, -43016064.0, -43016068.0, -43016070.0, -43016076.0, -43016080.0, -43016084.0, -43016090.0, -43016092.0, -43016096.0, -43016100.0, -43016104.0, -43016108.0, -43016110.0, -43016116.0, -43016120.0, -43016124.0, -43016130.0, -43016132.0, -43016136.0, -43016140.0, -43016144.0, -43016148.0, -43016150.0, -43016156.0, -43016160.0, -43016164.0, -43016170.0, -43016172.0, -43016176.0, -43016180.0, -43016184.0, -43016188.0, -43016190.0, -43016196.0, -43016200.0, -43016204.0, -43016210.0, -43016212.0, -43016216.0, -43016220.0, -43016224.0, -43016228.0, -43016230.0, -43016236.0, -43016240.0, -43016244.0, -43016250.0, -43016252.0, -43016256.0, -43016260.0, -43016264.0, -43016268.0, -43016270.0, -43016276.0, -43016280.0, -43016284.0, -43016290.0, -43016292.0, -43016296.0, -43016300.0, -43016304.0, -43016308.0, -43016310.0, -43016316.0, -43016320.0, -43016324.0, -43016330.0, -43016332.0, -43016336.0, -43016340.0, -43016344.0, -43016348.0, -43016350.0, -43016356.0, -43016360.0, -43016364.0, -43016370.0, -43016372.0, -43016376.0, -43016380.0, -43016384.0, -43016388.0, -43016390.0, -43016396.0, -43016400.0, -43016404.0, -43016410.0, -43016412.0, -43016416.0, -43016420.0, -43016424.0, -43016428.0, -43016430.0, -43016436.0, -43016440.0, -43016444.0, -43016450.0, -43016452.0, -43016456.0, -43016460.0, -43016464.0, -43016468.0, -43016470.0, -43016476.0, -43016480.0, -43016484.0, -43016490.0, -43016492.0, -43016496.0, -43016500.0, -43016504.0, -43016508.0, -43016510.0, -43016516.0, -43016520.0, -43016524.0, -43016530.0, -43016532.0, -43016536.0, -43016540.0, -43016544.0, -43016548.0, -43016550.0, -43016556.0, -43016560.0, -43016564.0, -43016570.0, -43016572.0, -43016576.0, -43016580.0, -43016584.0, -43016588.0, -43016590.0, -43016596.0, -43016600.0, -43016604.0, -43016610.0, -43016612.0, -43016616.0, -43016620.0, -43016624.0, -43016628.0, -43016630.0, -43016636.0, -43016640.0, -43016644.0, -43016650.0, -43016652.0, -43016656.0, -43016660.0, -43016664.0, -43016668.0, -43016670.0, -43016676.0, -43016680.0, -43016684.0, -43016690.0, -43016692.0, -43016696.0, -43016700.0, -43016704.0, -43016708.0, -43016710.0, -43016716.0, -43016720.0, -43016724.0, -43016730.0, -43016732.0, -43016736.0, -43016740.0, -43016748.0, -43016750.0, -43016756.0, -43016760.0, -43016764.0, -43016770.0, -43016772.0, -43016776.0, -43016780.0, -43016784.0, -43016788.0, -43016790.0, -43016796.0, -43016800.0, -43016804.0, -43016810.0, -43016812.0, -43016816.0, -43016820.0, -43016824.0, -43016828.0, -43016830.0, -43016836.0, -43016840.0, -43016844.0, -43016850.0, -43016852.0, -43016860.0, -43016864.0, -43016868.0, -43016870.0, -43016876.0, -43016880.0, -43016884.0, -43016890.0, -43016892.0, -43016896.0, -43016900.0, -43016904.0, -43016908.0, -43016910.0, -43016916.0, -43016920.0, -43016924.0, -43016930.0, -43016932.0, -43016936.0, -43016940.0, -43016944.0, -43016948.0, -43016950.0, -43016956.0, -43016960.0, -43016964.0, -43016970.0, -43016972.0, -43016976.0, -43016980.0, -43016984.0, -43016988.0, -43016990.0, -43016996.0, -43017000.0, -43017004.0, -43017010.0, -43017012.0, -43017016.0, -43017020.0, -43017024.0, -43017028.0, -43017030.0, -43017036.0, -43017040.0, -43017044.0, -43017050.0, -43017052.0, -43017056.0, -43017060.0, -43017064.0, -43017068.0, -43017070.0, -43017076.0, -43017080.0, -43017084.0, -43017090.0, -43017092.0, -43017096.0, -43017100.0, -43017104.0, -43017108.0, -43017110.0, -43017116.0, -43017120.0, -43017124.0, -43017130.0, -43017132.0, -43017136.0, -43017140.0, -43017144.0, -43017148.0, -43017150.0, -43017156.0, -43017160.0, -43017164.0, -43017170.0, -43017172.0, -43017176.0, -43017180.0, -43017184.0, -43017188.0, -43017190.0, -43017196.0, -43017200.0, -43017204.0, -43017210.0, -43017212.0, -43017216.0, -43017220.0, -43017224.0, -43017228.0, -43017230.0, -43017236.0, -43017240.0, -43017244.0, -43017250.0, -43017252.0, -43017256.0, -43017260.0, -43017264.0, -43017268.0, -43017270.0, -43017276.0, -43017280.0, -43017284.0, -43017290.0, -43017292.0, -43017296.0, -43017300.0, -43017304.0, -43017308.0, -43017310.0, -43017316.0, -43017320.0, -43017324.0, -43017330.0, -43017332.0, -43017336.0, -43017340.0, -43017344.0, -43017348.0, -43017350.0, -43017356.0, -43017360.0, -43017364.0, -43017370.0, -43017372.0, -43017376.0, -43017380.0, -43017384.0, -43017388.0, -43017390.0, -43017396.0, -43017400.0, -43017404.0, -43017410.0, -43017412.0, -43017416.0, -43017420.0, -43017424.0, -43017428.0, -43017430.0, -43017436.0, -43017440.0, -43017444.0, -43017450.0, -43017452.0, -43017456.0, -43017460.0, -43017464.0, -43017468.0, -43017470.0, -43017476.0, -43017480.0, -43017484.0, -43017490.0, -43017492.0, -43017496.0, -43017500.0, -43017504.0, -43017508.0, -43017510.0, -43017516.0, -43017520.0, -43019180.0, -43019184.0, -43019188.0, -43019190.0, -43019196.0, -43019200.0, -43019204.0, -43019210.0, -43019212.0, -43019216.0, -43019220.0, -43019224.0, -43019228.0, -43019230.0, -43019236.0, -43019240.0, -43019244.0, -43019250.0, -43019252.0, -43019256.0, -43019260.0, -43019264.0, -43019268.0, -43019270.0, -43019276.0, -43019280.0, -43019284.0, -43019290.0, -43019292.0, -43019296.0, -43019300.0, -43019304.0, -43019308.0, -43019310.0, -43019316.0, -43019320.0, -43019324.0, -43019330.0, -43019332.0, -43019336.0, -43019340.0, -43019344.0, -43019348.0, -43019350.0, -43019356.0, -43019360.0, -43019364.0, -43019370.0, -43019372.0, -43019376.0, -43019380.0, -43019384.0, -43019388.0, -43019390.0, -43019396.0, -43019400.0, -43019404.0, -43019410.0, -43019412.0, -43019416.0, -43019420.0, -43019424.0, -43019428.0, -43019430.0, -43019436.0, -43019440.0, -43019444.0, -43019450.0, -43019452.0, -43019456.0, -43019460.0, -43019464.0, -43019468.0, -43019470.0, -43019476.0, -43019480.0, -43019484.0, -43019490.0, -43019492.0, -43019496.0, -43019500.0, -43019504.0, -43019508.0, -43019510.0, -43019516.0, -43019520.0, -43019524.0, -43019530.0, -43019532.0, -43019536.0, -43019540.0, -43019544.0, -43019548.0, -43019550.0, -43019556.0, -43019560.0, -43019564.0, -43019570.0, -43019572.0, -43019576.0, -43019580.0, -43019584.0, -43019588.0, -43019590.0, -43019596.0, -43019600.0, -43019604.0, -43019610.0, -43019612.0, -43019616.0, -43019620.0, -43019624.0, -43019628.0, -43019630.0, -43019636.0, -43019640.0, -43019644.0, -43019650.0, -43019652.0, -43019656.0, -43019660.0, -43019664.0, -43019668.0, -43019670.0, -43019676.0, -43019680.0, -43019684.0, -43019690.0, -43019692.0, -43019696.0, -43019700.0, -43019704.0, -43019708.0, -43019710.0, -43019716.0, -43019720.0, -43019724.0, -43019730.0, -43019732.0, -43019736.0, -43019740.0, -43019744.0, -43019748.0, -43019750.0, -43019756.0, -43019760.0, -43019764.0, -43019770.0, -43019772.0, -43019776.0, -43019780.0, -43019784.0, -43019788.0, -43019790.0, -43019796.0, -43019800.0, -43019804.0, -43019810.0, -43019812.0, -43019816.0, -43019820.0, -43019824.0, -43019828.0, -43019830.0, -43019836.0, -43019840.0, -43019844.0, -43019850.0, -43019852.0, -43019856.0, -43019860.0, -43019864.0, -43019868.0, -43019870.0, -43019876.0, -43019880.0, -43019884.0, -43019890.0, -43019892.0, -43019896.0, -43019900.0, -43019904.0, -43019908.0, -43019910.0, -43019916.0, -43019920.0, -43019924.0, -43019930.0, -43019932.0, -43019936.0, -43019940.0, -43019944.0, -43019948.0, -43019950.0, -43019956.0, -43019960.0, -43019964.0, -43019970.0, -43019972.0, -43019976.0, -43019980.0, -43019984.0, -43019988.0, -43019990.0, -43019996.0, -43020000.0, -43020004.0, -43020010.0, -43020012.0, -43020016.0, -43020020.0, -43020024.0, -43020028.0, -43020030.0, -43020036.0, -43020040.0, -43020044.0, -43020050.0, -43020052.0, -43020056.0, -43020060.0, -43020064.0, -43020068.0, -43020070.0, -43020076.0, -43020080.0, -43020084.0, -43020090.0, -43020092.0, -43020096.0, -43020100.0, -43020104.0, -43020108.0, -43020110.0, -43020116.0, -43020120.0, -43020124.0, -43020130.0, -43020132.0, -43020136.0, -43020140.0, -43020144.0, -43020148.0, -43020150.0, -43020156.0, -43020160.0, -43020164.0, -43020170.0, -43020172.0, -43020176.0, -43020180.0, -43020184.0, -43020188.0, -43020190.0, -43020196.0, -43020200.0, -43020204.0, -43020210.0, -43020212.0, -43020216.0, -43020220.0, -43020224.0, -43020228.0, -43020230.0, -43020236.0, -43020240.0, -43020244.0, -43020250.0, -43020252.0, -43020256.0, -43020260.0, -43020264.0, -43020268.0, -43020270.0, -43020276.0, -43020280.0, -43020284.0, -43020290.0, -43020292.0, -43020296.0, -43020300.0, -43020304.0, -43020308.0, -43020310.0, -43020316.0, -43020320.0, -43020324.0, -43020330.0, -43020332.0, -43020336.0, -43020340.0, -43020344.0, -43020348.0, -43020350.0, -43020356.0, -43020360.0, -43020364.0, -43020370.0, -43020372.0, -43020376.0, -43020380.0, -43020384.0, -43020388.0, -43020390.0, -43020396.0, -43020400.0, -43020404.0, -43020410.0, -43020412.0, -43020416.0, -43020420.0, -43020424.0, -43020428.0, -43020430.0, -43020436.0, -43020440.0, -43020444.0, -43020450.0, -43020452.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8567631836620506\n",
      "Hamming Loss: 0.07308714505525249\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93     18689\n",
      "           1       0.00      0.00      0.00      2758\n",
      "\n",
      "    accuracy                           0.87     21447\n",
      "   macro avg       0.44      0.50      0.47     21447\n",
      "weighted avg       0.76      0.87      0.81     21447\n",
      "\n",
      "y_pred shape: \n",
      "(21447, 2)\n",
      "y_pred2 shape:\n",
      "(21447, 2)\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         4\n",
      "         1.0       0.20      1.00      0.33         1\n",
      "\n",
      "    accuracy                           0.20         5\n",
      "   macro avg       0.10      0.50      0.17         5\n",
      "weighted avg       0.04      0.20      0.07         5\n",
      "\n",
      "Hamming Loss: 0.09161654310626195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "y_pred3, pred3, y_test3, hl3 = list(),list(),list(),list()\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(1)\n",
    "  #]) \n",
    "    \n",
    "  #inputs = tf.keras.layers.Input(shape=(X_train.shape))\n",
    "  #outputs = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "  #model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "\n",
    "  #outputs = keras.layers.Dense(tf.keras.layers.Dense(number_of_classes)) #(x) \n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(2)\n",
    "  #])   \n",
    "  \n",
    "  #model = tf.contrib.learn.DNNClassifier(hidden_units=[5,10,5],\n",
    "  #                                          n_classes=4)    \n",
    "    \n",
    "\n",
    "  inputs = tf.keras.Input(shape=(X_train.shape))\n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    \n",
    "  outputs = tf.keras.layers.Dense(number_of_classes)  #, activation=activation)\n",
    "                             #kernel_initializer=initializer,\n",
    "                             #activation=activation) #(x) \n",
    "  #model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(2)\n",
    "  ]) \n",
    "    \n",
    "  #model.add(layers.Activation(activations.relu))\n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  #model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.001))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  #print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "    \n",
    "  #y_pred2.append(y_pred)\n",
    "  y_pred3 = np.append(y_pred3, y_pred2)\n",
    "  #pred2.append(pred)\n",
    "  pred3 = np.append(pred3, pred2)\n",
    "  #y_test2.append(y_test)\n",
    "  y_test3 = np.append(y_test3, y_test2)\n",
    "  #hl2.append(hl)\n",
    "  hl3 = np.append(hl3, hl)\n",
    "    \n",
    "print(\"y_pred shape: \")\n",
    "print(np.shape(y_pred))\n",
    "\n",
    "print(\"y_pred2 shape:\")\n",
    "print(np.shape(y_pred2))\n",
    "\n",
    "y_pred3 = np.concatenate((y_pred3[0], y_pred3[1], y_pred3[2], y_pred3[3], y_pred3[4]), axis=None)\n",
    "pred3 = np.concatenate((pred3[0], pred3[1], pred3[2], pred3[3], pred3[4]), axis=None)\n",
    "y_test3 = np.concatenate((y_test3[0], y_test3[1], y_test3[2], y_test3[3], y_test3[4]), axis=None)\n",
    "hl3 = np.concatenate((hl3[0], hl3[1], hl3[2], hl3[3], hl3[4]), axis=None)\n",
    "\n",
    "print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\")\n",
    "#print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "print(classification_report(y_test3, pred3))\n",
    "hl3_avg = sum(hl3) / len(hl3)\n",
    "print(\"Hamming Loss: \" + str(hl3_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12101, 55)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_KGFS50.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9680 samples\n",
      "Epoch 1/100\n",
      "9680/9680 [==============================] - 2s 160us/sample - loss: 5.8347 - binary_accuracy: 0.6195s - loss: 5.8338 - binary_ac\n",
      "Epoch 2/100\n",
      "9680/9680 [==============================] - 1s 100us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 3/100\n",
      "9680/9680 [==============================] - 1s 106us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 4/100\n",
      "9680/9680 [==============================] - 1s 120us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 5/100\n",
      "9680/9680 [==============================] - 1s 100us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 6/100\n",
      "9680/9680 [==============================] - 1s 91us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 7/100\n",
      "9680/9680 [==============================] - 1s 102us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 8/100\n",
      "9680/9680 [==============================] - 1s 129us/sample - loss: 5.8347 - binary_accuracy: 0.6195s - loss: 5.8492 - binary_ac\n",
      "Epoch 9/100\n",
      "9680/9680 [==============================] - 1s 116us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 10/100\n",
      "9680/9680 [==============================] - 1s 128us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 11/100\n",
      "9680/9680 [==============================] - 1s 142us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 12/100\n",
      "9680/9680 [==============================] - 1s 109us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 13/100\n",
      "9680/9680 [==============================] - 1s 121us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 14/100\n",
      "9680/9680 [==============================] - 1s 138us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 15/100\n",
      "9680/9680 [==============================] - 1s 135us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 16/100\n",
      "9680/9680 [==============================] - 1s 135us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 17/100\n",
      "9680/9680 [==============================] - 1s 133us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 18/100\n",
      "9680/9680 [==============================] - 1s 137us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 19/100\n",
      "9680/9680 [==============================] - 1s 140us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 20/100\n",
      "9680/9680 [==============================] - 1s 144us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 21/100\n",
      "9680/9680 [==============================] - 1s 148us/sample - loss: 5.8347 - binary_accuracy: 0.6195s - loss: 5.8347 - b\n",
      "Epoch 22/100\n",
      "9680/9680 [==============================] - 1s 134us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 23/100\n",
      "9680/9680 [==============================] - 1s 142us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 24/100\n",
      "9680/9680 [==============================] - 1s 129us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 25/100\n",
      "9680/9680 [==============================] - 1s 134us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 26/100\n",
      "9680/9680 [==============================] - 1s 133us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 27/100\n",
      "9680/9680 [==============================] - 1s 132us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 28/100\n",
      "9680/9680 [==============================] - 1s 133us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 29/100\n",
      "9680/9680 [==============================] - 1s 132us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 30/100\n",
      "9680/9680 [==============================] - 1s 134us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 31/100\n",
      "9680/9680 [==============================] - 1s 133us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 32/100\n",
      "9680/9680 [==============================] - 1s 132us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 33/100\n",
      "9680/9680 [==============================] - 1s 128us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 34/100\n",
      "9680/9680 [==============================] - 1s 137us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 35/100\n",
      "9680/9680 [==============================] - 1s 134us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 36/100\n",
      "9680/9680 [==============================] - 1s 131us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 37/100\n",
      "9680/9680 [==============================] - 1s 132us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 38/100\n",
      "9680/9680 [==============================] - 1s 130us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 39/100\n",
      "9680/9680 [==============================] - 1s 135us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 40/100\n",
      "9680/9680 [==============================] - 1s 131us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 41/100\n",
      "9680/9680 [==============================] - 1s 131us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 42/100\n",
      "9680/9680 [==============================] - 1s 133us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 43/100\n",
      "9680/9680 [==============================] - 1s 129us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 44/100\n",
      "9680/9680 [==============================] - 1s 134us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 45/100\n",
      "9680/9680 [==============================] - 1s 135us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 46/100\n",
      "9680/9680 [==============================] - 1s 129us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 47/100\n",
      "9680/9680 [==============================] - 1s 130us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 48/100\n",
      "9680/9680 [==============================] - 1s 139us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 49/100\n",
      "9680/9680 [==============================] - 1s 131us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 50/100\n",
      "9680/9680 [==============================] - 1s 129us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 51/100\n",
      "9680/9680 [==============================] - 1s 150us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 52/100\n",
      "9680/9680 [==============================] - 2s 185us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 53/100\n",
      "9680/9680 [==============================] - 2s 169us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 54/100\n",
      "9680/9680 [==============================] - 1s 135us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 55/100\n",
      "9680/9680 [==============================] - 1s 148us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 56/100\n",
      "9680/9680 [==============================] - 2s 162us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 57/100\n",
      "9680/9680 [==============================] - 1s 141us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 58/100\n",
      "9680/9680 [==============================] - 1s 138us/sample - loss: 5.8347 - binary_accuracy: 0.6195s - loss: 5.7689 - \n",
      "Epoch 59/100\n",
      "9680/9680 [==============================] - 1s 141us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 60/100\n",
      "9680/9680 [==============================] - 1s 155us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 61/100\n",
      "9680/9680 [==============================] - 1s 144us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 62/100\n",
      "9680/9680 [==============================] - 1s 144us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 63/100\n",
      "9680/9680 [==============================] - 1s 145us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 64/100\n",
      "9680/9680 [==============================] - 1s 140us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 65/100\n",
      "9680/9680 [==============================] - 1s 141us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 66/100\n",
      "9680/9680 [==============================] - 2s 155us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 67/100\n",
      "9680/9680 [==============================] - 1s 152us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 68/100\n",
      "9680/9680 [==============================] - 1s 144us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 69/100\n",
      "9680/9680 [==============================] - 2s 172us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 70/100\n",
      "9680/9680 [==============================] - 1s 122us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 71/100\n",
      "9680/9680 [==============================] - 1s 133us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 72/100\n",
      "9680/9680 [==============================] - ETA: 0s - loss: 5.8379 - binary_accuracy: 0.619 - 1s 110us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 73/100\n",
      "9680/9680 [==============================] - 1s 105us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 74/100\n",
      "9680/9680 [==============================] - 1s 138us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 75/100\n",
      "9680/9680 [==============================] - 1s 145us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 76/100\n",
      "9680/9680 [==============================] - 2s 160us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 77/100\n",
      "9680/9680 [==============================] - 2s 164us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 78/100\n",
      "9680/9680 [==============================] - 2s 156us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 79/100\n",
      "9680/9680 [==============================] - 2s 159us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 80/100\n",
      "9680/9680 [==============================] - 2s 166us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 81/100\n",
      "9680/9680 [==============================] - 1s 152us/sample - loss: 5.8347 - binary_accuracy: 0.6195s - loss: 5.7913 \n",
      "Epoch 82/100\n",
      "9680/9680 [==============================] - 1s 147us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 83/100\n",
      "9680/9680 [==============================] - 2s 162us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 84/100\n",
      "9680/9680 [==============================] - 1s 153us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 85/100\n",
      "9680/9680 [==============================] - 1s 131us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 86/100\n",
      "9680/9680 [==============================] - 1s 127us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 87/100\n",
      "9680/9680 [==============================] - 1s 122us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 88/100\n",
      "9680/9680 [==============================] - 2s 166us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 89/100\n",
      "9680/9680 [==============================] - 2s 186us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 90/100\n",
      "9680/9680 [==============================] - 1s 142us/sample - loss: 5.8347 - binary_accuracy: 0.6195s - loss: 5.8230 - binary_accu\n",
      "Epoch 91/100\n",
      "9680/9680 [==============================] - 1s 123us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 92/100\n",
      "9680/9680 [==============================] - 1s 117us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 93/100\n",
      "9680/9680 [==============================] - 1s 123us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 94/100\n",
      "9680/9680 [==============================] - 1s 116us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 95/100\n",
      "9680/9680 [==============================] - 1s 119us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 96/100\n",
      "9680/9680 [==============================] - 1s 150us/sample - loss: 5.8347 - binary_accuracy: 0.6195s - loss: 5.8426 - bina\n",
      "Epoch 97/100\n",
      "9680/9680 [==============================] - 1s 150us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 98/100\n",
      "9680/9680 [==============================] - 1s 149us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 99/100\n",
      "9680/9680 [==============================] - 2s 181us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n",
      "Epoch 100/100\n",
      "9680/9680 [==============================] - 2s 166us/sample - loss: 5.8347 - binary_accuracy: 0.6195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [28575552.0, 28575588.0, 28575620.0, 28575624.0, 28575628.0, 28575638.0, 28575670.0, 28575720.0, 28575724.0, 28575752.0, 28575766.0, 28575892.0, 28575924.0, 28576000.0, 28576012.0, 28576064.0, 28576098.0, 28576154.0, 28576232.0, 28576412.0, 28576416.0, 28576428.0, 28576440.0, 28576442.0, 28576444.0, 28576448.0, 28576464.0, 28576466.0, 28576468.0, 28576470.0, 28576474.0, 28576476.0, 28576486.0, 28576488.0, 28576494.0, 28576498.0, 28576500.0, 28576504.0, 28576506.0, 28576508.0, 28576512.0, 28576522.0, 28576524.0, 28576536.0, 28576558.0, 28576562.0, 28576568.0, 28576580.0, 28576582.0, 28576588.0, 28576596.0, 28576600.0, 28576608.0, 28576612.0, 28576614.0, 28576616.0, 28576624.0, 28576628.0, 28576632.0, 28576638.0, 28576648.0, 28576658.0, 28576664.0, 28576666.0, 28576668.0, 28576670.0, 28576678.0, 28576680.0, 28576688.0, 28576690.0, 28576694.0, 28576696.0, 28576698.0, 28576704.0, 28576708.0, 28576710.0, 28576712.0, 28576714.0, 28576716.0, 28576718.0, 28576720.0, 28576722.0, 28576724.0, 28576726.0, 28576728.0, 28576730.0, 28576732.0, 28576734.0, 28576736.0, 28576738.0, 28576740.0, 28576742.0, 28576744.0, 28576746.0, 28576748.0, 28576750.0, 28576752.0, 28576756.0, 28576758.0, 28576760.0, 28576762.0, 28576764.0, 28576766.0, 28576768.0, 28576770.0, 28576772.0, 28576774.0, 28576776.0, 28576778.0, 28576780.0, 28576782.0, 28576784.0, 28576786.0, 28576788.0, 28576790.0, 28576792.0, 28576794.0, 28576796.0, 28576798.0, 28576800.0, 28576802.0, 28576804.0, 28576806.0, 28576808.0, 28576810.0, 28576812.0, 28576816.0, 28576818.0, 28576820.0, 28576822.0, 28576824.0, 28576826.0, 28576828.0, 28576830.0, 28576832.0, 28576834.0, 28576836.0, 28576838.0, 28576840.0, 28576842.0, 28576844.0, 28576846.0, 28576848.0, 28576850.0, 28576852.0, 28576854.0, 28576856.0, 28576858.0, 28576860.0, 28576862.0, 28576864.0, 28576866.0, 28576868.0, 28576870.0, 28576872.0, 28576874.0, 28576876.0, 28576878.0, 28576880.0, 28576882.0, 28576884.0, 28576886.0, 28576888.0, 28576890.0, 28576892.0, 28576894.0, 28576896.0, 28576898.0, 28576900.0, 28576902.0, 28576904.0, 28576906.0, 28576908.0, 28576910.0, 28576912.0, 28576914.0, 28576916.0, 28576918.0, 28576920.0, 28576922.0, 28576924.0, 28576926.0, 28576928.0, 28576932.0, 28576934.0, 28576936.0, 28576938.0, 28576940.0, 28576942.0, 28576944.0, 28576946.0, 28576948.0, 28576952.0, 28576954.0, 28576956.0, 28576958.0, 28576960.0, 28576962.0, 28576964.0, 28576966.0, 28576968.0, 28576970.0, 28576972.0, 28576974.0, 28576976.0, 28576980.0, 28576982.0, 28576984.0, 28576988.0, 28576992.0, 28576994.0, 28577000.0, 28577010.0, 28577036.0, 28577048.0, 28577118.0, 28577136.0, 28577178.0, 28577180.0, 28577260.0, 28577284.0, 28577336.0, 28577348.0, 28577378.0, 28577448.0, 28577468.0, 28577572.0, 28577574.0, 28577602.0, 28577616.0, 28577632.0, 28577634.0, 28577638.0, 28577656.0, 28577660.0, 28577668.0, 28577684.0, 28577686.0, 28577690.0, 28577692.0, 28577696.0, 28577708.0, 28577712.0, 28577720.0, 28577722.0, 28577726.0, 28577728.0, 28577746.0, 28577748.0, 28577754.0, 28577756.0, 28577768.0, 28577772.0, 28577782.0, 28577798.0, 28577802.0, 28577804.0, 28577806.0, 28577808.0, 28577810.0, 28577812.0, 28577814.0, 28577822.0, 28577828.0, 28577834.0, 28577836.0, 28577838.0, 28577840.0, 28577844.0, 28577848.0, 28577850.0, 28577852.0, 28577854.0, 28577856.0, 28577858.0, 28577860.0, 28577862.0, 28577864.0, 28577866.0, 28577868.0, 28577870.0, 28577872.0, 28577874.0, 28577876.0, 28577878.0, 28577880.0, 28577882.0, 28577884.0, 28577886.0, 28577888.0, 28577890.0, 28577892.0, 28577894.0, 28577896.0, 28577898.0, 28577900.0, 28577902.0, 28577904.0, 28577906.0, 28577908.0, 28577910.0, 28577914.0, 28577916.0, 28577918.0, 28577920.0, 28577922.0, 28577924.0, 28577926.0, 28577928.0, 28577930.0, 28577932.0, 28577934.0, 28577936.0, 28577938.0, 28577940.0, 28577942.0, 28577944.0, 28577946.0, 28577948.0, 28577950.0, 28577952.0, 28577954.0, 28577956.0, 28577958.0, 28577960.0, 28577962.0, 28577964.0, 28577966.0, 28577968.0, 28577970.0, 28577972.0, 28577974.0, 28577976.0, 28577978.0, 28577980.0, 28577982.0, 28577984.0, 28577986.0, 28577988.0, 28577990.0, 28577992.0, 28577994.0, 28577996.0, 28577998.0, 28578000.0, 28578002.0, 28578004.0, 28578008.0, 28578010.0, 28578012.0, 28578014.0, 28578016.0, 28578018.0, 28578020.0, 28578022.0, 28578024.0, 28578026.0, 28578028.0, 28578030.0, 28578032.0, 28578034.0, 28578036.0, 28578038.0, 28578040.0, 28578042.0, 28578044.0, 28578046.0, 28578048.0, 28578050.0, 28578052.0, 28578054.0, 28578056.0, 28578058.0, 28578060.0, 28578062.0, 28578064.0, 28578066.0, 28578068.0, 28578070.0, 28578072.0, 28578074.0, 28578076.0, 28578078.0, 28578080.0, 28578082.0, 28578084.0, 28578086.0, 28578088.0, 28578090.0, 28578092.0, 28578094.0, 28578096.0, 28578100.0, 28578102.0, 28578104.0, 28578106.0, 28578108.0, 28578110.0, 28578112.0, 28578114.0, 28578116.0, 28578118.0, 28578120.0, 28578122.0, 28578124.0, 28578126.0, 28578128.0, 28578130.0, 28578132.0, 28578134.0, 28578136.0, 28578138.0, 28578140.0, 28578142.0, 28578144.0, 28578146.0, 28578148.0, 28578150.0, 28578152.0, 28578154.0, 28578156.0, 28578158.0, 28578160.0, 28578162.0, 28578164.0, 28578166.0, 28578168.0, 28578170.0, 28578172.0, 28578174.0, 28578176.0, 28578178.0, 28578180.0, 28578182.0, 28578184.0, 28578186.0, 28578188.0, 28578190.0, 28578192.0, 28578194.0, 28578196.0, 28578198.0, 28578200.0, 28578202.0, 28578204.0, 28578206.0, 28578208.0, 28578210.0, 28578212.0, 28578214.0, 28578216.0, 28578218.0, 28578220.0, 28578222.0, 28578224.0, 28578226.0, 28578228.0, 28578230.0, 28578232.0, 28578234.0, 28578236.0, 28578238.0, 28578240.0, 28578242.0, 28578244.0, 28578246.0, 28578248.0, 28578250.0, 28578252.0, 28578254.0, 28578256.0, 28578258.0, 28578260.0, 28578262.0, 28578264.0, 28578266.0, 28578268.0, 28578270.0, 28578272.0, 28578274.0, 28578276.0, 28578278.0, 28578280.0, 28578282.0, 28578284.0, 28578286.0, 28578288.0, 28578290.0, 28578292.0, 28578294.0, 28578296.0, 28578298.0, 28578300.0, 28578302.0, 28578304.0, 28578306.0, 28578308.0, 28578310.0, 28578312.0, 28578314.0, 28578316.0, 28578318.0, 28578320.0, 28578322.0, 28578324.0, 28578326.0, 28578328.0, 28578330.0, 28578332.0, 28578334.0, 28578336.0, 28578338.0, 28578340.0, 28578342.0, 28578344.0, 28578346.0, 28578348.0, 28578350.0, 28578352.0, 28578354.0, 28578356.0, 28578358.0, 28578360.0, 28578362.0, 28578364.0, 28578366.0, 28578368.0, 28578370.0, 28578372.0, 28578374.0, 28578378.0, 28578380.0, 28578382.0, 28578384.0, 28578386.0, 28578388.0, 28578390.0, 28578392.0, 28578396.0, 28578404.0, 28578426.0, 28578512.0, 28578518.0, 28578530.0, 28578544.0, 28578552.0, 28578556.0, 28578566.0, 28578568.0, 28578574.0, 28578580.0, 28578584.0, 28578592.0, 28578600.0, 28578602.0, 28578610.0, 28578620.0, 28578654.0, 28578740.0, 28578768.0, 28578782.0, 28578794.0, 28578796.0, 28578800.0, 28578804.0, 28578806.0, 28578808.0, 28578810.0, 28578812.0, 28578814.0, 28578816.0, 28578820.0, 28578822.0, 28578824.0, 28578826.0, 28578828.0, 28578830.0, 28578832.0, 28578834.0, 28578836.0, 28578838.0, 28578840.0, 28578842.0, 28578844.0, 28578846.0, 28578848.0, 28578850.0, 28578852.0, 28578854.0, 28578856.0, 28578858.0, 28578860.0, 28578862.0, 28578864.0, 28578866.0, 28578868.0, 28578870.0, 28578872.0, 28578874.0, 28578876.0, 28578878.0, 28578880.0, 28578882.0, 28578884.0, 28578886.0, 28578888.0, 28578890.0, 28578892.0, 28578894.0, 28578896.0, 28578898.0, 28578900.0, 28578902.0, 28578904.0, 28578906.0, 28578908.0, 28578910.0, 28578912.0, 28578914.0, 28578918.0, 28578920.0, 28578922.0, 28578926.0, 28578930.0, 28578932.0, 28578944.0, 28579066.0, 68900990.0, 68901060.0, 68901070.0, 68901090.0, 68901120.0, 68901160.0, 68901200.0, 68901240.0, 68901304.0, 68901310.0, 68901360.0, 68901420.0, 68901730.0, 68901740.0, 68901760.0, 68901780.0, 68901790.0, 68901810.0, 68901820.0, 68901840.0, 68901860.0, 68901890.0, 68901896.0, 68901900.0, 68901930.0, 68901950.0, 68901976.0, 68901980.0, 68901990.0, 68902000.0, 68902020.0, 68902040.0, 68902050.0, 68902056.0, 68902080.0, 68902100.0, 68902104.0, 68902110.0, 68902120.0, 68902130.0, 68902140.0, 68902150.0, 68902160.0, 68902170.0, 68902190.0, 68902260.0, 68902264.0, 68902290.0, 68902296.0, 68902300.0, 68902560.0, 68902570.0, 68902580.0, 68902584.0, 68902590.0, 68902600.0, 68902610.0, 68902616.0, 68902620.0, 68902630.0, 68902640.0, 68902650.0, 68902660.0, 68902664.0, 68902670.0, 68902680.0, 68902690.0, 68902696.0, 68902700.0, 68902710.0, 68902720.0, 68902730.0, 68902740.0, 68902744.0, 68902750.0, 68902760.0, 68902770.0, 68902776.0, 68902780.0, 68902790.0, 68902800.0, 68902810.0, 68902820.0, 68902824.0, 68902830.0, 68902840.0, 68902850.0, 68902856.0, 68902860.0, 68902870.0, 68902880.0, 68902890.0, 68902900.0, 68902904.0, 68902910.0, 68902920.0, 68902930.0, 68902936.0, 68902940.0, 68902950.0, 68902960.0, 68902970.0, 68902980.0, 68902984.0, 68902990.0, 68903000.0, 68903010.0, 68903016.0, 68903020.0, 68903030.0, 68903040.0, 68903050.0, 68903060.0, 68903090.0, 68903120.0, 68903430.0, 68903460.0, 68903470.0, 68903520.0, 68903550.0, 68903570.0, 68903630.0, 68903780.0, 68903860.0, 68903910.0, 68903930.0, 68903940.0, 68903970.0, 68904000.0, 68904024.0, 68904030.0, 68904050.0, 68904056.0, 68904060.0, 68904070.0, 68904080.0, 68904120.0, 68904130.0, 68904136.0, 68904140.0, 68904160.0, 68904180.0, 68904190.0, 68904200.0, 68904210.0, 68904216.0, 68904220.0, 68904240.0, 68904290.0, 68904296.0, 68904340.0, 68904344.0, 68904350.0, 68904360.0, 68904400.0, 68904450.0, 68904456.0, 68904480.0, 68904530.0, 68904540.0, 68904550.0, 68904560.0, 68904570.0, 68904580.0, 68904590.0, 68904610.0, 68904616.0, 68904620.0, 68904640.0, 68904660.0, 68904664.0, 68904670.0, 68904690.0, 68904700.0, 68904710.0, 68904720.0, 68904730.0, 68904740.0, 68904744.0, 68904750.0, 68904760.0, 68904770.0, 68904776.0, 68904780.0, 68904790.0, 68904800.0, 68904810.0, 68904820.0, 68904824.0, 68904830.0, 68904840.0, 68904850.0, 68904856.0, 68904860.0, 68904870.0, 68904880.0, 68904890.0, 68904900.0, 68904904.0, 68904910.0, 68904920.0, 68904930.0, 68904936.0, 68904940.0, 68904950.0, 68904960.0, 68904970.0, 68904980.0, 68904984.0, 68904990.0, 68905000.0, 68905010.0, 68905016.0, 68905020.0, 68905030.0, 68905040.0, 68905050.0, 68905060.0, 68905064.0, 68905070.0, 68905080.0, 68905090.0, 68905096.0, 68905100.0, 68905110.0, 68905120.0, 68905130.0, 68905140.0, 68905144.0, 68905150.0, 68905160.0, 68905170.0, 68905176.0, 68905180.0, 68905190.0, 68905200.0, 68905210.0, 68905220.0, 68905224.0, 68905230.0, 68905240.0, 68905250.0, 68905256.0, 68905260.0, 68905280.0, 68905310.0, 68905336.0, 68905350.0, 68905360.0, 68905380.0, 68905384.0, 68905390.0, 68905400.0, 68905410.0, 68905416.0, 68905420.0, 68905430.0, 68905440.0, 68905450.0, 68905460.0, 68905464.0, 68905470.0, 68905480.0, 68905490.0, 68905496.0, 68905500.0, 68905510.0, 68905520.0, 68905530.0, 68905540.0, 68905544.0, 68905550.0, 68905570.0, 68905576.0, 68905580.0, 68905590.0, 68905600.0, 68905610.0, 68905620.0, 68905624.0, 68905630.0, 68905640.0, 68905650.0, 68905656.0, 68905660.0, 68905670.0, 68905680.0, 68905690.0, 68905700.0, 68905704.0, 68905710.0, 68905720.0, 68905730.0, 68905736.0, 68905740.0, 68905750.0, 68905760.0, 68905770.0, 68905780.0, 68905784.0, 68905790.0, 68905800.0, 68905810.0, 68905816.0, 68905820.0, 68905830.0, 68905840.0, 68905850.0, 68905860.0, 68905864.0, 68905870.0, 68905880.0, 68905890.0, 68905896.0, 68905900.0, 68905910.0, 68905920.0, 68905940.0, 68905980.0, 68905990.0, 68906020.0, 68906030.0, 68906050.0, 68906060.0, 68906070.0, 68906090.0, 68906100.0, 68906180.0, 68906190.0, 68906220.0, 68906260.0, 68906640.0, 68906680.0, 68906690.0, 68906700.0, 68906740.0, 68906770.0, 68906800.0, 68906820.0, 68906824.0, 68906830.0, 68906840.0, 68906850.0, 68906856.0, 68906860.0, 68906870.0, 68906880.0, 68906890.0, 68906900.0, 68906904.0, 68906910.0, 68906920.0, 68906930.0, 68906936.0, 68906940.0, 68906950.0, 68906960.0, 68906970.0, 68906980.0, 68906984.0, 68906990.0, 68907010.0, 68907020.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7794299876084263\n",
      "Hamming Loss: 0.11338289962825279\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1946\n",
      "           1       0.20      1.00      0.33       475\n",
      "\n",
      "    accuracy                           0.20      2421\n",
      "   macro avg       0.10      0.50      0.16      2421\n",
      "weighted avg       0.04      0.20      0.06      2421\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9681 samples\n",
      "Epoch 1/100\n",
      "9681/9681 [==============================] - 3s 355us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 2/100\n",
      "9681/9681 [==============================] - 2s 171us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 3/100\n",
      "9681/9681 [==============================] - 1s 155us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 4/100\n",
      "9681/9681 [==============================] - 2s 168us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 5/100\n",
      "9681/9681 [==============================] - 1s 135us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 6/100\n",
      "9681/9681 [==============================] - 1s 127us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 7/100\n",
      "9681/9681 [==============================] - 1s 103us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 8/100\n",
      "9681/9681 [==============================] - 1s 107us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 9/100\n",
      "9681/9681 [==============================] - 1s 103us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 10/100\n",
      "9681/9681 [==============================] - 1s 105us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 11/100\n",
      "9681/9681 [==============================] - 1s 149us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 12/100\n",
      "9681/9681 [==============================] - 2s 164us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 13/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 14/100\n",
      "9681/9681 [==============================] - 1s 141us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 15/100\n",
      "9681/9681 [==============================] - 1s 114us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 16/100\n",
      "9681/9681 [==============================] - 2s 155us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 17/100\n",
      "9681/9681 [==============================] - 2s 197us/sample - loss: 1.8145 - binary_accuracy: 0.8823s - loss: 1.8174 - binary_accuracy: 0.88\n",
      "Epoch 18/100\n",
      "9681/9681 [==============================] - 1s 142us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 19/100\n",
      "9681/9681 [==============================] - 1s 148us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 20/100\n",
      "9681/9681 [==============================] - 2s 167us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 21/100\n",
      "9681/9681 [==============================] - 2s 192us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 22/100\n",
      "9681/9681 [==============================] - 1s 146us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 23/100\n",
      "9681/9681 [==============================] - 1s 139us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 24/100\n",
      "9681/9681 [==============================] - 1s 136us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 25/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 26/100\n",
      "9681/9681 [==============================] - 1s 132us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 27/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 28/100\n",
      "9681/9681 [==============================] - 1s 138us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 29/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 30/100\n",
      "9681/9681 [==============================] - 1s 140us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 31/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 32/100\n",
      "9681/9681 [==============================] - 1s 131us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 33/100\n",
      "9681/9681 [==============================] - 1s 154us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 34/100\n",
      "9681/9681 [==============================] - 1s 142us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 35/100\n",
      "9681/9681 [==============================] - 2s 162us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 36/100\n",
      "9681/9681 [==============================] - 2s 165us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 37/100\n",
      "9681/9681 [==============================] - 2s 170us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 38/100\n",
      "9681/9681 [==============================] - 2s 163us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 39/100\n",
      "9681/9681 [==============================] - 2s 167us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 40/100\n",
      "9681/9681 [==============================] - 2s 160us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 41/100\n",
      "9681/9681 [==============================] - 2s 158us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 42/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 43/100\n",
      "9681/9681 [==============================] - 2s 162us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 44/100\n",
      "9681/9681 [==============================] - 2s 176us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 45/100\n",
      "9681/9681 [==============================] - 2s 159us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 46/100\n",
      "9681/9681 [==============================] - 2s 160us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 47/100\n",
      "9681/9681 [==============================] - 1s 154us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 48/100\n",
      "9681/9681 [==============================] - 2s 161us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 49/100\n",
      "9681/9681 [==============================] - 2s 178us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 50/100\n",
      "9681/9681 [==============================] - 2s 163us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 51/100\n",
      "9681/9681 [==============================] - 2s 162us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 52/100\n",
      "9681/9681 [==============================] - 2s 168us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 53/100\n",
      "9681/9681 [==============================] - 2s 169us/sample - loss: 1.8145 - binary_accuracy: 0.8823s - loss: 1.\n",
      "Epoch 54/100\n",
      "9681/9681 [==============================] - 2s 171us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 55/100\n",
      "9681/9681 [==============================] - 2s 168us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 56/100\n",
      "9681/9681 [==============================] - 2s 166us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 57/100\n",
      "9681/9681 [==============================] - 2s 176us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 58/100\n",
      "9681/9681 [==============================] - 1s 149us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 59/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 60/100\n",
      "9681/9681 [==============================] - 1s 122us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 61/100\n",
      "9681/9681 [==============================] - 1s 143us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 62/100\n",
      "9681/9681 [==============================] - 2s 177us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 63/100\n",
      "9681/9681 [==============================] - 2s 159us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 64/100\n",
      "9681/9681 [==============================] - 2s 162us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 65/100\n",
      "9681/9681 [==============================] - 2s 164us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 66/100\n",
      "9681/9681 [==============================] - 2s 167us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 67/100\n",
      "9681/9681 [==============================] - 2s 163us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 68/100\n",
      "9681/9681 [==============================] - 2s 169us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 69/100\n",
      "9681/9681 [==============================] - 2s 169us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 70/100\n",
      "9681/9681 [==============================] - 2s 163us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 71/100\n",
      "9681/9681 [==============================] - 2s 161us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 72/100\n",
      "9681/9681 [==============================] - 2s 158us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 73/100\n",
      "9681/9681 [==============================] - 2s 171us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 74/100\n",
      "9681/9681 [==============================] - 2s 164us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 75/100\n",
      "9681/9681 [==============================] - 2s 159us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 76/100\n",
      "9681/9681 [==============================] - 2s 171us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 77/100\n",
      "9681/9681 [==============================] - 2s 168us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 78/100\n",
      "9681/9681 [==============================] - 2s 162us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 79/100\n",
      "9681/9681 [==============================] - 2s 156us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 80/100\n",
      "9681/9681 [==============================] - 2s 159us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 81/100\n",
      "9681/9681 [==============================] - 2s 171us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 82/100\n",
      "9681/9681 [==============================] - 1s 139us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 83/100\n",
      "9681/9681 [==============================] - 1s 123us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 84/100\n",
      "9681/9681 [==============================] - 1s 149us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 85/100\n",
      "9681/9681 [==============================] - 2s 179us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 86/100\n",
      "9681/9681 [==============================] - 2s 163us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 87/100\n",
      "9681/9681 [==============================] - 2s 166us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 88/100\n",
      "9681/9681 [==============================] - 2s 182us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 89/100\n",
      "9681/9681 [==============================] - 2s 164us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 90/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 91/100\n",
      "9681/9681 [==============================] - 1s 132us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 92/100\n",
      "9681/9681 [==============================] - 1s 138us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 93/100\n",
      "9681/9681 [==============================] - 1s 145us/sample - loss: 1.8145 - binary_accuracy: 0.8823s - loss: 1.8276 - binary_a\n",
      "Epoch 94/100\n",
      "9681/9681 [==============================] - 2s 175us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 95/100\n",
      "9681/9681 [==============================] - 2s 156us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 96/100\n",
      "9681/9681 [==============================] - 2s 158us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 97/100\n",
      "9681/9681 [==============================] - 2s 161us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 98/100\n",
      "9681/9681 [==============================] - 1s 155us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 99/100\n",
      "9681/9681 [==============================] - 1s 151us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n",
      "Epoch 100/100\n",
      "9681/9681 [==============================] - 2s 179us/sample - loss: 1.8145 - binary_accuracy: 0.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-127837540.0, -127837620.0, -127837656.0, -127837700.0, -127837704.0, -127837710.0, -127837720.0, -127837730.0, -127837736.0, -127837740.0, -127837750.0, -127837760.0, -127837770.0, -127837780.0, -127837784.0, -127837790.0, -127837800.0, -127837810.0, -127837816.0, -127837820.0, -127837830.0, -127837840.0, -127837850.0, -127837860.0, -127837864.0, -127837870.0, -127837880.0, -127837890.0, -127837896.0, -127837900.0, -127837910.0, -127837920.0, -127837930.0, -127837940.0, -127837970.0, -127838050.0, -127838100.0, -127838130.0, -127838160.0, -127838180.0, -127838240.0, -127838270.0, -127838290.0, -127838296.0, -127838320.0, -127838340.0, -127838344.0, -127838380.0, -127838400.0, -127838420.0, -127838430.0, -127838440.0, -127838470.0, -127838490.0, -127838500.0, -127838510.0, -127847680.0, -127847704.0, -127847740.0, -127847760.0, -127847770.0, -127847784.0, -127847790.0, -127847810.0, -127847816.0, -127847870.0, -127847880.0, -127847890.0, -127847896.0, -127847900.0, -127847910.0, -127847920.0, -127847930.0, -127847940.0, -127847944.0, -127847950.0, -127847960.0, -127847970.0, -127847976.0, -127847980.0, -127847990.0, -127848000.0, -127848010.0, -127848020.0, -127848024.0, -127848030.0, -127848040.0, -127848050.0, -127848056.0, -127848060.0, -127848070.0, -127848080.0, -127848090.0, -127848100.0, -127848104.0, -127848110.0, -127848120.0, -127848130.0, -127848136.0, -127848140.0, -127848150.0, -127848160.0, -127848170.0, -127848180.0, -127848184.0, -127848190.0, -127848200.0, -127848210.0, -127848216.0, -127848220.0, -127848230.0, -127848240.0, -127848250.0, -127848260.0, -127848264.0, -127848270.0, -127848280.0, -127848290.0, -127848296.0, -127848300.0, -127848310.0, -127848320.0, -127848330.0, -127848340.0, -127848376.0, -127848420.0, -127848430.0, -127848450.0, -127848456.0, -127848510.0, -127848530.0, -127848536.0, -127848570.0, -127848580.0, -127848590.0, -127848610.0, -127848620.0, -127848640.0, -127848650.0, -127848660.0, -127848664.0, -127848670.0, -127848690.0, -127848700.0, -127848710.0, -127848720.0, -127848730.0, -127848750.0, -127848770.0, -127848780.0, -127848820.0, -127848830.0, -127849100.0, -127849110.0, -127849120.0, -127849130.0, -127849144.0, -127849150.0, -127849170.0, -127849300.0, -127849304.0, -127849310.0, -127849320.0, -127849330.0, -127849336.0, -127849340.0, -127849350.0, -127849360.0, -127849370.0, -127849380.0, -127849384.0, -127849390.0, -127849400.0, -127849410.0, -127849416.0, -127849420.0, -127849430.0, -127849440.0, -127849450.0, -127849460.0, -127849464.0, -127849470.0, -127849480.0, -127849490.0, -127849496.0, -127849500.0, -127849510.0, -127849520.0, -127849530.0, -127849540.0, -127849544.0, -127849550.0, -127849560.0, -127849570.0, -127849576.0, -127849580.0, -127849590.0, -127849600.0, -127849610.0, -127849620.0, -127849624.0, -127849630.0, -127849640.0, -127849650.0, -127849656.0, -127849660.0, -127849670.0, -127849680.0, -127849690.0, -127849700.0, -127849704.0, -127849710.0, -127849720.0, -127849730.0, -127849736.0, -127849740.0, -127849750.0, -127849760.0, -127849770.0, -127849780.0, -127849784.0, -127849790.0, -127849800.0, -127849810.0, -127849816.0, -127849820.0, -127849830.0, -127849840.0, -127849850.0, -127849860.0, -127849864.0, -127849870.0, -127849880.0, -127849890.0, -127849896.0, -127849900.0, -127849910.0, -127849920.0, -127849930.0, -127849940.0, -127849944.0, -127849950.0, -127849960.0, -127849970.0, -127849976.0, -127849980.0, -127849990.0, -127850000.0, -127850010.0, -127850020.0, -127850024.0, -127850030.0, -127850040.0, -127850050.0, -127850056.0, -127850060.0, -127850070.0, -127850080.0, -127850090.0, -127850100.0, -127850110.0, -127850130.0, -127850136.0, -127850140.0, -127850150.0, -127850160.0, -127850180.0, -127850184.0, -127850190.0, -127850210.0, -127850216.0, -127850220.0, -127850240.0, -127850280.0, -127850290.0, -127850300.0, -127850310.0, -127850330.0, -127850340.0, -127850376.0, -127850390.0, -127850400.0, -127850410.0, -127850420.0, -127850430.0, -127850440.0, -127850450.0, -127850490.0, -127850510.0, -127850520.0, -127850530.0, -127850540.0, -127850560.0, -127850570.0, -127850580.0, -127850584.0, -127850610.0, -127850620.0, -127850690.0, -127852300.0, -127852330.0, -127852340.0, -127852350.0, -127852400.0, -127852460.0, -127852470.0, -127852480.0, -127852490.0, -127852500.0, -127852504.0, -127852510.0, -127852520.0, -127852530.0, -127852536.0, -127852540.0, -127852550.0, -127852560.0, -127852570.0, -127852580.0, -127852584.0, -127852590.0, -127852600.0, -127852610.0, -127852616.0, -127852620.0, -127852630.0, -127852640.0, -127852650.0, -127852660.0, -127852664.0, -127852670.0, -127852680.0, -127852690.0, -127852696.0, -127852700.0, -127852710.0, -127852720.0, -127852730.0, -127852740.0, -127852744.0, -127852750.0, -127852760.0, -127852770.0, -127852776.0, -127852780.0, -127852790.0, -127852800.0, -127852810.0, -127852820.0, -127852850.0, -127852880.0, -127852984.0, -127853000.0, -127853040.0, -127853060.0, -127853080.0, -127853096.0, -127853100.0, -127853130.0, -127853150.0, -127853180.0, -127853200.0, -127853220.0, -127853240.0, -127853260.0, -127853330.0, -127853384.0, -127853390.0, -127853420.0, -127853450.0, 6528936.0, 6528940.0, 6528944.0, 6528948.0, 6528952.0, 6528956.0, 6528960.0, 6528964.0, 6528968.0, 6528972.0, 6528976.0, 6528980.0, 6528984.0, 6528988.0, 6528992.0, 6528996.0, 6529000.0, 6529004.0, 6529008.0, 6529012.0, 6529016.0, 6529020.0, 6529024.0, 6529028.0, 6529032.0, 6529036.0, 6529040.0, 6529044.0, 6529048.0, 6529052.0, 6529056.0, 6529060.0, 6529064.0, 6529068.0, 6529072.0, 6529076.0, 6529080.0, 6529104.0, 6529184.0, 6529196.0, 6529200.0, 6529204.0, 6529208.0, 6529212.0, 6529216.0, 6529220.0, 6529224.0, 6529228.0, 6529232.0, 6529236.0, 6529240.0, 6529244.0, 6529248.0, 6529252.0, 6529256.0, 6529260.0, 6529264.0, 6529268.0, 6529272.0, 6529276.0, 6529280.0, 6529284.0, 6529288.0, 6529292.0, 6529296.0, 6529300.0, 6529304.0, 6529308.0, 6529312.0, 6529316.0, 6529320.0, 6529324.0, 6529328.0, 6529332.0, 6529336.0, 6529340.0, 6529344.0, 6529348.0, 6529352.0, 6529356.0, 6529360.0, 6529364.0, 6529368.0, 6529372.0, 6529376.0, 6529380.0, 6529384.0, 6529388.0, 6529392.0, 6529396.0, 6529400.0, 6529404.0, 6529408.0, 6529412.0, 6529416.0, 6529420.0, 6529424.0, 6529428.0, 6529432.0, 6529436.0, 6529440.0, 6529448.0, 6529456.0, 6529464.0, 6529468.0, 6529476.0, 6529500.0, 6529508.0, 6529516.0, 6529612.0, 6529624.0, 6529632.0, 6529640.0, 6529696.0, 6529712.0, 6529716.0, 6529720.0, 6529732.0, 6529744.0, 6529748.0, 6529764.0, 6529780.0, 6529788.0, 6529800.0, 6529812.0, 6529852.0, 6530004.0, 6530012.0, 6530020.0, 6530028.0, 6530032.0, 6530036.0, 6530040.0, 6530044.0, 6530048.0, 6530052.0, 6530056.0, 6530060.0, 6530064.0, 6530068.0, 6530072.0, 6530076.0, 6530080.0, 6530084.0, 6530088.0, 6530092.0, 6530096.0, 6530100.0, 6530104.0, 6530108.0, 6530112.0, 6530116.0, 6530120.0, 6530124.0, 6530128.0, 6530132.0, 6530136.0, 6530140.0, 6530144.0, 6530148.0, 6530152.0, 6530156.0, 6530160.0, 6530164.0, 6530168.0, 6530172.0, 6530184.0, 6530196.0, 6530224.0, 6530380.0, 6530412.0, 6530428.0, 6530448.0, 6530508.0, 6530516.0, 6530520.0, 6530532.0, 6530552.0, 6530568.0, 6530608.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6859504132231405\n",
      "Hamming Loss: 0.1706611570247934\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90      1975\n",
      "           1       0.00      0.00      0.00       445\n",
      "\n",
      "    accuracy                           0.82      2420\n",
      "   macro avg       0.41      0.50      0.45      2420\n",
      "weighted avg       0.67      0.82      0.73      2420\n",
      "\n",
      "Train on 9681 samples\n",
      "Epoch 1/100\n",
      "9681/9681 [==============================] - 5s 564us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 2/100\n",
      "9681/9681 [==============================] - 2s 162us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 3/100\n",
      "9681/9681 [==============================] - 2s 156us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 4/100\n",
      "9681/9681 [==============================] - 1s 132us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 5/100\n",
      "9681/9681 [==============================] - 1s 131us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 6/100\n",
      "9681/9681 [==============================] - 1s 132us/sample - loss: 5.8618 - binary_accuracy: 0.6177s - loss: 6.0909 - b\n",
      "Epoch 7/100\n",
      "9681/9681 [==============================] - 1s 137us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 8/100\n",
      "9681/9681 [==============================] - 1s 139us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 9/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 10/100\n",
      "9681/9681 [==============================] - 1s 143us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 11/100\n",
      "9681/9681 [==============================] - 1s 133us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 12/100\n",
      "9681/9681 [==============================] - 2s 166us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 13/100\n",
      "9681/9681 [==============================] - 2s 177us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 14/100\n",
      "9681/9681 [==============================] - 2s 175us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 15/100\n",
      "9681/9681 [==============================] - 2s 163us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 16/100\n",
      "9681/9681 [==============================] - 2s 177us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 17/100\n",
      "9681/9681 [==============================] - 2s 178us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 18/100\n",
      "9681/9681 [==============================] - 1s 112us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 19/100\n",
      "9681/9681 [==============================] - 1s 99us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 20/100\n",
      "9681/9681 [==============================] - 1s 103us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 21/100\n",
      "9681/9681 [==============================] - 1s 102us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 22/100\n",
      "9681/9681 [==============================] - 1s 109us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 23/100\n",
      "9681/9681 [==============================] - 2s 164us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 24/100\n",
      "9681/9681 [==============================] - 2s 172us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 25/100\n",
      "9681/9681 [==============================] - 2s 155us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 26/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 27/100\n",
      "9681/9681 [==============================] - 1s 145us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 28/100\n",
      "9681/9681 [==============================] - 1s 126us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 29/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 30/100\n",
      "9681/9681 [==============================] - 1s 127us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 31/100\n",
      "9681/9681 [==============================] - 1s 126us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 32/100\n",
      "9681/9681 [==============================] - 1s 126us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 33/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 34/100\n",
      "9681/9681 [==============================] - 1s 135us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 35/100\n",
      "9681/9681 [==============================] - ETA: 0s - loss: 5.8613 - binary_accuracy: 0.617 - 1s 135us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 36/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 37/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 38/100\n",
      "9681/9681 [==============================] - 1s 130us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 39/100\n",
      "9681/9681 [==============================] - 1s 130us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 40/100\n",
      "9681/9681 [==============================] - 1s 139us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 41/100\n",
      "9681/9681 [==============================] - 1s 133us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 42/100\n",
      "9681/9681 [==============================] - 1s 129us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 43/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 44/100\n",
      "9681/9681 [==============================] - 1s 122us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 45/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 46/100\n",
      "9681/9681 [==============================] - 1s 126us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 47/100\n",
      "9681/9681 [==============================] - 1s 119us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 48/100\n",
      "9681/9681 [==============================] - 1s 129us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 49/100\n",
      "9681/9681 [==============================] - 1s 123us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 50/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 51/100\n",
      "9681/9681 [==============================] - 1s 140us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 52/100\n",
      "9681/9681 [==============================] - 1s 129us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 53/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 54/100\n",
      "9681/9681 [==============================] - 1s 139us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 55/100\n",
      "9681/9681 [==============================] - 2s 171us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 56/100\n",
      "9681/9681 [==============================] - 2s 167us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 57/100\n",
      "9681/9681 [==============================] - 2s 187us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 58/100\n",
      "9681/9681 [==============================] - ETA: 0s - loss: 5.8625 - binary_accuracy: 0.617 - 2s 191us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 59/100\n",
      "9681/9681 [==============================] - 2s 175us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 60/100\n",
      "9681/9681 [==============================] - 2s 181us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 61/100\n",
      "9681/9681 [==============================] - 2s 157us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 62/100\n",
      "9681/9681 [==============================] - 2s 156us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 63/100\n",
      "9681/9681 [==============================] - 2s 160us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 64/100\n",
      "9681/9681 [==============================] - 2s 161us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 65/100\n",
      "9681/9681 [==============================] - 2s 164us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 66/100\n",
      "9681/9681 [==============================] - 2s 162us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 67/100\n",
      "9681/9681 [==============================] - 2s 160us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 68/100\n",
      "9681/9681 [==============================] - 2s 178us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 69/100\n",
      "9681/9681 [==============================] - 2s 164us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 70/100\n",
      "9681/9681 [==============================] - 2s 167us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 71/100\n",
      "9681/9681 [==============================] - 2s 165us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 72/100\n",
      "9681/9681 [==============================] - 2s 161us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 73/100\n",
      "9681/9681 [==============================] - 2s 192us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 74/100\n",
      "9681/9681 [==============================] - 2s 165us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 75/100\n",
      "9681/9681 [==============================] - 2s 174us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 76/100\n",
      "9681/9681 [==============================] - 2s 193us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 77/100\n",
      "9681/9681 [==============================] - 2s 179us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 78/100\n",
      "9681/9681 [==============================] - 1s 155us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 79/100\n",
      "9681/9681 [==============================] - 2s 202us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 80/100\n",
      "9681/9681 [==============================] - 2s 175us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 81/100\n",
      "9681/9681 [==============================] - 2s 166us/sample - loss: 5.8618 - binary_accuracy: 0.6177s - loss: 5.77\n",
      "Epoch 82/100\n",
      "9681/9681 [==============================] - 2s 186us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 83/100\n",
      "9681/9681 [==============================] - 1s 142us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 84/100\n",
      "9681/9681 [==============================] - 1s 133us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 85/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 86/100\n",
      "9681/9681 [==============================] - 1s 126us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 87/100\n",
      "9681/9681 [==============================] - 1s 130us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 88/100\n",
      "9681/9681 [==============================] - 1s 129us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 89/100\n",
      "9681/9681 [==============================] - 1s 130us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 90/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 91/100\n",
      "9681/9681 [==============================] - 1s 135us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 92/100\n",
      "9681/9681 [==============================] - 1s 131us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 93/100\n",
      "9681/9681 [==============================] - 1s 139us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 94/100\n",
      "9681/9681 [==============================] - 1s 137us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 95/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 96/100\n",
      "9681/9681 [==============================] - 1s 125us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 97/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 98/100\n",
      "9681/9681 [==============================] - 1s 123us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 99/100\n",
      "9681/9681 [==============================] - 1s 125us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n",
      "Epoch 100/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 5.8618 - binary_accuracy: 0.6177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [164955780.0, 164956000.0, 164956030.0, 164956050.0, 164956060.0, 164956080.0, 164956100.0, 164956110.0, 164956130.0, 164956160.0, 164956200.0, 164956220.0, 164956240.0, 164956260.0, 164956370.0, 164956560.0, 164956580.0, 164956600.0, 164956620.0, 164956640.0, 164956720.0, 164956750.0, 164956770.0, 164956780.0, 164956800.0, 164956820.0, 164956830.0, 164956850.0, 164956860.0, 164956880.0, 164956900.0, 164956910.0, 164956930.0, 164956940.0, 164956960.0, 164956980.0, 164957000.0, 164957010.0, 164957020.0, 164957040.0, 164957060.0, 164957070.0, 164957090.0, 164957100.0, 164957120.0, 164957140.0, 164957150.0, 164957170.0, 164957180.0, 164957200.0, 164957220.0, 164957230.0, 164957250.0, 164957260.0, 164957280.0, 164957300.0, 164957310.0, 164957330.0, 164957340.0, 164957360.0, 164957380.0, 164957390.0, 164957400.0, 164957420.0, 164957440.0, 164957460.0, 164957470.0, 164957490.0, 164957500.0, 164957540.0, 164957570.0, 164957600.0, 164957620.0, 164957630.0, 164957650.0, 164957660.0, 164957680.0, 164957700.0, 164957710.0, 164957730.0, 164957740.0, 164957760.0, 164957780.0, 164957800.0, 164957810.0, 164957820.0, 164957840.0, 164957860.0, 164957870.0, 164957890.0, 164957900.0, 164957920.0, 164957940.0, 164957950.0, 164957970.0, 164957980.0, 164958000.0, 164958020.0, 164958030.0, 164958050.0, 164958060.0, 164958080.0, 164958100.0, 164958110.0, 164958130.0, 164958140.0, 164958160.0, 164958180.0, 164958190.0, 164958200.0, 164958450.0, 164958460.0, 164958480.0, 181062080.0, 181062140.0, 181062200.0, 181062240.0, 181062270.0, 181062300.0, 181062340.0, 181062350.0, 181062370.0, 181062400.0, 181062430.0, 181062460.0, 181062480.0, 181062500.0, 181062510.0, 181062530.0, 181062540.0, 181062560.0, 181062580.0, 181062600.0, 181062610.0, 181062620.0, 181062640.0, 181062670.0, 181062690.0, 181062700.0, 181062720.0, 181062740.0, 181062780.0, 181062800.0, 181062820.0, 181062830.0, 181062860.0, 181062880.0, 181062910.0, 181062980.0, 181062990.0, 181063000.0, 181063070.0, 181063100.0, 181063170.0, 181063180.0, 181063200.0, 181063330.0, 181063400.0, 181063410.0, 181063420.0, 181063440.0, 181063460.0, 181063470.0, 181063490.0, 181063500.0, 181063520.0, 181063540.0, 181063550.0, 181063570.0, 181063580.0, 181063620.0, 181063630.0, 181063650.0, 181063660.0, 181063680.0, 181063700.0, 181063710.0, 181063730.0, 181063740.0, 181063760.0, 181063780.0, 181063790.0, 181063800.0, 181063820.0, 181063840.0, 181063860.0, 181063870.0, 181063890.0, 181063900.0, 181063920.0, 181063940.0, 181063950.0, 181063970.0, 181063980.0, 181064000.0, 181064020.0, 181064030.0, 181064050.0, 181064060.0, 181064080.0, 181064100.0, 181064110.0, 181064130.0, 181064140.0, 181064160.0, 181064180.0, 181064200.0, 181064210.0, 181064220.0, 181064240.0, 181064260.0, 181064270.0, 181064290.0, 181064300.0, 181064320.0, 181064340.0, 181064350.0, 181064380.0, 181064420.0, 181064480.0, 181064500.0, 181064530.0, 181064580.0, 181064590.0, 181064600.0, 181064620.0, 181064640.0, 181064660.0, 181064670.0, 181064690.0, 181064700.0, 181064720.0, 181064740.0, 181064750.0, 181064770.0, 181064780.0, 181064800.0, 181064820.0, 181064830.0, 181064850.0, 181064860.0, 181064880.0, 181064900.0, 181064910.0, 181064930.0, 181064940.0, 181064960.0, 181064980.0, 181065000.0, 181065010.0, 181065020.0, 181065040.0, 181065060.0, 181065070.0, 181065090.0, 181065100.0, 181065120.0, 181065140.0, 181065150.0, 181065170.0, 181065180.0, 181065200.0, 181065220.0, 181065230.0, 181065250.0, 181065260.0, 181065280.0, 181065300.0, 181065310.0, 181065330.0, 181065340.0, 181065360.0, 181065380.0, 181065390.0, 181065400.0, 181065420.0, 181065440.0, 181065460.0, 181065470.0, 181065490.0, 181065500.0, 181065520.0, 181065540.0, 181065550.0, 181065570.0, 181065580.0, 181065600.0, 181065620.0, 181065630.0, 181065650.0, 181065660.0, 181065680.0, 181065700.0, 181065710.0, 181065730.0, 181065740.0, 181065760.0, 181065780.0, 181065800.0, 181065810.0, 181065820.0, 181065840.0, 181065860.0, 181065870.0, 181065920.0, 181065940.0, 181065950.0, 181065970.0, 181065980.0, 181066020.0, 181066030.0, 181066050.0, 181066060.0, 181066160.0, 181066180.0, 181066200.0, 181066220.0, 181066260.0, 181066430.0, 181066480.0, 181066900.0, 181068220.0, 181068290.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7570247933884298\n",
      "Hamming Loss: 0.1287190082644628\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90      1962\n",
      "           1       0.00      0.00      0.00       458\n",
      "\n",
      "    accuracy                           0.81      2420\n",
      "   macro avg       0.41      0.50      0.45      2420\n",
      "weighted avg       0.66      0.81      0.73      2420\n",
      "\n",
      "Train on 9681 samples\n",
      "Epoch 1/100\n",
      "9681/9681 [==============================] - 2s 235us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 2/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 3/100\n",
      "9681/9681 [==============================] - 1s 125us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 4/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 5/100\n",
      "9681/9681 [==============================] - 1s 126us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 6/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 7/100\n",
      "9681/9681 [==============================] - 1s 122us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 8/100\n",
      "9681/9681 [==============================] - 1s 127us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 9/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 10/100\n",
      "9681/9681 [==============================] - 1s 125us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 11/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 12/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 13/100\n",
      "9681/9681 [==============================] - 1s 129us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 14/100\n",
      "9681/9681 [==============================] - 1s 131us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 15/100\n",
      "9681/9681 [==============================] - 1s 154us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 16/100\n",
      "9681/9681 [==============================] - 2s 173us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 17/100\n",
      "9681/9681 [==============================] - 2s 166us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 18/100\n",
      "9681/9681 [==============================] - 2s 179us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 19/100\n",
      "9681/9681 [==============================] - 1s 145us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 20/100\n",
      "9681/9681 [==============================] - 1s 140us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 21/100\n",
      "9681/9681 [==============================] - 2s 172us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 22/100\n",
      "9681/9681 [==============================] - 2s 169us/sample - loss: 2.0547 - binary_accuracy: 0.8667s - loss: 2.070\n",
      "Epoch 23/100\n",
      "9681/9681 [==============================] - 1s 131us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 24/100\n",
      "9681/9681 [==============================] - 1s 122us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 25/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 26/100\n",
      "9681/9681 [==============================] - 1s 135us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 27/100\n",
      "9681/9681 [==============================] - 1s 144us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 28/100\n",
      "9681/9681 [==============================] - 1s 141us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 29/100\n",
      "9681/9681 [==============================] - 1s 149us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 30/100\n",
      "9681/9681 [==============================] - 1s 130us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 31/100\n",
      "9681/9681 [==============================] - 1s 133us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 32/100\n",
      "9681/9681 [==============================] - 1s 144us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 33/100\n",
      "9681/9681 [==============================] - 1s 127us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 34/100\n",
      "9681/9681 [==============================] - 1s 152us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 35/100\n",
      "9681/9681 [==============================] - 2s 201us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 36/100\n",
      "9681/9681 [==============================] - 1s 143us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 37/100\n",
      "9681/9681 [==============================] - 2s 157us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 38/100\n",
      "9681/9681 [==============================] - 1s 150us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 39/100\n",
      "9681/9681 [==============================] - 1s 151us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 40/100\n",
      "9681/9681 [==============================] - 1s 147us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 41/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 42/100\n",
      "9681/9681 [==============================] - 1s 151us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 43/100\n",
      "9681/9681 [==============================] - 2s 163us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 44/100\n",
      "9681/9681 [==============================] - 2s 156us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 45/100\n",
      "9681/9681 [==============================] - 1s 127us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 46/100\n",
      "9681/9681 [==============================] - 1s 151us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 47/100\n",
      "9681/9681 [==============================] - 2s 171us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 48/100\n",
      "9681/9681 [==============================] - 2s 159us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 49/100\n",
      "9681/9681 [==============================] - 1s 118us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 50/100\n",
      "9681/9681 [==============================] - 2s 161us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 51/100\n",
      "9681/9681 [==============================] - 2s 159us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 52/100\n",
      "9681/9681 [==============================] - 1s 142us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 53/100\n",
      "9681/9681 [==============================] - 1s 113us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 54/100\n",
      "9681/9681 [==============================] - 1s 123us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 55/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 56/100\n",
      "9681/9681 [==============================] - 2s 210us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 57/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 58/100\n",
      "9681/9681 [==============================] - 2s 157us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 59/100\n",
      "9681/9681 [==============================] - 1s 136us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 60/100\n",
      "9681/9681 [==============================] - 1s 102us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 61/100\n",
      "9681/9681 [==============================] - 1s 118us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 62/100\n",
      "9681/9681 [==============================] - 1s 110us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 63/100\n",
      "9681/9681 [==============================] - 1s 108us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 64/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 65/100\n",
      "9681/9681 [==============================] - 1s 120us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 66/100\n",
      "9681/9681 [==============================] - 1s 133us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 67/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9681/9681 [==============================] - 1s 123us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 69/100\n",
      "9681/9681 [==============================] - 1s 150us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 70/100\n",
      "9681/9681 [==============================] - 1s 126us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 71/100\n",
      "9681/9681 [==============================] - 1s 109us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 72/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 73/100\n",
      "9681/9681 [==============================] - 2s 189us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 74/100\n",
      "9681/9681 [==============================] - 2s 160us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 75/100\n",
      "9681/9681 [==============================] - 1s 136us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 76/100\n",
      "9681/9681 [==============================] - 1s 140us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 77/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 78/100\n",
      "9681/9681 [==============================] - 1s 130us/sample - loss: 2.0547 - binary_accuracy: 0.8667s - loss: 2.1082 -\n",
      "Epoch 79/100\n",
      "9681/9681 [==============================] - 1s 148us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 80/100\n",
      "9681/9681 [==============================] - 1s 144us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 81/100\n",
      "9681/9681 [==============================] - 1s 133us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 82/100\n",
      "9681/9681 [==============================] - 1s 117us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 83/100\n",
      "9681/9681 [==============================] - 1s 154us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 84/100\n",
      "9681/9681 [==============================] - 1s 149us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 85/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 86/100\n",
      "9681/9681 [==============================] - 2s 171us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 87/100\n",
      "9681/9681 [==============================] - 2s 155us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 88/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0547 - binary_accuracy: 0.8667s - loss: 2.2575 - b\n",
      "Epoch 89/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 90/100\n",
      "9681/9681 [==============================] - 1s 115us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 91/100\n",
      "9681/9681 [==============================] - 1s 117us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 92/100\n",
      "9681/9681 [==============================] - 1s 120us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 93/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 94/100\n",
      "9681/9681 [==============================] - 1s 149us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 95/100\n",
      "9681/9681 [==============================] - 2s 157us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 96/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 97/100\n",
      "9681/9681 [==============================] - 1s 145us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 98/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 99/100\n",
      "9681/9681 [==============================] - 1s 119us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n",
      "Epoch 100/100\n",
      "9681/9681 [==============================] - 1s 138us/sample - loss: 2.0547 - binary_accuracy: 0.8667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-254272700.0, -254272740.0, -254272750.0, -254272770.0, -254272780.0, -254272800.0, -254272830.0, -254272900.0, -254272930.0, -254272980.0, -254273000.0, -254273010.0, -254273020.0, -254273040.0, -254273060.0, -254273070.0, -254273090.0, -254273100.0, -254273120.0, -254273140.0, -254273150.0, -254273170.0, -254273180.0, -254273200.0, -254273220.0, -254273230.0, -254273250.0, -254277650.0, -254277700.0, -254277710.0, -254277730.0, -254277740.0, -254277760.0, -254277780.0, -254277800.0, -254277810.0, -254277860.0, -254278370.0, -254278400.0, -254278420.0, -254278430.0, -254278450.0, -254278460.0, -254278480.0, -254278500.0, -254278510.0, -254278530.0, -254278540.0, -254278560.0, -254278580.0, -254278600.0, -254278610.0, -254278620.0, -254278640.0, -254278660.0, -254278670.0, -254278690.0, -254278700.0, -254278720.0, -254278740.0, -254278750.0, -254278780.0, -254279170.0, -254279220.0, -254279230.0, -254279250.0, -254279260.0, -254279280.0, -254279310.0, -254279330.0, -254280060.0, -254280100.0, -254280130.0, -254280140.0, -254280160.0, -254280180.0, -254280200.0, -254280210.0, -254280220.0, -254280240.0, -254280260.0, -254280270.0, -254280290.0, -254280300.0, -254280320.0, -254280340.0, -254280350.0, -254280370.0, -254280380.0, -254280400.0, -254280420.0, -254280430.0, -254280450.0, -254280460.0, -254280480.0, -254280500.0, -254280510.0, -254280540.0, -254285400.0, -254285490.0, -254285500.0, -254285520.0, -254285540.0, -254285550.0, -254285570.0, -254285580.0, -254285600.0, -254285620.0, -254285630.0, -254285650.0, -254285660.0, -254285680.0, -254285700.0, -254285710.0, -254285730.0, -254285740.0, -254285760.0, -254285780.0, -254285800.0, -254285810.0, -254285820.0, 228122720.0, 228122910.0, 228123060.0, 228123120.0, 228123180.0, 228123230.0, 228123280.0, 228123580.0, 228123600.0, 228123630.0, 228123650.0, 228123680.0, 228123820.0, 228123890.0, 228123900.0, 228123920.0, 228123940.0, 228123950.0, 228123970.0, 228123980.0, 228124000.0, 228124020.0, 228124030.0, 228124050.0, 228124060.0, 228124080.0, 228124100.0, 228124110.0, 228124130.0, 228124140.0, 228124160.0, 228124180.0, 228124200.0, 228124210.0, 228124220.0, 228124240.0, 228124260.0, 228124270.0, 228124290.0, 228124300.0, 228124320.0, 228124370.0, 228124420.0, 228124450.0, 228124480.0, 228124500.0, 228124530.0, 228124540.0, 228124560.0, 228124580.0, 228124590.0, 228124600.0, 228124640.0, 228124660.0, 228124670.0, 228124690.0, 228125120.0, 228125170.0, 228125200.0, 228125220.0, 228125230.0, 228125300.0, 228125310.0, 228125330.0, 228125340.0, 228125380.0, 228125390.0, 228125400.0, 228125420.0, 228125440.0, 228125460.0, 228125490.0, 228125500.0, 228125520.0, 228125540.0, 228125570.0, 228125580.0, 228125600.0, 228125630.0, 228125660.0, 228125760.0, 228125780.0, 228125800.0, 228125810.0, 228125840.0, 228125860.0, 228125870.0, 228125890.0, 228125900.0, 228125920.0, 228125940.0, 228125950.0, 228125970.0, 228125980.0, 228126000.0, 228126020.0, 228126030.0, 228126050.0, 228126060.0, 228126080.0, 228126100.0, 228126110.0, 228126130.0, 228126140.0, 228126160.0, 228126180.0, 228126190.0, 228126200.0, 228126220.0, 228126240.0, 228126260.0, 228126270.0, 228126290.0, 228126300.0, 228126320.0, 228126340.0, 228126350.0, 228126370.0, 228126380.0, 228126400.0, 228126420.0, 228126430.0, 228126450.0, 228126460.0, 228126480.0, 228126500.0, 228126510.0, 228126530.0, 228126540.0, 228126560.0, 228126580.0, 228126600.0, 228126610.0, 228126620.0, 228126640.0, 228126660.0, 228126670.0, 228126690.0, 228126700.0, 228126720.0, 228126740.0, 228126750.0, 228126770.0, 228126780.0, 228126800.0, 228126820.0, 228126830.0, 228126850.0, 228126860.0, 228126880.0, 228126900.0, 228126910.0, 228126930.0, 228126940.0, 228126960.0, 228126980.0, 228126990.0, 228127000.0, 228127020.0, 228127040.0, 228127070.0, 228127090.0, 228127100.0, 228127120.0, 228127140.0, 228127170.0, 228127200.0, 228127230.0, 228127280.0, 228127300.0, 228127340.0, 228127360.0, 228127410.0, 228127420.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7834710743801653\n",
      "Hamming Loss: 0.10826446280991736\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88      1896\n",
      "           1       0.00      0.00      0.00       524\n",
      "\n",
      "    accuracy                           0.78      2420\n",
      "   macro avg       0.39      0.50      0.44      2420\n",
      "weighted avg       0.61      0.78      0.69      2420\n",
      "\n",
      "Train on 9681 samples\n",
      "Epoch 1/100\n",
      "9681/9681 [==============================] - 2s 247us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 2/100\n",
      "9681/9681 [==============================] - 1s 115us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 3/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 4/100\n",
      "9681/9681 [==============================] - 1s 115us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 5/100\n",
      "9681/9681 [==============================] - 1s 117us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 6/100\n",
      "9681/9681 [==============================] - 1s 136us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 7/100\n",
      "9681/9681 [==============================] - 1s 150us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 8/100\n",
      "9681/9681 [==============================] - 1s 115us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 9/100\n",
      "9681/9681 [==============================] - 1s 127us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 10/100\n",
      "9681/9681 [==============================] - 1s 114us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 11/100\n",
      "9681/9681 [==============================] - 1s 139us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 12/100\n",
      "9681/9681 [==============================] - 1s 127us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 13/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 14/100\n",
      "9681/9681 [==============================] - 1s 144us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 15/100\n",
      "9681/9681 [==============================] - 1s 123us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 16/100\n",
      "9681/9681 [==============================] - 1s 129us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 17/100\n",
      "9681/9681 [==============================] - 1s 137us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 18/100\n",
      "9681/9681 [==============================] - 1s 149us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 19/100\n",
      "9681/9681 [==============================] - 1s 143us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 20/100\n",
      "9681/9681 [==============================] - 2s 201us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 21/100\n",
      "9681/9681 [==============================] - 2s 172us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 22/100\n",
      "9681/9681 [==============================] - 2s 156us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 23/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 24/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 25/100\n",
      "9681/9681 [==============================] - 2s 164us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 26/100\n",
      "9681/9681 [==============================] - 2s 161us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 27/100\n",
      "9681/9681 [==============================] - 2s 162us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 28/100\n",
      "9681/9681 [==============================] - 2s 161us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 29/100\n",
      "9681/9681 [==============================] - 2s 160us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 30/100\n",
      "9681/9681 [==============================] - 2s 168us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 31/100\n",
      "9681/9681 [==============================] - 2s 165us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 32/100\n",
      "9681/9681 [==============================] - 2s 156us/sample - loss: 2.0086 - binary_accuracy: 0.8697s - loss: 2.0408\n",
      "Epoch 33/100\n",
      "9681/9681 [==============================] - 2s 156us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 34/100\n",
      "9681/9681 [==============================] - 1s 143us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 35/100\n",
      "9681/9681 [==============================] - 1s 127us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 36/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 37/100\n",
      "9681/9681 [==============================] - 1s 143us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 38/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 39/100\n",
      "9681/9681 [==============================] - 1s 139us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 40/100\n",
      "9681/9681 [==============================] - 1s 151us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 41/100\n",
      "9681/9681 [==============================] - 1s 141us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 42/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 43/100\n",
      "9681/9681 [==============================] - 1s 148us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 44/100\n",
      "9681/9681 [==============================] - 1s 149us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 45/100\n",
      "9681/9681 [==============================] - 2s 155us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 46/100\n",
      "9681/9681 [==============================] - 1s 150us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 47/100\n",
      "9681/9681 [==============================] - 1s 154us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 48/100\n",
      "9681/9681 [==============================] - 1s 154us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 49/100\n",
      "9681/9681 [==============================] - 1s 150us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 50/100\n",
      "9681/9681 [==============================] - 1s 143us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 51/100\n",
      "9681/9681 [==============================] - 1s 141us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 52/100\n",
      "9681/9681 [==============================] - 1s 150us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 53/100\n",
      "9681/9681 [==============================] - 1s 153us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 54/100\n",
      "9681/9681 [==============================] - 1s 132us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 55/100\n",
      "9681/9681 [==============================] - 1s 101us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 56/100\n",
      "9681/9681 [==============================] - 1s 118us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 57/100\n",
      "9681/9681 [==============================] - 1s 128us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 58/100\n",
      "9681/9681 [==============================] - 1s 136us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 59/100\n",
      "9681/9681 [==============================] - 1s 134us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 60/100\n",
      "9681/9681 [==============================] - 1s 121us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 61/100\n",
      "9681/9681 [==============================] - 1s 113us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 62/100\n",
      "9681/9681 [==============================] - 1s 129us/sample - loss: 2.0086 - binary_accuracy: 0.8697s - loss: 2.0386 - binary_accurac\n",
      "Epoch 63/100\n",
      "9681/9681 [==============================] - 1s 112us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 64/100\n",
      "9681/9681 [==============================] - 1s 108us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 65/100\n",
      "9681/9681 [==============================] - 1s 104us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 66/100\n",
      "9681/9681 [==============================] - 1s 118us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 67/100\n",
      "9681/9681 [==============================] - 1s 118us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 68/100\n",
      "9681/9681 [==============================] - 1s 133us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 69/100\n",
      "9681/9681 [==============================] - 1s 110us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 70/100\n",
      "9681/9681 [==============================] - 1s 113us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 71/100\n",
      "9681/9681 [==============================] - 1s 112us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 72/100\n",
      "9681/9681 [==============================] - 1s 122us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 73/100\n",
      "9681/9681 [==============================] - 1s 106us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 74/100\n",
      "9681/9681 [==============================] - 1s 107us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 75/100\n",
      "9681/9681 [==============================] - 1s 107us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 76/100\n",
      "9681/9681 [==============================] - 1s 111us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 77/100\n",
      "9681/9681 [==============================] - 1s 108us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 78/100\n",
      "9681/9681 [==============================] - 1s 106us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 79/100\n",
      "9681/9681 [==============================] - 1s 106us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 80/100\n",
      "9681/9681 [==============================] - 1s 106us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 81/100\n",
      "9681/9681 [==============================] - 1s 106us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 82/100\n",
      "9681/9681 [==============================] - 1s 120us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 83/100\n",
      "9681/9681 [==============================] - 1s 107us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 84/100\n",
      "9681/9681 [==============================] - 1s 122us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 85/100\n",
      "9681/9681 [==============================] - 1s 107us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 86/100\n",
      "9681/9681 [==============================] - 1s 109us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 87/100\n",
      "9681/9681 [==============================] - 1s 108us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 88/100\n",
      "9681/9681 [==============================] - 1s 101us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 89/100\n",
      "9681/9681 [==============================] - 1s 103us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 90/100\n",
      "9681/9681 [==============================] - 1s 101us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 91/100\n",
      "9681/9681 [==============================] - 1s 99us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 92/100\n",
      "9681/9681 [==============================] - 1s 99us/sample - loss: 2.0086 - binary_accuracy: 0.86970s - loss: 2.0838 - binar\n",
      "Epoch 93/100\n",
      "9681/9681 [==============================] - 1s 100us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 94/100\n",
      "9681/9681 [==============================] - 1s 116us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 95/100\n",
      "9681/9681 [==============================] - 1s 108us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 96/100\n",
      "9681/9681 [==============================] - 1s 103us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 97/100\n",
      "9681/9681 [==============================] - 1s 124us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 98/100\n",
      "9681/9681 [==============================] - 1s 116us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 99/100\n",
      "9681/9681 [==============================] - 1s 132us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n",
      "Epoch 100/100\n",
      "9681/9681 [==============================] - 1s 107us/sample - loss: 2.0086 - binary_accuracy: 0.8697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-55963990.0, -55964028.0, -55964388.0, -55964390.0, -55964400.0, -55964404.0, -55964410.0, -55964412.0, -55964416.0, -55964420.0, -55964424.0, -55964428.0, -55964430.0, -55964436.0, -55964440.0, -55964444.0, -55964450.0, -55964452.0, -55964456.0, -55964460.0, -55964464.0, -55964468.0, -55964470.0, -55964476.0, -55964480.0, -55964484.0, -55964490.0, -55964492.0, -55964496.0, -55966490.0, -55966496.0, -55966500.0, -55966510.0, -55966524.0, -55966530.0, -55966544.0, -55966548.0, -55966556.0, -55966564.0, -55966572.0, -55966596.0, -55966610.0, -55966670.0, -55966880.0, -55966884.0, -55966892.0, -55966904.0, -55966908.0, -55966910.0, -55966916.0, -55966920.0, -55966924.0, -55966930.0, -55966932.0, -55966936.0, -55966940.0, -55966944.0, -55966948.0, -55966950.0, -55966956.0, -55966960.0, -55966964.0, -55966970.0, -55966972.0, -55966976.0, -55966980.0, -55966984.0, -55966988.0, -55966990.0, -55966996.0, -55967000.0, -55967004.0, -55967010.0, -55967012.0, -55967016.0, -55967020.0, -55967024.0, -55967028.0, -55967030.0, -55967036.0, -55967040.0, -55967044.0, -55967050.0, -55967052.0, -55967056.0, -55967060.0, -55967064.0, -55967068.0, -55967070.0, -55967080.0, -55967084.0, -55967090.0, -55967092.0, -55967096.0, -55967100.0, -55967104.0, -55967108.0, -55967110.0, -55967116.0, -55967120.0, -55967124.0, -55967130.0, -55967132.0, -55967136.0, -55967140.0, -55967144.0, -55967148.0, -55967150.0, -55967156.0, -55967160.0, -55967164.0, -55967170.0, -55967172.0, -55967176.0, -55967180.0, -55967184.0, -55967188.0, -55967190.0, -55967196.0, -55967200.0, -55967204.0, -55967210.0, -55967212.0, -55967216.0, -55967224.0, -55967228.0, -55967230.0, -55967236.0, -55967240.0, -55967244.0, -55967252.0, -55967256.0, -55967260.0, -55967264.0, -55967268.0, -55967270.0, -55967280.0, -55967284.0, -55967292.0, -55970100.0, -55970156.0, -55970160.0, -55970270.0, -55970304.0, -55970330.0, -55970390.0, -55970404.0, -55970416.0, -55970450.0, -55970476.0, -55970480.0, -55970500.0, -55970510.0, -55970520.0, -55970532.0, -55970540.0, -55970544.0, -55970548.0, -55970550.0, -55970556.0, -55970560.0, -55970564.0, -55970572.0, -55970580.0, -55970584.0, -55970588.0, -55970590.0, -55970596.0, -55970600.0, -55970604.0, -55970610.0, -55970612.0, -55970616.0, -55970620.0, -55970624.0, -55970628.0, -55970630.0, -55970636.0, -55970640.0, -55970644.0, -55970650.0, -55970652.0, -55970656.0, -55970660.0, -55970664.0, -55970668.0, -55970670.0, -55970676.0, -55970680.0, -55970684.0, -55970690.0, -55970692.0, -55970696.0, -55970700.0, -55970704.0, -55970708.0, -55970716.0, -55970720.0, -55970724.0, -55970730.0, -55970732.0, -55970736.0, -55970740.0, -55970744.0, -55970748.0, -55970750.0, -55970756.0, -55970760.0, -55970764.0, -55970770.0, -55970772.0, -55970776.0, -55970780.0, -55970784.0, -55970788.0, -55970790.0, -55970796.0, -55970800.0, -55970804.0, -55970810.0, -55970812.0, -55970816.0, -55970820.0, -55970824.0, -55970828.0, -55970830.0, -55970836.0, -55970840.0, -55970844.0, -55970850.0, -55970852.0, -55970856.0, -55970860.0, -55970868.0, -55970870.0, -55970876.0, -55970880.0, -55970884.0, -55970890.0, -55970892.0, -55970896.0, -55970900.0, -55970904.0, -55970908.0, -55970910.0, -55970916.0, -55970920.0, -55970924.0, -55970930.0, -55970932.0, -55970936.0, -55970940.0, -55970944.0, -55970948.0, -55970950.0, -55970956.0, -55970960.0, -55970970.0, -55970972.0, -55970976.0, -55970980.0, -55970984.0, -55970988.0, -55970990.0, -55970996.0, -55971000.0, -55971004.0, -55971010.0, -55971012.0, -55971016.0, -55971020.0, -55971024.0, -55971028.0, -55971030.0, -55971036.0, -55971040.0, -55971044.0, -55971050.0, -55971052.0, -55971056.0, -55971060.0, -55971064.0, -55971068.0, -55971070.0, -55971076.0, -55971080.0, -55971084.0, -55971090.0, -55971092.0, -55971096.0, -55971100.0, -55971104.0, -55971108.0, -55971110.0, -55971116.0, -55971120.0, -55971124.0, -55971130.0, -55971132.0, -55971136.0, -55971140.0, -55971144.0, -55971148.0, -55971150.0, -55971156.0, -55971160.0, -55971164.0, -55971170.0, -55971172.0, -55971176.0, -55971180.0, -55971184.0, -55971188.0, -55971190.0, -55971196.0, -55971200.0, -55971204.0, -55971210.0, -55971212.0, -55971216.0, -55971220.0, -55971224.0, -55971228.0, -55971230.0, -55971236.0, -55971240.0, -55971244.0, -55971250.0, -55971252.0, -55971256.0, -55971260.0, -55971264.0, -55971268.0, -55971270.0, -55971276.0, -55971280.0, -55971284.0, -55971290.0, -55971292.0, -55971296.0, -55971300.0, -55971304.0, -55971308.0, -55971310.0, -55971316.0, -55971320.0, -55971324.0, -55971330.0, -55971332.0, -55971336.0, -55971340.0, -55971344.0, -55971348.0, -55971350.0, -55971356.0, -55971360.0, -55971364.0, -55971370.0, -55971372.0, -55971376.0, -55971380.0, -55971384.0, -55971388.0, -55971390.0, -55971396.0, -55971400.0, -55971404.0, -55971410.0, -55971412.0, -55971416.0, -55971424.0, -55971428.0, -55971430.0, -55971436.0, -55971440.0, -55971444.0, -55971456.0, -55971476.0, -55971636.0, -55971670.0, -55971812.0, -55971816.0, -55971836.0, -55971876.0, -55971970.0, -55972070.0, -55972096.0, -55972104.0, -55972140.0, -55972148.0, -55972160.0, -55972164.0, -55972176.0, -55972180.0, -55972184.0, -55972188.0, -55972190.0, -55972196.0, -55972200.0, -55972204.0, -55972210.0, -55972212.0, -55972216.0, -55972220.0, -55972224.0, -55972228.0, -55972230.0, -55972236.0, -55972240.0, -55972244.0, -55972250.0, -55972252.0, -55972256.0, -55972260.0, -55972264.0, -55972268.0, -55972270.0, -55972280.0, -55972284.0, -55972290.0, -55972292.0, -55972296.0, -55972300.0, -55972304.0, -55972308.0, -55972310.0, -55972316.0, -55972320.0, -55972324.0, -55972330.0, -55972332.0, -55972336.0, -55972340.0, -55972344.0, -55972348.0, -55972350.0, -55972356.0, -55972360.0, -55972364.0, -55972370.0, -55972372.0, -55972376.0, -55972380.0, -55972384.0, -55972388.0, -55972390.0, -55972396.0, -55972400.0, -55972404.0, -55972410.0, -55972412.0, -55972416.0, -55972420.0, -55972424.0, -55972428.0, -55972430.0, -55972436.0, -55972440.0, -55972444.0, -55972450.0, -55972452.0, -55972456.0, -55972460.0, -55972464.0, -55972468.0, -55972470.0, -55972476.0, -55972480.0, -55972484.0, -55972490.0, -55972492.0, -55972496.0, -55972500.0, -55972530.0, -55977876.0, -55977920.0, -55977960.0, -55978050.0, -55978060.0, -55978068.0, -55978092.0, -55978100.0, -55978116.0, -55978120.0, -55978130.0, -55978148.0, -55978160.0, -55978176.0, -55978184.0, -55978188.0, -55978190.0, -55978200.0, -55978210.0, -55978212.0, -55978216.0, -55978224.0, -55978228.0, -55978230.0, -55978236.0, -55978240.0, -55978244.0, -55978250.0, -55978252.0, -55978256.0, -55978260.0, -55978264.0, -55978268.0, -55978270.0, -55978276.0, -55978280.0, -55978284.0, -55978290.0, -55978296.0, -55978300.0, -55978304.0, -55978308.0, -55978310.0, -55978320.0, -55978324.0, -55978330.0, -55978332.0, -55978336.0, -55978340.0, -55978344.0, -55978348.0, -55978350.0, -55978360.0, -55978364.0, -55978370.0, -55978372.0, -55978376.0, -55978380.0, -55978384.0, -55978388.0, -55978390.0, -55978396.0, -55978400.0, -55978404.0, -55978410.0, -55978412.0, -55978416.0, -55978420.0, -55978424.0, -55978428.0, -55978430.0, -55978436.0, -55978440.0, -55978444.0, -55978450.0, -55978452.0, -55978456.0, -55978460.0, -55978464.0, -55978468.0, -55978470.0, -55978476.0, -55978480.0, -55978484.0, -55978490.0, -55978492.0, -55978496.0, -55978500.0, -55978504.0, -55978508.0, -55978510.0, -55978516.0, -55978520.0, -55978524.0, -55978530.0, -55978532.0, -55978540.0, -55978544.0, -55978548.0, -55978550.0, -55978556.0, -55978560.0, -55978564.0, -55978570.0, -55978572.0, -55978584.0, -55978612.0, 61641470.0, 61641490.0, 61641496.0, 61641504.0, 61641510.0, 61641520.0, 61641530.0, 61641536.0, 61641544.0, 61641550.0, 61641560.0, 61641570.0, 61641576.0, 61641584.0, 61641590.0, 61641600.0, 61641610.0, 61641616.0, 61641624.0, 61641630.0, 61641640.0, 61641650.0, 61641656.0, 61641664.0, 61641670.0, 61641680.0, 61641690.0, 61641696.0, 61641704.0, 61641710.0, 61641720.0, 61641730.0, 61641736.0, 61641744.0, 61641750.0, 61641760.0, 61641770.0, 61641776.0, 61641784.0, 61641790.0, 61641800.0, 61641816.0, 61641824.0, 61641850.0, 61641856.0, 61641864.0, 61641910.0, 61641970.0, 61642944.0, 61642950.0, 61642960.0, 61642970.0, 61642976.0, 61642984.0, 61642990.0, 61643000.0, 61643010.0, 61643016.0, 61643024.0, 61643030.0, 61643040.0, 61643050.0, 61643056.0, 61643064.0, 61643070.0, 61643080.0, 61643090.0, 61643096.0, 61643104.0, 61643110.0, 61643120.0, 61643130.0, 61643136.0, 61643144.0, 61643150.0, 61643160.0, 61643170.0, 61643176.0, 61643184.0, 61643190.0, 61643200.0, 61643210.0, 61643216.0, 61643224.0, 61643240.0, 61643250.0, 61643256.0, 61643264.0, 61643270.0, 61643290.0, 61643296.0, 61643304.0, 61643320.0, 61643330.0, 61643344.0, 61643360.0, 61643370.0, 61643376.0, 61643384.0, 61643390.0, 61643400.0, 61643410.0, 61643416.0, 61643424.0, 61643430.0, 61643440.0, 61643450.0, 61643456.0, 61643464.0, 61643470.0, 61643480.0, 61643490.0, 61643496.0, 61643504.0, 61643510.0, 61643520.0, 61643530.0, 61643536.0, 61643544.0, 61643550.0, 61643560.0, 61643570.0, 61643576.0, 61643584.0, 61643590.0, 61643600.0, 61643610.0, 61643616.0, 61643624.0, 61643630.0, 61643640.0, 61643650.0, 61643656.0, 61643664.0, 61643670.0, 61643680.0, 61643690.0, 61643696.0, 61643704.0, 61643710.0, 61643720.0, 61643730.0, 61643736.0, 61643744.0, 61643750.0, 61643760.0, 61643770.0, 61643776.0, 61643784.0, 61643790.0, 61643800.0, 61643810.0, 61643816.0, 61643824.0, 61643830.0, 61643840.0, 61643850.0, 61643856.0, 61643864.0, 61643870.0, 61643880.0, 61643890.0, 61643896.0, 61643904.0, 61643910.0, 61643920.0, 61643930.0, 61643936.0, 61643944.0, 61643950.0, 61643960.0, 61643976.0, 61643984.0, 61643990.0, 61644000.0, 61644010.0, 61644016.0, 61644024.0, 61644030.0, 61644040.0, 61644050.0, 61644096.0, 61644110.0, 61644120.0, 61644136.0, 61644144.0, 61644150.0, 61644160.0, 61644170.0, 61644176.0, 61644224.0, 61644776.0, 61644784.0, 61644790.0, 61644800.0, 61644810.0, 61644816.0, 61644824.0, 61644830.0, 61644840.0, 61644850.0, 61644856.0, 61644864.0, 61644870.0, 61644880.0, 61644890.0, 61644896.0, 61644904.0, 61644910.0, 61644920.0, 61644930.0, 61644936.0, 61644944.0, 61644950.0, 61644960.0, 61644970.0, 61644976.0, 61644984.0, 61644990.0, 61645000.0, 61645010.0, 61645016.0, 61645024.0, 61645030.0, 61645040.0, 61645050.0, 61645064.0, 61645070.0, 61645080.0, 61645096.0, 61645110.0, 61645120.0, 61645136.0, 61645150.0, 61645170.0, 61645176.0, 61645184.0, 61645190.0, 61645200.0, 61645210.0, 61645216.0, 61645230.0, 61645310.0, 61645360.0, 61645370.0, 61645680.0, 61645690.0, 61645696.0, 61645704.0, 61645710.0, 61645720.0, 61645730.0, 61645736.0, 61645744.0, 61645750.0, 61645760.0, 61645770.0, 61645776.0, 61645784.0, 61645790.0, 61645810.0, 61645904.0, 61645936.0, 61646050.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7677685950413223\n",
      "Hamming Loss: 0.12024793388429753\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89      1950\n",
      "           1       0.00      0.00      0.00       470\n",
      "\n",
      "    accuracy                           0.81      2420\n",
      "   macro avg       0.40      0.50      0.45      2420\n",
      "weighted avg       0.65      0.81      0.72      2420\n",
      "\n",
      "y_pred shape: \n",
      "(2420, 2)\n",
      "y_pred2 shape:\n",
      "(2420, 2)\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       5.0\n",
      "         1.0       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       5.0\n",
      "   macro avg       0.00      0.00      0.00       5.0\n",
      "weighted avg       0.00      0.00      0.00       5.0\n",
      "\n",
      "Hamming Loss: 0.12825509232234478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "y_pred3, pred3, y_test3, hl3 = list(),list(),list(),list()\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(1)\n",
    "  #]) \n",
    "    \n",
    "  #inputs = tf.keras.layers.Input(shape=(X_train.shape))\n",
    "  #outputs = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "  #model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "\n",
    "  #outputs = keras.layers.Dense(tf.keras.layers.Dense(number_of_classes)) #(x) \n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(2)\n",
    "  #])   \n",
    "  \n",
    "  #model = tf.contrib.learn.DNNClassifier(hidden_units=[5,10,5],\n",
    "  #                                          n_classes=4)    \n",
    "    \n",
    "\n",
    "  inputs = tf.keras.Input(shape=(X_train.shape))\n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    \n",
    "  outputs = tf.keras.layers.Dense(number_of_classes)  #, activation=activation)\n",
    "                             #kernel_initializer=initializer,\n",
    "                             #activation=activation) #(x) \n",
    "  #model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(2)\n",
    "  ]) \n",
    "    \n",
    "  #model.add(layers.Activation(activations.relu))\n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  #model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.001))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  #print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "    \n",
    "  #y_pred2.append(y_pred)\n",
    "  y_pred3 = np.append(y_pred3, y_pred2)\n",
    "  #pred2.append(pred)\n",
    "  pred3 = np.append(pred3, pred2)\n",
    "  #y_test2.append(y_test)\n",
    "  y_test3 = np.append(y_test3, y_test2)\n",
    "  #hl2.append(hl)\n",
    "  hl3 = np.append(hl3, hl)\n",
    "    \n",
    "print(\"y_pred shape: \")\n",
    "print(np.shape(y_pred))\n",
    "\n",
    "print(\"y_pred2 shape:\")\n",
    "print(np.shape(y_pred2))\n",
    "\n",
    "y_pred3 = np.concatenate((y_pred3[0], y_pred3[1], y_pred3[2], y_pred3[3], y_pred3[4]), axis=None)\n",
    "pred3 = np.concatenate((pred3[0], pred3[1], pred3[2], pred3[3], pred3[4]), axis=None)\n",
    "y_test3 = np.concatenate((y_test3[0], y_test3[1], y_test3[2], y_test3[3], y_test3[4]), axis=None)\n",
    "hl3 = np.concatenate((hl3[0], hl3[1], hl3[2], hl3[3], hl3[4]), axis=None)\n",
    "\n",
    "print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\")\n",
    "#print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "print(classification_report(y_test3, pred3))\n",
    "hl3_avg = sum(hl3) / len(hl3)\n",
    "print(\"Hamming Loss: \" + str(hl3_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195472, 7)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_KGFS5.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 156377 samples\n",
      "Epoch 1/100\n",
      "156377/156377 [==============================] - 42s 266us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 2/100\n",
      "156377/156377 [==============================] - 40s 258us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 3/100\n",
      "156377/156377 [==============================] - 49s 316us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 4/100\n",
      "156377/156377 [==============================] - 47s 302us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 5/100\n",
      "156377/156377 [==============================] - 50s 320us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 6/100\n",
      "156377/156377 [==============================] - 47s 303us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 7/100\n",
      "156377/156377 [==============================] - 47s 300us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 8/100\n",
      "156377/156377 [==============================] - 52s 335us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 9/100\n",
      "156377/156377 [==============================] - 52s 333us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 10/100\n",
      "156377/156377 [==============================] - 56s 356us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 11/100\n",
      "156377/156377 [==============================] - 53s 342us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 12/100\n",
      "156377/156377 [==============================] - 38s 243us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9859 - b\n",
      "Epoch 13/100\n",
      "156377/156377 [==============================] - 40s 254us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 14/100\n",
      "156377/156377 [==============================] - 39s 251us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9839 - binary_accuracy: 0.\n",
      "Epoch 15/100\n",
      "156377/156377 [==============================] - 37s 237us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 16/100\n",
      "156377/156377 [==============================] - 39s 252us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 17/100\n",
      "156377/156377 [==============================] - 37s 239us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 18/100\n",
      "156377/156377 [==============================] - 39s 251us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 19/100\n",
      "156377/156377 [==============================] - 40s 254us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 20/100\n",
      "156377/156377 [==============================] - 41s 261us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 21/100\n",
      "156377/156377 [==============================] - 40s 257us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 22/100\n",
      "156377/156377 [==============================] - 36s 231us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9843 - binary_accur - ETA: 0s - loss: 8.9848 - binary_accurac\n",
      "Epoch 23/100\n",
      "156377/156377 [==============================] - 37s 237us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 24/100\n",
      "156377/156377 [==============================] - 33s 211us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 25/100\n",
      "156377/156377 [==============================] - 34s 218us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - lo\n",
      "Epoch 26/100\n",
      "156377/156377 [==============================] - 29s 189us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9844 - binary_accuracy\n",
      "Epoch 27/100\n",
      "156377/156377 [==============================] - 31s 196us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 28/100\n",
      "156377/156377 [==============================] - 34s 220us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 29/100\n",
      "156377/156377 [==============================] - 34s 219us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 30/100\n",
      "156377/156377 [==============================] - 36s 227us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 31/100\n",
      "156377/156377 [==============================] - 36s 233us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 32/100\n",
      "156377/156377 [==============================] - 37s 238us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 33/100\n",
      "156377/156377 [==============================] - 38s 243us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 34/100\n",
      "156377/156377 [==============================] - 27s 170us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss:\n",
      "Epoch 35/100\n",
      "156377/156377 [==============================] - 29s 184us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 36/100\n",
      "156377/156377 [==============================] - 25s 158us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9844 - binary_accuracy: 0.417\n",
      "Epoch 37/100\n",
      "156377/156377 [==============================] - 22s 138us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9838 - binary_accuracy: 0.4\n",
      "Epoch 38/100\n",
      "156377/156377 [==============================] - 22s 139us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 39/100\n",
      "156377/156377 [==============================] - 22s 138us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9846 - binary_accuracy\n",
      "Epoch 40/100\n",
      "156377/156377 [==============================] - 22s 139us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 41/100\n",
      "156377/156377 [==============================] - 22s 139us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9849 - binary_accuracy:\n",
      "Epoch 42/100\n",
      "156377/156377 [==============================] - 22s 140us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 43/100\n",
      "156377/156377 [==============================] - 22s 141us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 44/100\n",
      "156377/156377 [==============================] - 21s 137us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9847 - binary_accuracy: 0\n",
      "Epoch 45/100\n",
      "156377/156377 [==============================] - 22s 139us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 46/100\n",
      "156377/156377 [==============================] - 27s 172us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 47/100\n",
      "156377/156377 [==============================] - 25s 161us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 48/100\n",
      "156377/156377 [==============================] - 23s 150us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 49/100\n",
      "156377/156377 [==============================] - 23s 150us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 50/100\n",
      "156377/156377 [==============================] - 23s 149us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 51/100\n",
      "156377/156377 [==============================] - 24s 154us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 52/100\n",
      "156377/156377 [==============================] - 30s 191us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 53/100\n",
      "156377/156377 [==============================] - 28s 182us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9851 - binary_accur\n",
      "Epoch 54/100\n",
      "156377/156377 [==============================] - 26s 168us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 55/100\n",
      "156377/156377 [==============================] - 23s 145us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 56/100\n",
      "156377/156377 [==============================] - 24s 153us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 57/100\n",
      "156377/156377 [==============================] - 27s 173us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 58/100\n",
      "156377/156377 [==============================] - 31s 195us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 59/100\n",
      "156377/156377 [==============================] - 30s 193us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 60/100\n",
      "156377/156377 [==============================] - 22s 142us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 61/100\n",
      "156377/156377 [==============================] - 24s 157us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 62/100\n",
      "156377/156377 [==============================] - 23s 149us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - los\n",
      "Epoch 63/100\n",
      "156377/156377 [==============================] - 22s 140us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 64/100\n",
      "156377/156377 [==============================] - 27s 172us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 65/100\n",
      "156377/156377 [==============================] - 22s 140us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 66/100\n",
      "156377/156377 [==============================] - 23s 148us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9846 - binary_accuracy: 0.417\n",
      "Epoch 67/100\n",
      "156377/156377 [==============================] - 24s 155us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 68/100\n",
      "156377/156377 [==============================] - 24s 154us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 69/100\n",
      "156377/156377 [==============================] - 22s 143us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 70/100\n",
      "156377/156377 [==============================] - 22s 138us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 71/100\n",
      "156377/156377 [==============================] - 26s 165us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 72/100\n",
      "156377/156377 [==============================] - 27s 174us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9844 - binary_ac\n",
      "Epoch 73/100\n",
      "156377/156377 [==============================] - 26s 164us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 74/100\n",
      "156377/156377 [==============================] - 22s 141us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - los\n",
      "Epoch 75/100\n",
      "156377/156377 [==============================] - 26s 167us/sample - loss: 8.9847 - binary_accuracy: 0.4175 - loss: 8.9843 - b\n",
      "Epoch 76/100\n",
      "156377/156377 [==============================] - 25s 163us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 77/100\n",
      "156377/156377 [==============================] - 31s 196us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 78/100\n",
      "156377/156377 [==============================] - 34s 216us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 79/100\n",
      "156377/156377 [==============================] - 20s 131us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 80/100\n",
      "156377/156377 [==============================] - 23s 145us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 81/100\n",
      "156377/156377 [==============================] - 20s 127us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 82/100\n",
      "156377/156377 [==============================] - 24s 151us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 83/100\n",
      "156377/156377 [==============================] - 19s 120us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 84/100\n",
      "156377/156377 [==============================] - 19s 121us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 85/100\n",
      "156377/156377 [==============================] - 20s 129us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 86/100\n",
      "156377/156377 [==============================] - 21s 135us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 87/100\n",
      "156377/156377 [==============================] - 19s 124us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 88/100\n",
      "156377/156377 [==============================] - 17s 112us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 89/100\n",
      "156377/156377 [==============================] - 21s 131us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 90/100\n",
      "156377/156377 [==============================] - 21s 134us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 91/100\n",
      "156377/156377 [==============================] - 18s 113us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 92/100\n",
      "156377/156377 [==============================] - 20s 129us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 93/100\n",
      "156377/156377 [==============================] - 21s 133us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 94/100\n",
      "156377/156377 [==============================] - 18s 112us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 95/100\n",
      "156377/156377 [==============================] - 21s 132us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 96/100\n",
      "156377/156377 [==============================] - 21s 133us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 97/100\n",
      "156377/156377 [==============================] - 18s 114us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 98/100\n",
      "156377/156377 [==============================] - 19s 118us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 99/100\n",
      "156377/156377 [==============================] - 19s 122us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n",
      "Epoch 100/100\n",
      "156377/156377 [==============================] - 17s 112us/sample - loss: 8.9847 - binary_accuracy: 0.4175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-161178100.0, -161178110.0, -161178130.0, -161178140.0, -161178160.0, -161178180.0, -161178190.0, -161178200.0, -161178220.0, -161178240.0, -161178260.0, -161178270.0, -161178290.0, -161178300.0, -161178320.0, -161178340.0, -161178350.0, -161178370.0, -161178380.0, -161178400.0, -161178420.0, -161178430.0, -161178450.0, -161178460.0, -161178480.0, -161178500.0, -161178510.0, -161178530.0, -161178540.0, -161178560.0, -161178580.0, -161178600.0, -161178610.0, -161178620.0, -161178640.0, -161178660.0, -161178670.0, -161178690.0, -161178700.0, -161178720.0, -161178740.0, -161178750.0, -161178770.0, -161178780.0, -161178800.0, -161178820.0, -161178830.0, -161178850.0, -161178860.0, -161178880.0, -161178900.0, -161178910.0, -161178930.0, -161178940.0, -161178960.0, -161178980.0, -161178990.0, -161179000.0, -161179020.0, -161179040.0, -161179060.0, -161179070.0, -161179090.0, -161179100.0, -161179120.0, -161179140.0, -161179150.0, -161179170.0, -161179180.0, -161179200.0, -161179220.0, -161179230.0, -161179250.0, -161179260.0, -161179280.0, -161179300.0, -161179310.0, -161179330.0, -161179340.0, -161179360.0, -161179380.0, -161179680.0, -161179700.0, -161179710.0, -161179730.0, -161179740.0, -161179760.0, -161179780.0, -161179790.0, -161179800.0, -161179820.0, -161179840.0, -161179860.0, -161179870.0, -161179890.0, -161179900.0, -161179920.0, -161179940.0, -161179950.0, -161179970.0, -161179980.0, -161180000.0, -161180020.0, -161180030.0, -161180050.0, -161180060.0, -161180080.0, -161180100.0, -161180110.0, -161180130.0, -161180140.0, -161180160.0, -161180180.0, -161180200.0, -161180210.0, -161180220.0, -161180240.0, -161180260.0, -161180270.0, -161180290.0, -161180300.0, -161180320.0, -161180340.0, -161180350.0, -161180370.0, -161180380.0, -161180400.0, -161180420.0, -161180430.0, -161180450.0, -161180460.0, -161180480.0, -161180500.0, -161180510.0, -161180530.0, -161180540.0, -161180560.0, -161180580.0, -161180590.0, -161180600.0, -161180620.0, -161180640.0, -161180660.0, -161180670.0, -161180690.0, -161180700.0, -161180720.0, -161180740.0, -161180750.0, -161180770.0, -161180780.0, -161180800.0, -161180820.0, -161180830.0, -161180850.0, -161180860.0, -161180880.0, -161180900.0, -161180910.0, -161180930.0, -161180940.0, -161180980.0, -161181000.0, -161181010.0, -161181020.0, -161181040.0, -161181060.0, -161181070.0, -161181090.0, -161181100.0, -161181120.0, -161181140.0, -161181150.0, -161181170.0, -161181180.0, -161181200.0, -161181220.0, -161181230.0, -161181250.0, -161181260.0, -161181280.0, -161181300.0, -161181310.0, -161181330.0, -161181340.0, -161181360.0, -161181380.0, -161181390.0, -161181400.0, -161181420.0, -161181440.0, -161181460.0, -161181470.0, -161181490.0, -161181500.0, -161181520.0, -161181540.0, -161181550.0, -161181570.0, -161181580.0, -161181600.0, -161181620.0, -161181630.0, -161181650.0, -161181660.0, -161181680.0, -161181700.0, -161181710.0, -161181730.0, -161181740.0, -161181760.0, -161181780.0, -161181800.0, -161181810.0, -161181820.0, -161181840.0, -161181860.0, -161181870.0, -161181890.0, -161181900.0, -161181920.0, -161181940.0, -161181950.0, -161181970.0, -161181980.0, -161182000.0, -161182020.0, -161182030.0, -161182050.0, -161182060.0, -161182080.0, -72923300.0, -72923304.0, -72923310.0, -72923320.0, -72923330.0, -72923336.0, -72923340.0, -72923350.0, -72923360.0, -72923370.0, -72923380.0, -72923384.0, -72923390.0, -72923400.0, -72923410.0, -72923416.0, -72923420.0, -72923430.0, -72923440.0, -72923450.0, -72923460.0, -72923464.0, -72923470.0, -72923480.0, -72923490.0, -72923496.0, -72923500.0, -72923510.0, -72923520.0, -72923530.0, -72923540.0, -72923544.0, -72923550.0, -72923560.0, -72923570.0, -72923576.0, -72923580.0, -72923590.0, -72923600.0, -72923610.0, -72923620.0, -72923624.0, -72923630.0, -72923640.0, -72923650.0, -72923656.0, -72923660.0, -72923670.0, -72923680.0, -72923690.0, -72923700.0, -72923704.0, -72923710.0, -72923720.0, -72923730.0, -72923736.0, -72923740.0, -72923750.0, -72923760.0, -72923770.0, -72923780.0, -72923784.0, -72923790.0, -72923800.0, -72923810.0, -72923816.0, -72923820.0, -72923830.0, -72923840.0, -72923864.0, -72923870.0, -72923880.0, -72923890.0, -72923896.0, -72923900.0, -72923910.0, -72923920.0, -72923930.0, -72923940.0, -72923944.0, -72923950.0, -72923960.0, -72923970.0, -72923976.0, -72923980.0, -72923990.0, -72924000.0, -72924010.0, -72924020.0, -72924024.0, -72924030.0, -72924040.0, -72924050.0, -72924056.0, -72924060.0, -72924070.0, -72924080.0, -72924090.0, -72924100.0, -72924104.0, -72924110.0, -72924120.0, -72924130.0, -72924136.0, -72924140.0, -72924150.0, -72924160.0, -72924170.0, -72924180.0, -72924184.0, -72924190.0, -72924200.0, -72924210.0, -72924216.0, -72924220.0, -72924230.0, -72924240.0, -72924250.0, -72924260.0, -72924264.0, -72924270.0, -72924280.0, -72924290.0, -72924296.0, -72924300.0, -72924310.0, -72924320.0, -72924330.0, -72924340.0, -72924344.0, -72924350.0, -72924360.0, -72924370.0, -72924376.0, -72924380.0, -72924390.0, -72924400.0, -72924410.0, -72924420.0, -72924424.0, -72924430.0, -72924440.0, -72924450.0, -72924456.0, -72924460.0, -72924470.0, -72924480.0, -72924490.0, -72924500.0, -72924504.0, -72924510.0, -72924520.0, -72924530.0, -72924536.0, -72924540.0, -72924550.0, -72924560.0, -72924570.0, -72924580.0, -72924584.0, -72924590.0, -72924600.0, -72924610.0, -72924616.0, -72924620.0, -72924630.0, -72924640.0, -72924650.0, -72924660.0, -72924664.0, -72924670.0, -72924680.0, -72924690.0, -72924696.0, -72924700.0, -72924710.0, -72924720.0, -72924730.0, -72924740.0, -72924744.0, -72924750.0, -72924760.0, -72924770.0, -72924776.0, -72924780.0, -72924790.0, -72924800.0, -72924810.0, -72924820.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8298247857782325\n",
      "Hamming Loss: 0.08703158971735515\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92     33253\n",
      "           1       0.00      0.00      0.00      5842\n",
      "\n",
      "    accuracy                           0.85     39095\n",
      "   macro avg       0.43      0.50      0.46     39095\n",
      "weighted avg       0.72      0.85      0.78     39095\n",
      "\n",
      "Train on 156377 samples\n",
      "Epoch 1/100\n",
      "156377/156377 [==============================] - 22s 142us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 2/100\n",
      "156377/156377 [==============================] - 18s 116us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 3/100\n",
      "156377/156377 [==============================] - 17s 111us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 4/100\n",
      "156377/156377 [==============================] - 21s 132us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 5/100\n",
      "156377/156377 [==============================] - 18s 112us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 6/100\n",
      "156377/156377 [==============================] - 22s 141us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 7/100\n",
      "156377/156377 [==============================] - 18s 117us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 8/100\n",
      "156377/156377 [==============================] - 20s 127us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 9/100\n",
      "156377/156377 [==============================] - 19s 122us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 10/100\n",
      "156377/156377 [==============================] - 18s 118us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 11/100\n",
      "156377/156377 [==============================] - 17s 110us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 12/100\n",
      "156377/156377 [==============================] - 19s 124us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 13/100\n",
      "156377/156377 [==============================] - 20s 129us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 14/100\n",
      "156377/156377 [==============================] - 17s 110us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 15/100\n",
      "156377/156377 [==============================] - 19s 120us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 16/100\n",
      "156377/156377 [==============================] - 17s 109us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 17/100\n",
      "156377/156377 [==============================] - 18s 117us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 18/100\n",
      "156377/156377 [==============================] - 18s 114us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 19/100\n",
      "156377/156377 [==============================] - 20s 128us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 20/100\n",
      "156377/156377 [==============================] - 17s 110us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 21/100\n",
      "156377/156377 [==============================] - 19s 121us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 22/100\n",
      "156377/156377 [==============================] - 17s 112us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 23/100\n",
      "156377/156377 [==============================] - 17s 110us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 24/100\n",
      "156377/156377 [==============================] - 17s 109us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 25/100\n",
      "156377/156377 [==============================] - 17s 109us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 26/100\n",
      "156377/156377 [==============================] - 17s 109us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 27/100\n",
      "156377/156377 [==============================] - 17s 109us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 28/100\n",
      "156377/156377 [==============================] - 17s 110us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 29/100\n",
      "156377/156377 [==============================] - 17s 109us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 30/100\n",
      "156377/156377 [==============================] - 17s 109us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 31/100\n",
      "156377/156377 [==============================] - 17s 110us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 32/100\n",
      "156377/156377 [==============================] - 17s 111us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 33/100\n",
      "156377/156377 [==============================] - 17s 110us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 34/100\n",
      "156377/156377 [==============================] - 17s 110us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 35/100\n",
      "156377/156377 [==============================] - 19s 121us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 36/100\n",
      "156377/156377 [==============================] - 20s 126us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 37/100\n",
      "156377/156377 [==============================] - 18s 114us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 38/100\n",
      "156377/156377 [==============================] - 21s 132us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 39/100\n",
      "156377/156377 [==============================] - 19s 119us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 40/100\n",
      "156377/156377 [==============================] - 18s 117us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 41/100\n",
      "156377/156377 [==============================] - 23s 146us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 42/100\n",
      "156377/156377 [==============================] - 19s 121us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 43/100\n",
      "156377/156377 [==============================] - 17s 112us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 44/100\n",
      "156377/156377 [==============================] - 20s 126us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 45/100\n",
      "156377/156377 [==============================] - 25s 158us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 46/100\n",
      "156377/156377 [==============================] - 38s 241us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 47/100\n",
      "156377/156377 [==============================] - 30s 192us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 48/100\n",
      "156377/156377 [==============================] - 28s 178us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 49/100\n",
      "156377/156377 [==============================] - 24s 151us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - loss: 8.9813 - b - ETA: 1s - loss: 8.9851 -\n",
      "Epoch 50/100\n",
      "156377/156377 [==============================] - 24s 151us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 51/100\n",
      "156377/156377 [==============================] - 24s 151us/sample - loss: 8.9845 - binary_accuracy: 0.4175 ETA: 0s - loss: 8.9852 - binary_accuracy: 0.\n",
      "Epoch 52/100\n",
      "156377/156377 [==============================] - 25s 157us/sample - loss: 8.9845 - binary_accuracy: 0.4175 0s - loss: 8.9839 - binary_accura\n",
      "Epoch 53/100\n",
      "156377/156377 [==============================] - 22s 143us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 54/100\n",
      "156377/156377 [==============================] - 23s 147us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 55/100\n",
      "156377/156377 [==============================] - 24s 153us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 56/100\n",
      "156377/156377 [==============================] - 27s 174us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - loss: 8.9843 - binary_accur\n",
      "Epoch 57/100\n",
      "156377/156377 [==============================] - 29s 186us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - ETA: 0s - loss: 8.9853 - binary_accura\n",
      "Epoch 58/100\n",
      "156377/156377 [==============================] - 27s 172us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 59/100\n",
      "156377/156377 [==============================] - 27s 175us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - los\n",
      "Epoch 60/100\n",
      "156377/156377 [==============================] - 24s 151us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 61/100\n",
      "156377/156377 [==============================] - 28s 179us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 62/100\n",
      "156377/156377 [==============================] - 23s 148us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 63/100\n",
      "156377/156377 [==============================] - 27s 170us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 64/100\n",
      "156377/156377 [==============================] - 26s 169us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 65/100\n",
      "156377/156377 [==============================] - 22s 143us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 66/100\n",
      "156377/156377 [==============================] - 22s 143us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - loss: 8.9838 - binary_accu - ETA: 1s -\n",
      "Epoch 67/100\n",
      "156377/156377 [==============================] - 22s 143us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 68/100\n",
      "156377/156377 [==============================] - 23s 149us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 69/100\n",
      "156377/156377 [==============================] - 24s 151us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 70/100\n",
      "156377/156377 [==============================] - 24s 152us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 71/100\n",
      "156377/156377 [==============================] - 22s 143us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - loss: 8.9844 - binary_accurac\n",
      "Epoch 72/100\n",
      "156377/156377 [==============================] - 22s 140us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 73/100\n",
      "156377/156377 [==============================] - 22s 144us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 74/100\n",
      "156377/156377 [==============================] - 25s 161us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 75/100\n",
      "156377/156377 [==============================] - 25s 161us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 76/100\n",
      "156377/156377 [==============================] - 23s 145us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 77/100\n",
      "156377/156377 [==============================] - 22s 144us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 78/100\n",
      "156377/156377 [==============================] - 22s 143us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 79/100\n",
      "156377/156377 [==============================] - 22s 143us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - lo\n",
      "Epoch 80/100\n",
      "156377/156377 [==============================] - 22s 141us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 81/100\n",
      "156377/156377 [==============================] - 23s 146us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 82/100\n",
      "156377/156377 [==============================] - 37s 234us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 83/100\n",
      "156377/156377 [==============================] - 34s 219us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 84/100\n",
      "156377/156377 [==============================] - 33s 209us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 85/100\n",
      "156377/156377 [==============================] - 35s 222us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 86/100\n",
      "156377/156377 [==============================] - 30s 192us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - loss: 8.9869 - binar - ETA: 0s - loss: 8.9861 - binary_accurac\n",
      "Epoch 87/100\n",
      "156377/156377 [==============================] - 30s 194us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 88/100\n",
      "156377/156377 [==============================] - 29s 184us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - lo\n",
      "Epoch 89/100\n",
      "156377/156377 [==============================] - 31s 201us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - loss: 8.987\n",
      "Epoch 90/100\n",
      "156377/156377 [==============================] - 29s 184us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 91/100\n",
      "156377/156377 [==============================] - 32s 202us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - - ETA: 1s - loss: 8.98\n",
      "Epoch 92/100\n",
      "156377/156377 [==============================] - 32s 207us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 93/100\n",
      "156377/156377 [==============================] - 29s 187us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 94/100\n",
      "156377/156377 [==============================] - 30s 191us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - loss: 8.9851 - binary_accuracy: 0\n",
      "Epoch 95/100\n",
      "156377/156377 [==============================] - 31s 196us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 96/100\n",
      "156377/156377 [==============================] - 28s 182us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 97/100\n",
      "156377/156377 [==============================] - 29s 185us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 98/100\n",
      "156377/156377 [==============================] - 29s 186us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n",
      "Epoch 99/100\n",
      "156377/156377 [==============================] - 33s 211us/sample - loss: 8.9845 - binary_accuracy: 0.4175 - loss: 8.98\n",
      "Epoch 100/100\n",
      "156377/156377 [==============================] - 29s 187us/sample - loss: 8.9845 - binary_accuracy: 0.4175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-272919650.0, -272919680.0, -272919700.0, -272919740.0, -272919780.0, -272919800.0, -272919840.0, -272919870.0, -272919900.0, -272919940.0, -272919970.0, -272920000.0, -272920030.0, -272920060.0, -272920100.0, -272920130.0, -272920160.0, -272920200.0, -272920220.0, -272920260.0, -272920300.0, -272920320.0, -272920350.0, -272920400.0, -272920420.0, -272920450.0, -272920480.0, -272920500.0, -272920540.0, -272920580.0, -272920640.0, -272920670.0, -272920700.0, -272920740.0, -272921200.0, -272921250.0, -272921280.0, -272921300.0, -272921340.0, -272921380.0, -272921400.0, -272921440.0, -272921470.0, -272921500.0, -272921540.0, -272921570.0, -272921600.0, -272921630.0, -272921660.0, -272921700.0, -272921730.0, -272921760.0, -272921800.0, -272921820.0, -272921860.0, -272922370.0, -272922400.0, -272922430.0, -272922460.0, -272922500.0, -272922530.0, -272922560.0, -272922600.0, -272922620.0, -272922660.0, -272922700.0, -272922720.0, -272922750.0, -272922780.0, -272922800.0, -272922850.0, -272922880.0, -272922900.0, -272922940.0, -272922980.0, -272923000.0, -272923040.0, -272923070.0, -272923100.0, -272923140.0, -272923170.0, -272923200.0, -272923230.0, -272923260.0, -272923300.0, -272923330.0, -272923360.0, -272923400.0, -272923420.0, -272923460.0, -272923500.0, -272923520.0, -272923550.0, -272923600.0, -272923620.0, -272923650.0, -272923680.0, -272923700.0, -272923740.0, -272923780.0, -272923800.0, -272923840.0, -272923870.0, -272923900.0, -272923940.0, -272923970.0, -272924000.0, -272924030.0, -272924060.0, -272924100.0, -272924130.0, -272924160.0, -272924200.0, -272924220.0, -272924260.0, -272924300.0, -272924670.0, -272924700.0, -272924740.0, -2947738.0, -2947744.0, -2947746.0, -2947750.0, -2947752.0, -2947754.0, -2947756.0, -2947758.0, -2947760.0, -2947762.0, -2947764.0, -2947766.0, -2947768.0, -2947770.0, -2947772.0, -2947774.0, -2947776.0, -2947778.0, -2947780.0, -2947782.0, -2947784.0, -2947786.0, -2947788.0, -2947790.0, -2947792.0, -2947794.0, -2947796.0, -2947798.0, -2947800.0, -2947802.0, -2947804.0, -2947806.0, -2947808.0, -2947810.0, -2947812.0, -2947814.0, -2947816.0, -2947818.0, -2947820.0, -2947822.0, -2947824.0, -2947826.0, -2947828.0, -2947830.0, -2947832.0, -2947834.0, -2947836.0, -2947838.0, -2947840.0, -2947842.0, -2947844.0, -2947846.0, -2947848.0, -2947850.0, -2947852.0, -2947854.0, -2947856.0, -2947858.0, -2947860.0, -2947862.0, -2947864.0, -2947866.0, -2947868.0, -2947870.0, -2947872.0, -2947874.0, -2947876.0, -2947878.0, -2947880.0, -2947882.0, -2947884.0, -2947886.0, -2947888.0, -2947890.0, -2947892.0, -2947894.0, -2947896.0, -2947898.0, -2947900.0, -2947902.0, -2947904.0, -2947906.0, -2947908.0, -2947910.0, -2947912.0, -2947914.0, -2947916.0, -2947918.0, -2947920.0, -2947922.0, -2947924.0, -2947926.0, -2947928.0, -2947930.0, -2947932.0, -2947934.0, -2947936.0, -2947938.0, -2947940.0, -2947942.0, -2947944.0, -2947946.0, -2947948.0, -2947950.0, -2947952.0, -2947954.0, -2947956.0, -2947958.0, -2947960.0, -2947962.0, -2947964.0, -2947966.0, -2947968.0, -2947970.0, -2947972.0, -2947974.0, -2947976.0, -2947978.0, -2947980.0, -2947982.0, -2947984.0, -2947986.0, -2947988.0, -2947990.0, -2947992.0, -2947994.0, -2947996.0, -2947998.0, -2948000.0, -2948002.0, -2948004.0, -2948006.0, -2948008.0, -2948010.0, -2948012.0, -2948014.0, -2948016.0, -2948018.0, -2948020.0, -2948022.0, -2948024.0, -2948026.0, -2948028.0, -2948030.0, -2948032.0, -2948034.0, -2948036.0, -2948038.0, -2948040.0, -2948042.0, -2948044.0, -2948046.0, -2948048.0, -2948050.0, -2948052.0, -2948054.0, -2948056.0, -2948058.0, -2948060.0, -2948062.0, -2948064.0, -2948066.0, -2948068.0, -2948070.0, -2948072.0, -2948074.0, -2948076.0, -2948078.0, -2948080.0, -2948082.0, -2948084.0, -2948086.0, -2948088.0, -2948090.0, -2948092.0, -2948094.0, -2948096.0, -2948098.0, -2948100.0, -2948102.0, -2948104.0, -2948106.0, -2948108.0, -2948110.0, -2948112.0, -2948114.0, -2948116.0, -2948118.0, -2948120.0, -2948122.0, -2948124.0, -2948126.0, -2948128.0, -2948130.0, -2948132.0, -2948134.0, -2948136.0, -2948138.0, -2948140.0, -2948142.0, -2948144.0, -2948146.0, -2948148.0, -2948150.0, -2948152.0, -2948154.0, -2948156.0, -2948158.0, -2948160.0, -2948162.0, -2948164.0, -2948166.0, -2948168.0, -2948170.0, -2948172.0, -2948174.0, -2948176.0, -2948178.0, -2948180.0, -2948182.0, -2948184.0, -2948186.0, -2948188.0, -2948190.0, -2948192.0, -2948194.0, -2948196.0, -2948198.0, -2948200.0, -2948202.0, -2948204.0, -2948206.0, -2948208.0, -2948210.0, -2948212.0, -2948214.0, -2948216.0, -2948218.0, -2948220.0, -2948222.0, -2948224.0, -2948226.0, -2948228.0, -2948230.0, -2948232.0, -2948234.0, -2948236.0, -2948238.0, -2948240.0, -2948242.0, -2948244.0, -2948246.0, -2948248.0, -2948250.0, -2948252.0, -2948254.0, -2948256.0, -2948258.0, -2948260.0, -2948262.0, -2948264.0, -2948266.0, -2948268.0, -2948270.0, -2948272.0, -2948274.0, -2948276.0, -2948278.0, -2948280.0, -2948282.0, -2948284.0, -2948286.0, -2948288.0, -2948290.0, -2948292.0, -2948294.0, -2948296.0, -2948298.0, -2948300.0, -2948302.0, -2948304.0, -2948306.0, -2948308.0, -2948310.0, -2948312.0, -2948314.0, -2948316.0, -2948318.0, -2948320.0, -2948322.0, -2948324.0, -2948326.0, -2948328.0, -2948330.0, -2948332.0, -2948334.0, -2948336.0, -2948338.0, -2948340.0, -2948342.0, -2948344.0, -2948346.0, -2948348.0, -2948350.0, -2948352.0, -2948354.0, -2948356.0, -2948358.0, -2948360.0, -2948362.0, -2948364.0, -2948366.0, -2948368.0, -2948370.0, -2948372.0, -2948374.0, -2948376.0, -2948378.0, -2948380.0, -2948382.0, -2948384.0, -2948386.0, -2948388.0, -2948390.0, -2948392.0, -2948394.0, -2948396.0, -2948398.0, -2948400.0, -2948402.0, -2948404.0, -2948406.0, -2948410.0, -2948412.0, -2948414.0, -2948416.0, -2948418.0, -2948422.0, -2948424.0, -2948428.0, -2948434.0, -2948436.0, -2948440.0, -2948444.0, -2948448.0, -2948536.0, -2948540.0, -2948544.0, -2948546.0, -2948548.0, -2948550.0, -2948554.0, -2948556.0, -2948558.0, -2948560.0, -2948562.0, -2948564.0, -2948566.0, -2948568.0, -2948570.0, -2948572.0, -2948574.0, -2948576.0, -2948578.0, -2948580.0, -2948582.0, -2948584.0, -2948586.0, -2948588.0, -2948590.0, -2948592.0, -2948594.0, -2948596.0, -2948598.0, -2948600.0, -2948602.0, -2948604.0, -2948606.0, -2948608.0, -2948610.0, -2948612.0, -2948614.0, -2948616.0, -2948618.0, -2948620.0, -2948622.0, -2948624.0, -2948626.0, -2948628.0, -2948630.0, -2948632.0, -2948634.0, -2948636.0, -2948638.0, -2948640.0, -2948642.0, -2948644.0, -2948646.0, -2948648.0, -2948650.0, -2948652.0, -2948654.0, -2948656.0, -2948658.0, -2948660.0, -2948662.0, -2948664.0, -2948666.0, -2948668.0, -2948670.0, -2948672.0, -2948674.0, -2948676.0, -2948678.0, -2948680.0, -2948682.0, -2948684.0, -2948686.0, -2948688.0, -2948690.0, -2948692.0, -2948694.0, -2948696.0, -2948698.0, -2948700.0, -2948702.0, -2948704.0, -2948706.0, -2948708.0, -2948710.0, -2948712.0, -2948714.0, -2948716.0, -2948718.0, -2948720.0, -2948722.0, -2948724.0, -2948726.0, -2948728.0, -2948730.0, -2948732.0, -2948734.0, -2948736.0, -2948738.0, -2948740.0, -2948742.0, -2948744.0, -2948746.0, -2948748.0, -2948750.0, -2948752.0, -2948754.0, -2948756.0, -2948758.0, -2948760.0, -2948762.0, -2948764.0, -2948766.0, -2948768.0, -2948770.0, -2948772.0, -2948774.0, -2948776.0, -2948778.0, -2948780.0, -2948782.0, -2948784.0, -2948786.0, -2948788.0, -2948790.0, -2948792.0, -2948794.0, -2948796.0, -2948798.0, -2948800.0, -2948802.0, -2948804.0, -2948806.0, -2948808.0, -2948810.0, -2948812.0, -2948814.0, -2948816.0, -2948818.0, -2948820.0, -2948822.0, -2948824.0, -2948826.0, -2948828.0, -2948830.0, -2948832.0, -2948834.0, -2948836.0, -2948838.0, -2948840.0, -2948842.0, -2948844.0, -2948846.0, -2948848.0, -2948850.0, -2948852.0, -2948854.0, -2948856.0, -2948858.0, -2948860.0, -2948862.0, -2948864.0, -2948866.0, -2948868.0, -2948870.0, -2948872.0, -2948874.0, -2948876.0, -2948878.0, -2948880.0, -2948882.0, -2948884.0, -2948886.0, -2948888.0, -2948890.0, -2948892.0, -2948894.0, -2948896.0, -2948898.0, -2948900.0, -2948902.0, -2948904.0, -2948906.0, -2948908.0, -2948910.0, -2948912.0, -2948914.0, -2948916.0, -2948918.0, -2948920.0, -2948922.0, -2948924.0, -2948926.0, -2948928.0, -2948930.0, -2948932.0, -2948934.0, -2948936.0, -2948938.0, -2948940.0, -2948942.0, -2948944.0, -2948946.0, -2948948.0, -2948950.0, -2948952.0, -2948954.0, -2948956.0, -2948958.0, -2948960.0, -2948962.0, -2948964.0, -2948966.0, -2948968.0, -2948970.0, -2948972.0, -2948974.0, -2948976.0, -2948978.0, -2948980.0, -2948982.0, -2948984.0, -2948986.0, -2948988.0, -2948990.0, -2948992.0, -2948994.0, -2948996.0, -2948998.0, -2949000.0, -2949002.0, -2949004.0, -2949006.0, -2949008.0, -2949010.0, -2949012.0, -2949014.0, -2949016.0, -2949018.0, -2949020.0, -2949022.0, -2949024.0, -2949026.0, -2949028.0, -2949030.0, -2949032.0, -2949034.0, -2949036.0, -2949038.0, -2949040.0, -2949042.0, -2949044.0, -2949046.0, -2949048.0, -2949050.0, -2949052.0, -2949054.0, -2949056.0, -2949058.0, -2949060.0, -2949062.0, -2949064.0, -2949066.0, -2949068.0, -2949070.0, -2949072.0, -2949074.0, -2949076.0, -2949078.0, -2949080.0, -2949082.0, -2949084.0, -2949086.0, -2949088.0, -2949090.0, -2949092.0, -2949094.0, -2949096.0, -2949098.0, -2949100.0, -2949102.0, -2949104.0, -2949106.0, -2949108.0, -2949110.0, -2949112.0, -2949114.0, -2949116.0, -2949118.0, -2949120.0, -2949122.0, -2949124.0, -2949126.0, -2949128.0, -2949130.0, -2949132.0, -2949134.0, -2949136.0, -2949138.0, -2949140.0, -2949142.0, -2949144.0, -2949146.0, -2949148.0, -2949150.0, -2949152.0, -2949154.0, -2949156.0, -2949158.0, -2949160.0, -2949162.0, -2949164.0, -2949166.0, -2949168.0, -2949170.0, -2949172.0, -2949174.0, -2949176.0, -2949178.0, -2949180.0, -2949182.0, -2949184.0, -2949186.0, -2949188.0, -2949190.0, -2949192.0, -2949194.0, -2949196.0, -2949198.0, -2949200.0, -2949202.0, -2949204.0, -2949206.0, -2949208.0, -2949210.0, -2949212.0, -2949214.0, -2949216.0, -2949218.0, -2949220.0, -2949222.0, -2949224.0, -2949226.0, -2949228.0, -2949230.0, -2949232.0, -2949234.0, -2949236.0, -2949238.0, -2949240.0, -2949242.0, -2949244.0, -2949246.0, -2949248.0, -2949250.0, -2949252.0, -2949254.0, -2949256.0, -2949258.0, -2949260.0, -2949262.0, -2949264.0, -2949266.0, -2949268.0, -2949270.0, -2949272.0, -2949274.0, -2949276.0, -2949278.0, -2949280.0, -2949282.0, -2949284.0, -2949286.0, -2949288.0, -2949290.0, -2949292.0, -2949294.0, -2949296.0, -2949298.0, -2949300.0, -2949302.0, -2949304.0, -2949306.0, -2949308.0, -2949310.0, -2949312.0, -2949314.0, -2949316.0, -2949318.0, -2949320.0, -2949322.0, -2949324.0, -2949326.0, -2949328.0, -2949330.0, -2949332.0, -2949334.0, -2949336.0, -2949338.0, -2949340.0, -2949342.0, -2949344.0, -2949346.0, -2949348.0, -2949350.0, -2949352.0, -2949354.0, -2949356.0, -2949358.0, -2949360.0, -2949362.0, -2949364.0, -2949366.0, -2949368.0, -2949370.0, -2949372.0, -2949374.0, -2949376.0, -2949378.0, -2949380.0, -2949382.0, -2949384.0, -2949386.0, -2949388.0, -2949390.0, -2949392.0, -2949394.0, -2949396.0, -2949398.0, -2949400.0, -2949402.0, -2949404.0, -2949406.0, -2949408.0, -2949410.0, -2949412.0, -2949414.0, -2949416.0, -2949418.0, -2949420.0, -2949422.0, -2949424.0, -2949426.0, -2949428.0, -2949430.0, -2949432.0, -2949434.0, -2949436.0, -2949442.0, -2949444.0, -2950244.0, -2950252.0, -2950264.0, -2950266.0, -2950268.0, -2950270.0, -2950274.0, -2950276.0, -2950280.0, -2950282.0, -2950284.0, -2950286.0, -2950288.0, -2950290.0, -2950292.0, -2950294.0, -2950296.0, -2950298.0, -2950300.0, -2950302.0, -2950306.0, -2950310.0, -2950312.0, -2950314.0, -2950316.0, -2950318.0, -2950320.0, -2950322.0, -2950324.0, -2950326.0, -2950328.0, -2950330.0, -2950332.0, -2950334.0, -2950336.0, -2950338.0, -2950340.0, -2950342.0, -2950344.0, -2950346.0, -2950348.0, -2950350.0, -2950352.0, -2950354.0, -2950356.0, -2950358.0, -2950360.0, -2950362.0, -2950364.0, -2950366.0, -2950368.0, -2950370.0, -2950372.0, -2950374.0, -2950376.0, -2950378.0, -2950380.0, -2950382.0, -2950384.0, -2950386.0, -2950388.0, -2950390.0, -2950392.0, -2950394.0, -2950396.0, -2950398.0, -2950400.0, -2950402.0, -2950404.0, -2950406.0, -2950408.0, -2950410.0, -2950412.0, -2950414.0, -2950416.0, -2950418.0, -2950420.0, -2950422.0, -2950424.0, -2950426.0, -2950428.0, -2950430.0, -2950432.0, -2950434.0, -2950436.0, -2950438.0, -2950440.0, -2950442.0, -2950444.0, -2950446.0, -2950448.0, -2950450.0, -2950452.0, -2950454.0, -2950456.0, -2950458.0, -2950460.0, -2950462.0, -2950464.0, -2950466.0, -2950468.0, -2950470.0, -2950472.0, -2950474.0, -2950476.0, -2950478.0, -2950480.0, -2950482.0, -2950484.0, -2950486.0, -2950488.0, -2950490.0, -2950492.0, -2950494.0, -2950496.0, -2950498.0, -2950500.0, -2950502.0, -2950504.0, -2950506.0, -2950508.0, -2950510.0, -2950512.0, -2950514.0, -2950516.0, -2950518.0, -2950520.0, -2950522.0, -2950524.0, -2950526.0, -2950528.0, -2950530.0, -2950532.0, -2950534.0, -2950536.0, -2950538.0, -2950540.0, -2950542.0, -2950544.0, -2950546.0, -2950548.0, -2950550.0, -2950552.0, -2950554.0, -2950556.0, -2950558.0, -2950560.0, -2950562.0, -2950564.0, -2950566.0, -2950568.0, -2950570.0, -2950572.0, -2950574.0, -2950576.0, -2950578.0, -2950580.0, -2950582.0, -2950584.0, -2950610.0, -2951814.0, -2951834.0, -2951838.0, -2951840.0, -2951846.0, -2951848.0, -2951850.0, -2951854.0, -2951858.0, -2951860.0, -2951864.0, -2951866.0, -2951868.0, -2951870.0, -2951872.0, -2951874.0, -2951876.0, -2951878.0, -2951880.0, -2951882.0, -2951884.0, -2951886.0, -2951888.0, -2951890.0, -2951892.0, -2951894.0, -2951896.0, -2951898.0, -2951900.0, -2951902.0, -2951904.0, -2951906.0, -2951908.0, -2951910.0, -2951912.0, -2951914.0, -2951916.0, -2951918.0, -2951920.0, -2951922.0, -2951924.0, -2951926.0, -2951928.0, -2951930.0, -2951932.0, -2951934.0, -2951936.0, -2951938.0, -2951940.0, -2951942.0, -2951944.0, -2951946.0, -2951948.0, -2951950.0, -2951952.0, -2951954.0, -2951956.0, -2951958.0, -2951960.0, -2951962.0, -2951964.0, -2951966.0, -2951968.0, -2951970.0, -2951972.0, -2951974.0, -2951976.0, -2951978.0, -2951980.0, -2951982.0, -2951984.0, -2951986.0, -2951988.0, -2951990.0, -2951992.0, -2951994.0, -2951996.0, -2951998.0, -2952000.0, -2952002.0, -2952004.0, -2952006.0, -2952008.0, -2952010.0, -2952012.0, -2952014.0, -2952016.0, -2952018.0, -2952020.0, -2952022.0, -2952024.0, -2952026.0, -2952028.0, -2952030.0, -2952032.0, -2952034.0, -2952036.0, -2952038.0, -2952040.0, -2952042.0, -2952044.0, -2952046.0, -2952048.0, -2952050.0, -2952052.0, -2952054.0, -2952056.0, -2952058.0, -2952060.0, -2952062.0, -2952064.0, -2952066.0, -2952068.0, -2952070.0, -2952072.0, -2952074.0, -2952076.0, -2952078.0, -2952080.0, -2952082.0, -2952084.0, -2952086.0, -2952088.0, -2952090.0, -2952092.0, -2952094.0, -2952096.0, -2952098.0, -2952100.0, -2952102.0, -2952104.0, -2952106.0, -2952108.0, -2952110.0, -2952112.0, -2952114.0, -2952116.0, -2952118.0, -2952120.0, -2952122.0, -2952124.0, -2952126.0, -2952128.0, -2952130.0, -2952132.0, -2952134.0, -2952136.0, -2952138.0, -2952140.0, -2952142.0, -2952144.0, -2952146.0, -2952148.0, -2952150.0, -2952152.0, -2952154.0, -2952156.0, -2952158.0, -2952160.0, -2952162.0, -2952164.0, -2952166.0, -2952168.0, -2952170.0, -2952172.0, -2952174.0, -2952176.0, -2952178.0, -2952180.0, -2952182.0, -2952184.0, -2952186.0, -2952188.0, -2952190.0, -2952192.0, -2952194.0, -2952196.0, -2952198.0, -2952200.0, -2952202.0, -2952204.0, -2952206.0, -2952208.0, -2952210.0, -2952212.0, -2952214.0, -2952216.0, -2952218.0, -2952220.0, -2952222.0, -2952224.0, -2952226.0, -2952228.0, -2952230.0, -2952232.0, -2952234.0, -2952236.0, -2952238.0, -2952240.0, -2952242.0, -2952244.0, -2952246.0, -2952248.0, -2952250.0, -2952252.0, -2952254.0, -2952256.0, -2952258.0, -2952260.0, -2952262.0, -2952264.0, -2952266.0, -2952268.0, -2952270.0, -2952272.0, -2952274.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8239416805218058\n",
      "Hamming Loss: 0.09287632689602252\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     33863\n",
      "           1       0.13      1.00      0.24      5232\n",
      "\n",
      "    accuracy                           0.13     39095\n",
      "   macro avg       0.07      0.50      0.12     39095\n",
      "weighted avg       0.02      0.13      0.03     39095\n",
      "\n",
      "Train on 156378 samples\n",
      "Epoch 1/100\n",
      "156378/156378 [==============================] - 31s 199us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4375 - binary_accuracy: 0. - ETA: 1s - loss: 6.4366\n",
      "Epoch 2/100\n",
      "156378/156378 [==============================] - 31s 200us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 3/100\n",
      "156378/156378 [==============================] - 29s 186us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 4/100\n",
      "156378/156378 [==============================] - 30s 194us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 5/100\n",
      "156378/156378 [==============================] - 33s 210us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 6/100\n",
      "156378/156378 [==============================] - 30s 193us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 7/100\n",
      "156378/156378 [==============================] - 29s 187us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 8/100\n",
      "156378/156378 [==============================] - 31s 199us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 9/100\n",
      "156378/156378 [==============================] - 30s 194us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.44\n",
      "Epoch 10/100\n",
      "156378/156378 [==============================] - 30s 191us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 11/100\n",
      "156378/156378 [==============================] - 32s 207us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 12/100\n",
      "156378/156378 [==============================] - 30s 192us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 13/100\n",
      "156378/156378 [==============================] - 30s 189us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4381 - binary_accu\n",
      "Epoch 14/100\n",
      "156378/156378 [==============================] - 32s 206us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 15/100\n",
      "156378/156378 [==============================] - 30s 190us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4383 - binary_accuracy: 0.\n",
      "Epoch 16/100\n",
      "156378/156378 [==============================] - 29s 187us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 17/100\n",
      "156378/156378 [==============================] - 30s 194us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6\n",
      "Epoch 18/100\n",
      "156378/156378 [==============================] - 29s 187us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - los\n",
      "Epoch 19/100\n",
      "156378/156378 [==============================] - 36s 232us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 20/100\n",
      "156378/156378 [==============================] - 34s 216us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 21/100\n",
      "156378/156378 [==============================] - 40s 253us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 22/100\n",
      "156378/156378 [==============================] - 32s 208us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 23/100\n",
      "156378/156378 [==============================] - 31s 198us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 24/100\n",
      "156378/156378 [==============================] - 31s 200us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4 - ETA: 0s - loss: 6.4386 - binary_accuracy: 0.5\n",
      "Epoch 25/100\n",
      "156378/156378 [==============================] - 32s 203us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 26/100\n",
      "156378/156378 [==============================] - 31s 197us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 27/100\n",
      "156378/156378 [==============================] - 30s 195us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4404 - - ETA: 0s - loss: 6.4384 - binary_accuracy: 0.58\n",
      "Epoch 28/100\n",
      "156378/156378 [==============================] - 37s 236us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6 - ETA: 3s - loss: 6.4332 - binary_\n",
      "Epoch 29/100\n",
      "156378/156378 [==============================] - 35s 221us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 30/100\n",
      "156378/156378 [==============================] - 34s 217us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 31/100\n",
      "156378/156378 [==============================] - 36s 233us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 32/100\n",
      "156378/156378 [==============================] - 31s 196us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 33/100\n",
      "156378/156378 [==============================] - 33s 211us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 34/100\n",
      "156378/156378 [==============================] - 31s 198us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 35/100\n",
      "156378/156378 [==============================] - 31s 201us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 36/100\n",
      "156378/156378 [==============================] - 31s 201us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 37/100\n",
      "156378/156378 [==============================] - 47s 298us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6\n",
      "Epoch 38/100\n",
      "156378/156378 [==============================] - 56s 359us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 39/100\n",
      "156378/156378 [==============================] - 41s 260us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - los - ETA: 0s - loss: 6.4387 - binary_accuracy: 0.\n",
      "Epoch 40/100\n",
      "156378/156378 [==============================] - 36s 227us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 41/100\n",
      "156378/156378 [==============================] - 31s 198us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4378 - binary_accur\n",
      "Epoch 42/100\n",
      "156378/156378 [==============================] - 34s 216us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 43/100\n",
      "156378/156378 [==============================] - 31s 198us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 44/100\n",
      "156378/156378 [==============================] - 31s 199us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4388 - binary_accu\n",
      "Epoch 45/100\n",
      "156378/156378 [==============================] - 31s 201us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4383 - \n",
      "Epoch 46/100\n",
      "156378/156378 [==============================] - 33s 213us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 47/100\n",
      "156378/156378 [==============================] - 30s 190us/sample - loss: 6.4383 - binary_accuracy: 0.5801s - loss: 6.4414 - binary_ - ETA: 11s -  - ETA: 7s - loss: 6.4377 - bina - ETA: 0s - loss: 6.4392 - binary\n",
      "Epoch 48/100\n",
      "156378/156378 [==============================] - 35s 226us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss\n",
      "Epoch 49/100\n",
      "156378/156378 [==============================] - 34s 218us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 50/100\n",
      "156378/156378 [==============================] - 37s 238us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 51/100\n",
      "156378/156378 [==============================] - 31s 198us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 52/100\n",
      "156378/156378 [==============================] - 37s 237us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 53/100\n",
      "156378/156378 [==============================] - 31s 196us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 54/100\n",
      "156378/156378 [==============================] - 32s 204us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4381 - binary_accurac - ETA:  - ETA: 1s - loss: 6.\n",
      "Epoch 55/100\n",
      "156378/156378 [==============================] - 31s 200us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.43 - ETA: 5s - loss: 6.4410 - bi - ETA: 3s - loss: 6.4388 - ETA: 2s \n",
      "Epoch 56/100\n",
      "156378/156378 [==============================] - 31s 196us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 57/100\n",
      "156378/156378 [==============================] - 32s 205us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss - ET\n",
      "Epoch 58/100\n",
      "156378/156378 [==============================] - 35s 222us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 59/100\n",
      "156378/156378 [==============================] - 38s 240us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 60/100\n",
      "156378/156378 [==============================] - 31s 200us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 61/100\n",
      "156378/156378 [==============================] - 31s 197us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 62/100\n",
      "156378/156378 [==============================] - 31s 199us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 63/100\n",
      "156378/156378 [==============================] - 31s 200us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 64/100\n",
      "156378/156378 [==============================] - 36s 228us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4336 - bin\n",
      "Epoch 65/100\n",
      "156378/156378 [==============================] - 33s 210us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4380 - binary_accurac - ETA: 1s - loss: 6.4390\n",
      "Epoch 66/100\n",
      "156378/156378 [==============================] - 31s 201us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4381 - binary_accuracy: 0.58\n",
      "Epoch 67/100\n",
      "156378/156378 [==============================] - 31s 195us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 68/100\n",
      "156378/156378 [==============================] - 33s 211us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 69/100\n",
      "156378/156378 [==============================] - 40s 253us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 70/100\n",
      "156378/156378 [==============================] - 34s 216us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - \n",
      "Epoch 71/100\n",
      "156378/156378 [==============================] - 31s 199us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 72/100\n",
      "156378/156378 [==============================] - 34s 220us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 73/100\n",
      "156378/156378 [==============================] - 33s 212us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 74/100\n",
      "156378/156378 [==============================] - 33s 211us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 75/100\n",
      "156378/156378 [==============================] - 32s 206us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4392 - binary_\n",
      "Epoch 76/100\n",
      "156378/156378 [==============================] - 33s 208us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 77/100\n",
      "156378/156378 [==============================] - 32s 205us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 78/100\n",
      "156378/156378 [==============================] - 35s 223us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 79/100\n",
      "156378/156378 [==============================] - 30s 195us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 80/100\n",
      "156378/156378 [==============================] - 29s 188us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 81/100\n",
      "156378/156378 [==============================] - 30s 189us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 82/100\n",
      "156378/156378 [==============================] - 29s 188us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 83/100\n",
      "156378/156378 [==============================] - 30s 192us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 84/100\n",
      "156378/156378 [==============================] - 30s 191us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 85/100\n",
      "156378/156378 [==============================] - 30s 193us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 86/100\n",
      "156378/156378 [==============================] - 31s 199us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss: 6.4437 - \n",
      "Epoch 87/100\n",
      "156378/156378 [==============================] - 34s 220us/sample - loss: 6.4383 - binary_accuracy: 0.5801A: 0s - loss: 6.4373 - binary_acc\n",
      "Epoch 88/100\n",
      "156378/156378 [==============================] - 41s 262us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 89/100\n",
      "156378/156378 [==============================] - 34s 214us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 90/100\n",
      "156378/156378 [==============================] - ETA: 0s - loss: 6.4385 - binary_accuracy: 0.580 - 34s 215us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 91/100\n",
      "156378/156378 [==============================] - 30s 192us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 92/100\n",
      "156378/156378 [==============================] - 39s 250us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 93/100\n",
      "156378/156378 [==============================] - 51s 325us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 94/100\n",
      "156378/156378 [==============================] - 58s 374us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 95/100\n",
      "156378/156378 [==============================] - 57s 365us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - loss:  - \n",
      "Epoch 96/100\n",
      "156378/156378 [==============================] - 59s 378us/sample - loss: 6.4383 - binary_accuracy: 0.5801 - l - ETA: 1s - loss: 6.438\n",
      "Epoch 97/100\n",
      "156378/156378 [==============================] - 67s 429us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 98/100\n",
      "156378/156378 [==============================] - 64s 412us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 99/100\n",
      "156378/156378 [==============================] - 74s 471us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n",
      "Epoch 100/100\n",
      "156378/156378 [==============================] - 58s 370us/sample - loss: 6.4383 - binary_accuracy: 0.5801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [140071680.0, 140071700.0, 140071710.0, 140071730.0, 140071740.0, 140071760.0, 140071780.0, 140071790.0, 140072600.0, 140072620.0, 140072640.0, 140072660.0, 140072670.0, 140072900.0, 140072910.0, 140072930.0, 140072940.0, 140072960.0, 140072980.0, 140073000.0, 140073010.0, 140073020.0, 140073040.0, 140073060.0, 140073070.0, 140073090.0, 140073100.0, 140073120.0, 140073140.0, 140073150.0, 140073170.0, 140073180.0, 140073200.0, 140073220.0, 140073230.0, 140073250.0, 140073260.0, 140073280.0, 140073300.0, 140073310.0, 140073330.0, 140073340.0, 140073360.0, 140073380.0, 140073390.0, 140073500.0, 140073520.0, 140073540.0, 140073550.0, 140073570.0, 140073580.0, 140073600.0, 140073620.0, 140073630.0, 140073650.0, 140073660.0, 140073680.0, 140073730.0, 140073740.0, 140073760.0, 140073780.0, 140073800.0, 140073810.0, 140073820.0, 140073840.0, 140073860.0, 140073870.0, 140073890.0, 140073900.0, 140073920.0, 140073940.0, 140073950.0, 140073970.0, 140073980.0, 140075060.0, 140075070.0, 140075090.0, 140075100.0, 140075120.0, 140075140.0, 140075230.0, 140075250.0, 140075260.0, 140075280.0, 140075300.0, 140075310.0, 140075330.0, 140075340.0, 140075360.0, 140075380.0, 140075400.0, 140075410.0, 140075420.0, 140075440.0, 140075460.0, 140075470.0, 140075780.0, 140075790.0, 140075800.0, 140075820.0, 140075840.0, 140075860.0, 298644930.0, 298644960.0, 298645000.0, 298645020.0, 298645060.0, 298645100.0, 298648770.0, 298648800.0, 298648830.0, 298649000.0, 298649020.0, 298649060.0, 298649100.0, 298649120.0, 298649150.0, 298649180.0, 298649200.0, 298649250.0, 298649280.0, 298649300.0, 298649340.0, 298649380.0, 298649400.0, 298649440.0, 298649470.0, 298649500.0, 298649540.0, 298649570.0, 298649600.0, 298649630.0, 298649660.0, 298649700.0, 298649730.0, 298649760.0, 298649800.0, 298649820.0, 298649860.0, 298649900.0, 298649920.0, 298649950.0, 298650000.0, 298650370.0, 298650400.0, 298650430.0, 298650660.0, 298650700.0, 298650720.0, 298650750.0, 298650780.0, 298650800.0, 298650850.0, 298650880.0, 298650900.0, 298650940.0, 298650980.0, 298651000.0, 298651040.0, 298651520.0, 298651550.0, 298651600.0, 298651620.0, 298651650.0, 298651870.0, 298651900.0, 298651940.0, 298651970.0, 298652000.0, 298652030.0, 298652060.0, 298652100.0, 298652130.0, 298652160.0, 298652200.0, 298652220.0, 298652260.0, 298652300.0, 298652320.0, 298652350.0, 298652380.0, 298653100.0, 298653120.0, 298653150.0, 298653200.0, 298653220.0, 298653250.0, 298653280.0, 298653300.0, 298653340.0, 298653380.0, 298653400.0, 298653440.0, 298653470.0, 298653500.0, 298653540.0, 298658500.0, 298658530.0, 298658560.0, 298658600.0, 298658620.0, 298658660.0, 298658700.0, 298658720.0, 298658750.0, 298658780.0, 298658800.0, 298658850.0, 298658880.0, 298658900.0, 298658940.0, 298658980.0, 298659000.0, 298659040.0, 298659070.0, 298659100.0, 298659140.0, 298660030.0, 298660060.0, 298660100.0, 298660130.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8027318770143756\n",
      "Hamming Loss: 0.10464521409935028\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     33707\n",
      "           1       0.14      1.00      0.24      5387\n",
      "\n",
      "    accuracy                           0.14     39094\n",
      "   macro avg       0.07      0.50      0.12     39094\n",
      "weighted avg       0.02      0.14      0.03     39094\n",
      "\n",
      "Train on 156378 samples\n",
      "Epoch 1/100\n",
      "156378/156378 [==============================] - 110s 705us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 2/100\n",
      "156378/156378 [==============================] - 97s 618us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 3/100\n",
      "156378/156378 [==============================] - 64s 409us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 4/100\n",
      "156378/156378 [==============================] - 35s 226us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 5/100\n",
      "156378/156378 [==============================] - 31s 199us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 6/100\n",
      "156378/156378 [==============================] - 32s 206us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 7/100\n",
      "156378/156378 [==============================] - 38s 246us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 8/100\n",
      "156378/156378 [==============================] - 32s 202us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 9/100\n",
      "156378/156378 [==============================] - 36s 233us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 10/100\n",
      "156378/156378 [==============================] - 34s 217us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 11/100\n",
      "156378/156378 [==============================] - 32s 204us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 12/100\n",
      "156378/156378 [==============================] - 40s 253us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 13/100\n",
      "156378/156378 [==============================] - 34s 216us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0280 - binary_ac\n",
      "Epoch 14/100\n",
      "156378/156378 [==============================] - 32s 206us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 15/100\n",
      "156378/156378 [==============================] - 33s 209us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0281 - binary_accuracy\n",
      "Epoch 16/100\n",
      "156378/156378 [==============================] - 37s 239us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 17/100\n",
      "156378/156378 [==============================] - 38s 245us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 18/100\n",
      "156378/156378 [==============================] - 36s 228us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 19/100\n",
      "156378/156378 [==============================] - 32s 206us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 20/100\n",
      "156378/156378 [==============================] - 33s 212us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 21/100\n",
      "156378/156378 [==============================] - 34s 218us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 22/100\n",
      "156378/156378 [==============================] - 37s 234us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0290 -\n",
      "Epoch 23/100\n",
      "156378/156378 [==============================] - 35s 223us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0284 - binary_accura\n",
      "Epoch 24/100\n",
      "156378/156378 [==============================] - 34s 219us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 25/100\n",
      "156378/156378 [==============================] - 34s 215us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 26/100\n",
      "156378/156378 [==============================] - 34s 216us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 27/100\n",
      "156378/156378 [==============================] - 35s 221us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0280 - binary_ac\n",
      "Epoch 28/100\n",
      "156378/156378 [==============================] - 34s 215us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 29/100\n",
      "156378/156378 [==============================] - 33s 210us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 30/100\n",
      "156378/156378 [==============================] - 38s 243us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 31/100\n",
      "156378/156378 [==============================] - 36s 229us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 32/100\n",
      "156378/156378 [==============================] - 45s 291us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 33/100\n",
      "156378/156378 [==============================] - 33s 214us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 1\n",
      "Epoch 34/100\n",
      "156378/156378 [==============================] - 38s 242us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 35/100\n",
      "156378/156378 [==============================] - 33s 209us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 36/100\n",
      "156378/156378 [==============================] - 33s 213us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 37/100\n",
      "156378/156378 [==============================] - 34s 220us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0297 - ETA: 1s - los - ETA: 0s - loss: 14.0281 - binary_\n",
      "Epoch 38/100\n",
      "156378/156378 [==============================] - 32s 203us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 39/100\n",
      "156378/156378 [==============================] - 36s 232us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 40/100\n",
      "156378/156378 [==============================] - 39s 252us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 41/100\n",
      "156378/156378 [==============================] - 33s 210us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 42/100\n",
      "156378/156378 [==============================] - 35s 224us/sample - loss: 14.0283 - binary_accuracy: 0.0881- los\n",
      "Epoch 43/100\n",
      "156378/156378 [==============================] - 36s 227us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 44/100\n",
      "156378/156378 [==============================] - 35s 226us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 45/100\n",
      "156378/156378 [==============================] - 32s 207us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 46/100\n",
      "156378/156378 [==============================] - 34s 216us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 47/100\n",
      "156378/156378 [==============================] - 38s 240us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 48/100\n",
      "156378/156378 [==============================] - 33s 214us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 49/100\n",
      "156378/156378 [==============================] - 39s 253us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 50/100\n",
      "156378/156378 [==============================] - 37s 239us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 51/100\n",
      "156378/156378 [==============================] - 37s 237us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 52/100\n",
      "156378/156378 [==============================] - 33s 210us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 53/100\n",
      "156378/156378 [==============================] - 41s 259us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 54/100\n",
      "156378/156378 [==============================] - 40s 259us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.02\n",
      "Epoch 55/100\n",
      "156378/156378 [==============================] - 32s 203us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 56/100\n",
      "156378/156378 [==============================] - 39s 251us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 57/100\n",
      "156378/156378 [==============================] - 35s 224us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 58/100\n",
      "156378/156378 [==============================] - 39s 249us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 59/100\n",
      "156378/156378 [==============================] - 38s 242us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 60/100\n",
      "156378/156378 [==============================] - 36s 231us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 61/100\n",
      "156378/156378 [==============================] - 40s 256us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 62/100\n",
      "156378/156378 [==============================] - 31s 199us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 63/100\n",
      "156378/156378 [==============================] - 46s 293us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 64/100\n",
      "156378/156378 [==============================] - 43s 273us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 65/100\n",
      "156378/156378 [==============================] - 35s 225us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 66/100\n",
      "156378/156378 [==============================] - 30s 194us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 67/100\n",
      "156378/156378 [==============================] - 34s 215us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 68/100\n",
      "156378/156378 [==============================] - 39s 250us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 69/100\n",
      "156378/156378 [==============================] - 45s 287us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 70/100\n",
      "156378/156378 [==============================] - 41s 259us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 71/100\n",
      "156378/156378 [==============================] - 42s 271us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.02\n",
      "Epoch 72/100\n",
      "156378/156378 [==============================] - 33s 210us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 73/100\n",
      "156378/156378 [==============================] - 43s 272us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 74/100\n",
      "156378/156378 [==============================] - 36s 227us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 75/100\n",
      "156378/156378 [==============================] - 32s 202us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0282 - binary_accu - ETA: 3s - loss: 14.0275 - - ETA: 0s - l\n",
      "Epoch 76/100\n",
      "156378/156378 [==============================] - 32s 204us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 77/100\n",
      "156378/156378 [==============================] - 32s 208us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0295 - binary_\n",
      "Epoch 78/100\n",
      "156378/156378 [==============================] - 33s 212us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 79/100\n",
      "156378/156378 [==============================] - 41s 260us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0289 - b\n",
      "Epoch 80/100\n",
      "156378/156378 [==============================] - 36s 230us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 81/100\n",
      "156378/156378 [==============================] - 36s 229us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 82/100\n",
      "156378/156378 [==============================] - 34s 217us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 83/100\n",
      "156378/156378 [==============================] - 35s 225us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 84/100\n",
      "156378/156378 [==============================] - 37s 237us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 85/100\n",
      "156378/156378 [==============================] - 39s 252us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 86/100\n",
      "156378/156378 [==============================] - 43s 273us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 87/100\n",
      "156378/156378 [==============================] - 38s 244us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 88/100\n",
      "156378/156378 [==============================] - 49s 311us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 89/100\n",
      "156378/156378 [==============================] - 38s 244us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 90/100\n",
      "156378/156378 [==============================] - 33s 211us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 91/100\n",
      "156378/156378 [==============================] - 40s 256us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 92/100\n",
      "156378/156378 [==============================] - 32s 206us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0289 - binary_accuracy: 0.\n",
      "Epoch 93/100\n",
      "156378/156378 [==============================] - 41s 261us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 94/100\n",
      "156378/156378 [==============================] - 45s 286us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 95/100\n",
      "156378/156378 [==============================] - 34s 219us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 96/100\n",
      "156378/156378 [==============================] - 43s 274us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 97/100\n",
      "156378/156378 [==============================] - 42s 270us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 98/100\n",
      "156378/156378 [==============================] - 38s 242us/sample - loss: 14.0283 - binary_accuracy: 0.0881- loss: 14.0279\n",
      "Epoch 99/100\n",
      "156378/156378 [==============================] - 40s 258us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n",
      "Epoch 100/100\n",
      "156378/156378 [==============================] - 32s 207us/sample - loss: 14.0283 - binary_accuracy: 0.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-30531136.0, -30531144.0, -30531148.0, -30531152.0, -30531156.0, -30531160.0, -30531164.0, -30531168.0, -30531172.0, -30531176.0, -30531180.0, -30531184.0, -30531188.0, -30531192.0, -30531196.0, -30531200.0, -30531204.0, -30531208.0, -30531212.0, -30531216.0, -30531220.0, -30531224.0, -30531228.0, -30531232.0, -30531236.0, -30531240.0, -30531244.0, -30531248.0, -30531252.0, -30531256.0, -30531260.0, -30531264.0, -30531268.0, -30531272.0, -30531276.0, -30531280.0, -30531284.0, -30531288.0, -30531292.0, -30531296.0, -30531300.0, -30531304.0, -30531308.0, -30531312.0, -30531316.0, -30531320.0, -30531328.0, -30531748.0, -30531752.0, -30531760.0, -30531764.0, -30531768.0, -30531772.0, -30531776.0, -30531780.0, -30531784.0, -30531788.0, -30531792.0, -30531796.0, -30531800.0, -30531804.0, -30531808.0, -30531812.0, -30531816.0, -30531820.0, -30531824.0, -30531828.0, -30531832.0, -30531836.0, -30531840.0, -30531844.0, -30531848.0, -30531852.0, -30531856.0, -30531860.0, -30531864.0, -30531868.0, -30531872.0, -30531876.0, -30531880.0, -30531884.0, -30531888.0, -30531892.0, -30531896.0, -30531900.0, -30531904.0, -30531908.0, -30531912.0, -30531916.0, -30531920.0, -30531924.0, -30531928.0, -30531932.0, -30531936.0, -30531940.0, -30531944.0, -30531948.0, -30531952.0, -30531956.0, -30532428.0, -30532432.0, -30532436.0, -30532440.0, -30532444.0, -30532448.0, -30532452.0, -30532456.0, -30532460.0, -30532464.0, -30532468.0, -30532472.0, -30532476.0, -30532480.0, -30532484.0, -30532488.0, -30532492.0, -30532496.0, -30532500.0, -30532504.0, -30532508.0, -30532512.0, -30532516.0, -30532520.0, -30532524.0, -30532528.0, -30532532.0, -30532536.0, -30532540.0, -30532544.0, -30532548.0, -30532552.0, -30532556.0, -30532560.0, -30532564.0, -30532568.0, -30532572.0, -30532576.0, -30532580.0, -30532584.0, -30532588.0, -30532592.0, -30532596.0, -30532600.0, -30532604.0, -30532608.0, -30532612.0, -30532616.0, -30532624.0, -30532740.0, -30532744.0, -30532748.0, -30532752.0, -30532756.0, -30532760.0, -30532764.0, -30532768.0, -30532772.0, -30532776.0, -30532780.0, -30532784.0, -30532788.0, -30532792.0, -30532796.0, -30532800.0, -30532804.0, -30532808.0, -30532812.0, -30532816.0, -30532820.0, -30532824.0, -30532828.0, -30532832.0, -30532836.0, -30532840.0, -30532844.0, -30532848.0, -30532852.0, -30532856.0, -30532860.0, -30532864.0, -30532868.0, -30532872.0, -30532876.0, -30532880.0, -30532884.0, -30532888.0, -30532892.0, -30532896.0, -30532900.0, -30532904.0, -30532908.0, -30532912.0, -30532916.0, -30532920.0, -30532924.0, -30532928.0, -30532932.0, -30532936.0, -30532940.0, -30532944.0, -30532948.0, -30532952.0, -30532956.0, -30532960.0, -30532964.0, -30532968.0, -30532972.0, -30532976.0, -30532980.0, -30532984.0, -30532988.0, -30532992.0, -30532996.0, -30533000.0, -30533004.0, -30533008.0, -30533012.0, -30533016.0, -30533020.0, -30533024.0, -30533028.0, -30533032.0, -30533036.0, -30533040.0, -30533044.0, -30533048.0, -30533052.0, -30533056.0, -30533060.0, -30533064.0, -30533068.0, -30533072.0, -30533076.0, -30533080.0, -30533084.0, -30533088.0, -30533092.0, -30533096.0, -30533100.0, -30533104.0, -30533108.0, -30533112.0, -30533316.0, -30533320.0, -30533324.0, -30533328.0, -30533332.0, -30533336.0, -30533340.0, -30533344.0, -30533348.0, -30533352.0, -30533356.0, -30533360.0, -30533364.0, -30533368.0, -30533372.0, -30533376.0, -30533380.0, -30533384.0, -30533388.0, -30533392.0, -30533396.0, -30533400.0, -30533404.0, -30533408.0, -30533412.0, -30533416.0, -30533420.0, -30533424.0, -30533428.0, -30533432.0, -30533436.0, -30533440.0, -30533444.0, -30533448.0, -30533452.0, -30533456.0, -30533460.0, -30533464.0, -30533468.0, -30533472.0, -30533476.0, -30533480.0, -30533484.0, -30533488.0, -30533492.0, -30533496.0, -30533504.0, -30533508.0, -30534904.0, -30534908.0, -30534912.0, -30534916.0, -30534920.0, -30534924.0, -30534928.0, -30534932.0, -30534936.0, -30534940.0, -30534944.0, -30534948.0, -30534952.0, -30534956.0, -30534960.0, -30534964.0, -30534968.0, -30534972.0, -30534976.0, -30534980.0, -30534984.0, -30534988.0, -30534992.0, -30534996.0, -30535000.0, -30535004.0, -30535008.0, -30535012.0, -30535016.0, -30535020.0, -30535024.0, -30535028.0, -30535032.0, -30535036.0, -30535040.0, -30535044.0, -30535048.0, -30535052.0, -30535056.0, -30535060.0, -30535064.0, -30535068.0, -30535072.0, -30535076.0, -30535080.0, -30535084.0, -30535088.0, -30535092.0, -30535096.0, -30535100.0, -30535104.0, -30535108.0, -30535112.0, -30535116.0, -30535120.0, -30535124.0, -30535128.0, -30535132.0, -30535136.0, -30535140.0, -30535144.0, -30535148.0, -30535152.0, -30535156.0, -30535160.0, -30535164.0, -30535168.0, -30535172.0, -30535176.0, -30535180.0, -30535184.0, -30535188.0, -30535192.0, -30535196.0, -30535200.0, -30535204.0, -30535208.0, -30535212.0, -30535216.0, -30535220.0, -30535224.0, -30535228.0, -30535232.0, -30535236.0, -30535240.0, -30535244.0, -30535248.0, -30535252.0, -30535256.0, -30535260.0, -30535264.0, 9184955.0, 9184956.0, 9184960.0, 9184962.0, 9184964.0, 9184966.0, 9184968.0, 9184970.0, 9184971.0, 9184972.0, 9184973.0, 9184974.0, 9184975.0, 9184976.0, 9184977.0, 9184978.0, 9184979.0, 9184980.0, 9184981.0, 9184982.0, 9184983.0, 9184984.0, 9184985.0, 9184986.0, 9184987.0, 9184988.0, 9184989.0, 9184990.0, 9184991.0, 9184992.0, 9184993.0, 9184994.0, 9184995.0, 9184996.0, 9184997.0, 9184998.0, 9184999.0, 9185000.0, 9185001.0, 9185002.0, 9185003.0, 9185004.0, 9185005.0, 9185006.0, 9185007.0, 9185008.0, 9185009.0, 9185010.0, 9185011.0, 9185012.0, 9185013.0, 9185014.0, 9185015.0, 9185016.0, 9185017.0, 9185018.0, 9185019.0, 9185020.0, 9185021.0, 9185022.0, 9185023.0, 9185024.0, 9185025.0, 9185026.0, 9185027.0, 9185028.0, 9185029.0, 9185030.0, 9185031.0, 9185032.0, 9185033.0, 9185034.0, 9185035.0, 9185036.0, 9185037.0, 9185038.0, 9185039.0, 9185040.0, 9185041.0, 9185042.0, 9185043.0, 9185044.0, 9185045.0, 9185046.0, 9185047.0, 9185048.0, 9185049.0, 9185050.0, 9185051.0, 9185052.0, 9185053.0, 9185054.0, 9185055.0, 9185056.0, 9185057.0, 9185058.0, 9185059.0, 9185060.0, 9185061.0, 9185062.0, 9185063.0, 9185064.0, 9185065.0, 9185066.0, 9185067.0, 9185068.0, 9185069.0, 9185070.0, 9185071.0, 9185072.0, 9185073.0, 9185074.0, 9185075.0, 9185076.0, 9185077.0, 9185078.0, 9185079.0, 9185080.0, 9185081.0, 9185082.0, 9185083.0, 9185084.0, 9185085.0, 9185086.0, 9185087.0, 9185088.0, 9185089.0, 9185090.0, 9185091.0, 9185092.0, 9185093.0, 9185094.0, 9185095.0, 9185096.0, 9185097.0, 9185098.0, 9185099.0, 9185100.0, 9185101.0, 9185102.0, 9185103.0, 9185104.0, 9185105.0, 9185106.0, 9185107.0, 9185108.0, 9185109.0, 9185110.0, 9185111.0, 9185112.0, 9185113.0, 9185114.0, 9185115.0, 9185116.0, 9185117.0, 9185118.0, 9185119.0, 9185120.0, 9185121.0, 9185122.0, 9185123.0, 9185124.0, 9185125.0, 9185126.0, 9185127.0, 9185128.0, 9185129.0, 9185130.0, 9185131.0, 9185132.0, 9185133.0, 9185134.0, 9185135.0, 9185136.0, 9185137.0, 9185138.0, 9185139.0, 9185140.0, 9185141.0, 9185142.0, 9185143.0, 9185144.0, 9185145.0, 9185146.0, 9185147.0, 9185148.0, 9185149.0, 9185150.0, 9185151.0, 9185152.0, 9185153.0, 9185154.0, 9185155.0, 9185156.0, 9185157.0, 9185158.0, 9185159.0, 9185160.0, 9185161.0, 9185162.0, 9185163.0, 9185164.0, 9185165.0, 9185166.0, 9185167.0, 9185168.0, 9185169.0, 9185170.0, 9185171.0, 9185172.0, 9185173.0, 9185174.0, 9185175.0, 9185176.0, 9185177.0, 9185178.0, 9185179.0, 9185180.0, 9185181.0, 9185182.0, 9185183.0, 9185184.0, 9185185.0, 9185186.0, 9185187.0, 9185188.0, 9185189.0, 9185190.0, 9185191.0, 9185192.0, 9185193.0, 9185194.0, 9185195.0, 9185196.0, 9185198.0, 9185199.0, 9185200.0, 9185209.0, 9185216.0, 9185801.0, 9185803.0, 9185805.0, 9185806.0, 9185807.0, 9185809.0, 9185810.0, 9185811.0, 9185812.0, 9185813.0, 9185814.0, 9185815.0, 9185816.0, 9185817.0, 9185818.0, 9185819.0, 9185820.0, 9185821.0, 9185822.0, 9185823.0, 9185824.0, 9185825.0, 9185826.0, 9185827.0, 9185828.0, 9185829.0, 9185830.0, 9185831.0, 9185832.0, 9185833.0, 9185834.0, 9185835.0, 9185836.0, 9185837.0, 9185838.0, 9185839.0, 9185840.0, 9185841.0, 9185842.0, 9185843.0, 9185844.0, 9185845.0, 9185846.0, 9185847.0, 9185848.0, 9185849.0, 9185850.0, 9185851.0, 9185852.0, 9185853.0, 9185854.0, 9185855.0, 9185856.0, 9185857.0, 9185858.0, 9185859.0, 9185860.0, 9185861.0, 9185862.0, 9185863.0, 9185864.0, 9185865.0, 9185866.0, 9185867.0, 9185868.0, 9185869.0, 9185870.0, 9185871.0, 9185872.0, 9185873.0, 9185874.0, 9185875.0, 9185876.0, 9185877.0, 9185878.0, 9185879.0, 9185880.0, 9185881.0, 9185882.0, 9185883.0, 9185884.0, 9185885.0, 9185886.0, 9185887.0, 9185888.0, 9185889.0, 9185890.0, 9185891.0, 9185892.0, 9185893.0, 9185894.0, 9185895.0, 9185896.0, 9185897.0, 9185898.0, 9185899.0, 9185900.0, 9185901.0, 9185902.0, 9185903.0, 9185904.0, 9185905.0, 9185906.0, 9185907.0, 9185908.0, 9185909.0, 9185910.0, 9185911.0, 9185912.0, 9185913.0, 9185914.0, 9185915.0, 9185916.0, 9185917.0, 9185918.0, 9185919.0, 9185920.0, 9185921.0, 9185922.0, 9185923.0, 9185924.0, 9185925.0, 9185926.0, 9185927.0, 9185928.0, 9185929.0, 9185930.0, 9185931.0, 9185932.0, 9185933.0, 9185934.0, 9185935.0, 9185936.0, 9185937.0, 9185938.0, 9185939.0, 9185940.0, 9185941.0, 9185942.0, 9185943.0, 9185944.0, 9185945.0, 9185946.0, 9185947.0, 9185948.0, 9185949.0, 9185950.0, 9185951.0, 9185952.0, 9185953.0, 9185954.0, 9185955.0, 9185956.0, 9185957.0, 9185958.0, 9185959.0, 9185960.0, 9185961.0, 9185962.0, 9185963.0, 9185964.0, 9185965.0, 9185966.0, 9185968.0, 9185969.0, 9185971.0, 9185972.0, 9185975.0, 9186016.0, 9186021.0, 9186023.0, 9186024.0, 9186025.0, 9186026.0, 9186027.0, 9186028.0, 9186029.0, 9186031.0, 9186032.0, 9186033.0, 9186034.0, 9186035.0, 9186036.0, 9186037.0, 9186038.0, 9186039.0, 9186040.0, 9186041.0, 9186042.0, 9186043.0, 9186044.0, 9186045.0, 9186046.0, 9186047.0, 9186048.0, 9186049.0, 9186050.0, 9186051.0, 9186052.0, 9186053.0, 9186054.0, 9186055.0, 9186056.0, 9186057.0, 9186058.0, 9186059.0, 9186060.0, 9186061.0, 9186062.0, 9186063.0, 9186064.0, 9186065.0, 9186066.0, 9186067.0, 9186068.0, 9186069.0, 9186070.0, 9186071.0, 9186072.0, 9186073.0, 9186074.0, 9186075.0, 9186076.0, 9186077.0, 9186078.0, 9186079.0, 9186080.0, 9186081.0, 9186082.0, 9186083.0, 9186084.0, 9186085.0, 9186086.0, 9186087.0, 9186088.0, 9186089.0, 9186090.0, 9186091.0, 9186092.0, 9186093.0, 9186094.0, 9186095.0, 9186096.0, 9186097.0, 9186098.0, 9186099.0, 9186100.0, 9186101.0, 9186102.0, 9186103.0, 9186104.0, 9186105.0, 9186106.0, 9186107.0, 9186108.0, 9186109.0, 9186110.0, 9186111.0, 9186112.0, 9186113.0, 9186114.0, 9186115.0, 9186116.0, 9186117.0, 9186118.0, 9186119.0, 9186120.0, 9186121.0, 9186122.0, 9186123.0, 9186124.0, 9186125.0, 9186126.0, 9186127.0, 9186128.0, 9186129.0, 9186130.0, 9186131.0, 9186132.0, 9186133.0, 9186134.0, 9186135.0, 9186136.0, 9186137.0, 9186138.0, 9186139.0, 9186140.0, 9186141.0, 9186142.0, 9186143.0, 9186144.0, 9186145.0, 9186146.0, 9186147.0, 9186148.0, 9186149.0, 9186150.0, 9186151.0, 9186152.0, 9186153.0, 9186154.0, 9186155.0, 9186156.0, 9186157.0, 9186158.0, 9186159.0, 9186160.0, 9186161.0, 9186162.0, 9186163.0, 9186164.0, 9186165.0, 9186166.0, 9186167.0, 9186168.0, 9186169.0, 9186170.0, 9186171.0, 9186172.0, 9186173.0, 9186174.0, 9186175.0, 9186176.0, 9186177.0, 9186178.0, 9186179.0, 9186180.0, 9186181.0, 9186182.0, 9186183.0, 9186184.0, 9186185.0, 9186186.0, 9186187.0, 9186188.0, 9186189.0, 9186190.0, 9186191.0, 9186192.0, 9186193.0, 9186194.0, 9186195.0, 9186196.0, 9186197.0, 9186198.0, 9186199.0, 9186200.0, 9186201.0, 9186202.0, 9186203.0, 9186204.0, 9186205.0, 9186206.0, 9186207.0, 9186208.0, 9186209.0, 9186210.0, 9186211.0, 9186212.0, 9186213.0, 9186214.0, 9186215.0, 9186216.0, 9186217.0, 9186218.0, 9186219.0, 9186220.0, 9186221.0, 9186222.0, 9186223.0, 9186224.0, 9186225.0, 9186226.0, 9186227.0, 9186228.0, 9186229.0, 9186230.0, 9186231.0, 9186232.0, 9186233.0, 9186234.0, 9186235.0, 9186236.0, 9186237.0, 9186238.0, 9186239.0, 9186240.0, 9186241.0, 9186242.0, 9186243.0, 9186244.0, 9186245.0, 9186246.0, 9186247.0, 9186248.0, 9186249.0, 9186250.0, 9186251.0, 9186252.0, 9186253.0, 9186254.0, 9186255.0, 9186256.0, 9186257.0, 9186258.0, 9186259.0, 9186260.0, 9186261.0, 9186262.0, 9186263.0, 9186264.0, 9186265.0, 9186266.0, 9186267.0, 9186268.0, 9186269.0, 9186270.0, 9186271.0, 9186272.0, 9186273.0, 9186274.0, 9186275.0, 9186276.0, 9186277.0, 9186278.0, 9186279.0, 9186280.0, 9186281.0, 9186282.0, 9186283.0, 9186284.0, 9186285.0, 9186286.0, 9186287.0, 9186288.0, 9186289.0, 9186290.0, 9186291.0, 9186292.0, 9186293.0, 9186294.0, 9186295.0, 9186296.0, 9186297.0, 9186298.0, 9186299.0, 9186300.0, 9186301.0, 9186302.0, 9186303.0, 9186304.0, 9186305.0, 9186306.0, 9186307.0, 9186308.0, 9186309.0, 9186310.0, 9186311.0, 9186312.0, 9186313.0, 9186314.0, 9186315.0, 9186316.0, 9186317.0, 9186318.0, 9186319.0, 9186320.0, 9186321.0, 9186322.0, 9186323.0, 9186324.0, 9186325.0, 9186326.0, 9186327.0, 9186328.0, 9186329.0, 9186330.0, 9186331.0, 9186332.0, 9186333.0, 9186334.0, 9186335.0, 9186336.0, 9186337.0, 9186338.0, 9186339.0, 9186340.0, 9186341.0, 9186342.0, 9186343.0, 9186344.0, 9186345.0, 9186346.0, 9186347.0, 9186348.0, 9186349.0, 9186350.0, 9186351.0, 9186352.0, 9186353.0, 9186354.0, 9186355.0, 9186356.0, 9186357.0, 9186358.0, 9186359.0, 9186360.0, 9186361.0, 9186362.0, 9186363.0, 9186364.0, 9186365.0, 9186366.0, 9186367.0, 9186368.0, 9186369.0, 9186370.0, 9186371.0, 9186372.0, 9186373.0, 9186374.0, 9186375.0, 9186376.0, 9186377.0, 9186378.0, 9186379.0, 9186380.0, 9186381.0, 9186382.0, 9186383.0, 9186384.0, 9186385.0, 9186386.0, 9186387.0, 9186388.0, 9186389.0, 9186390.0, 9186391.0, 9186392.0, 9186393.0, 9186394.0, 9186395.0, 9186396.0, 9186397.0, 9186398.0, 9186399.0, 9186400.0, 9186401.0, 9186402.0, 9186403.0, 9186404.0, 9186405.0, 9186406.0, 9186407.0, 9186408.0, 9186409.0, 9186410.0, 9186411.0, 9186412.0, 9186413.0, 9186415.0, 9186417.0, 9186419.0, 9186421.0, 9186422.0, 9186424.0, 9186425.0, 9186428.0, 9186567.0, 9186580.0, 9186581.0, 9186582.0, 9186584.0, 9186585.0, 9186586.0, 9186587.0, 9186588.0, 9186589.0, 9186590.0, 9186591.0, 9186592.0, 9186593.0, 9186594.0, 9186595.0, 9186596.0, 9186597.0, 9186598.0, 9186599.0, 9186600.0, 9186601.0, 9186602.0, 9186603.0, 9186604.0, 9186605.0, 9186606.0, 9186607.0, 9186608.0, 9186609.0, 9186610.0, 9186611.0, 9186612.0, 9186613.0, 9186614.0, 9186615.0, 9186616.0, 9186617.0, 9186618.0, 9186619.0, 9186620.0, 9186621.0, 9186622.0, 9186623.0, 9186624.0, 9186625.0, 9186626.0, 9186627.0, 9186628.0, 9186629.0, 9186630.0, 9186631.0, 9186632.0, 9186633.0, 9186634.0, 9186635.0, 9186636.0, 9186637.0, 9186638.0, 9186639.0, 9186640.0, 9186641.0, 9186642.0, 9186643.0, 9186644.0, 9186645.0, 9186646.0, 9186647.0, 9186648.0, 9186649.0, 9186650.0, 9186651.0, 9186652.0, 9186653.0, 9186654.0, 9186655.0, 9186656.0, 9186657.0, 9186658.0, 9186659.0, 9186660.0, 9186661.0, 9186662.0, 9186663.0, 9186664.0, 9186665.0, 9186666.0, 9186667.0, 9186668.0, 9186669.0, 9186670.0, 9186671.0, 9186672.0, 9186673.0, 9186674.0, 9186675.0, 9186676.0, 9186677.0, 9186678.0, 9186679.0, 9186680.0, 9186681.0, 9186682.0, 9186683.0, 9186684.0, 9186685.0, 9186686.0, 9186687.0, 9186688.0, 9186689.0, 9186690.0, 9186691.0, 9186692.0, 9186693.0, 9186694.0, 9186695.0, 9186696.0, 9186697.0, 9186698.0, 9186699.0, 9186700.0, 9186701.0, 9186702.0, 9186703.0, 9186704.0, 9186705.0, 9186706.0, 9186707.0, 9186708.0, 9186709.0, 9186710.0, 9186711.0, 9186712.0, 9186713.0, 9186714.0, 9186715.0, 9186716.0, 9186717.0, 9186718.0, 9186719.0, 9186720.0, 9186721.0, 9186722.0, 9186723.0, 9186724.0, 9186725.0, 9186726.0, 9186727.0, 9186728.0, 9186729.0, 9186730.0, 9186731.0, 9186732.0, 9186733.0, 9186734.0, 9186735.0, 9186736.0, 9186737.0, 9186738.0, 9186739.0, 9186740.0, 9186741.0, 9186742.0, 9186743.0, 9186744.0, 9186745.0, 9186746.0, 9186747.0, 9186748.0, 9186749.0, 9186750.0, 9186751.0, 9186752.0, 9186753.0, 9186754.0, 9186755.0, 9186756.0, 9186757.0, 9186758.0, 9186759.0, 9186760.0, 9186761.0, 9186762.0, 9186764.0, 9186885.0, 9186886.0, 9186888.0, 9186889.0, 9186890.0, 9186891.0, 9186892.0, 9186893.0, 9186894.0, 9186896.0, 9186897.0, 9186898.0, 9186899.0, 9186900.0, 9186901.0, 9186902.0, 9186903.0, 9186904.0, 9186905.0, 9186906.0, 9186907.0, 9186908.0, 9186909.0, 9186910.0, 9186911.0, 9186912.0, 9186913.0, 9186914.0, 9186915.0, 9186916.0, 9186917.0, 9186918.0, 9186919.0, 9186920.0, 9186921.0, 9186922.0, 9186923.0, 9186924.0, 9186925.0, 9186926.0, 9186927.0, 9186928.0, 9186929.0, 9186930.0, 9186931.0, 9186932.0, 9186933.0, 9186934.0, 9186935.0, 9186936.0, 9186937.0, 9186938.0, 9186939.0, 9186940.0, 9186941.0, 9186942.0, 9186943.0, 9186944.0, 9186945.0, 9186946.0, 9186947.0, 9186948.0, 9186949.0, 9186950.0, 9186951.0, 9186952.0, 9186953.0, 9186954.0, 9186955.0, 9186956.0, 9186957.0, 9186958.0, 9186959.0, 9186960.0, 9186961.0, 9186962.0, 9186963.0, 9186964.0, 9186965.0, 9186966.0, 9186967.0, 9186968.0, 9186969.0, 9186970.0, 9186971.0, 9186972.0, 9186973.0, 9186974.0, 9186975.0, 9186976.0, 9186977.0, 9186978.0, 9186979.0, 9186980.0, 9186981.0, 9186982.0, 9186983.0, 9186984.0, 9186985.0, 9186986.0, 9186987.0, 9186988.0, 9186989.0, 9186990.0, 9186991.0, 9186992.0, 9186993.0, 9186994.0, 9186995.0, 9186996.0, 9186997.0, 9186998.0, 9186999.0, 9187000.0, 9187001.0, 9187002.0, 9187003.0, 9187004.0, 9187005.0, 9187006.0, 9187007.0, 9187008.0, 9187009.0, 9187010.0, 9187011.0, 9187012.0, 9187013.0, 9187014.0, 9187015.0, 9187016.0, 9187017.0, 9187018.0, 9187019.0, 9187020.0, 9187021.0, 9187022.0, 9187023.0, 9187024.0, 9187025.0, 9187026.0, 9187027.0, 9187028.0, 9187029.0, 9187030.0, 9187031.0, 9187033.0, 9187034.0, 9187035.0, 9187036.0, 9187037.0, 9187038.0, 9187045.0, 9187047.0, 9187051.0, 9187055.0, 9187057.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8187957231288688\n",
      "Hamming Loss: 0.0932496035197217\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     33086\n",
      "           1       0.15      1.00      0.27      6008\n",
      "\n",
      "    accuracy                           0.15     39094\n",
      "   macro avg       0.08      0.50      0.13     39094\n",
      "weighted avg       0.02      0.15      0.04     39094\n",
      "\n",
      "Train on 156378 samples\n",
      "Epoch 1/100\n",
      "156378/156378 [==============================] - 40s 254us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4576 - binary_accurac\n",
      "Epoch 2/100\n",
      "156378/156378 [==============================] - 34s 219us/sample - loss: 1.4565 - binary_accuracy: 0.9055ETA: 7s  - ETA: 4s - loss: - ETA: 3s - loss: 1.4592 - binary_accura - ETA:\n",
      "Epoch 3/100\n",
      "156378/156378 [==============================] - 34s 215us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 4/100\n",
      "156378/156378 [==============================] - 33s 209us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 5/100\n",
      "156378/156378 [==============================] - 32s 206us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 6/100\n",
      "156378/156378 [==============================] - 40s 257us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 7/100\n",
      "156378/156378 [==============================] - 61s 392us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 8/100\n",
      "156378/156378 [==============================] - 45s 291us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 9/100\n",
      "156378/156378 [==============================] - 50s 320us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 10/100\n",
      "156378/156378 [==============================] - 50s 320us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 11/100\n",
      "156378/156378 [==============================] - 47s 302us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 12/100\n",
      "156378/156378 [==============================] - 47s 301us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 13/100\n",
      "156378/156378 [==============================] - 50s 319us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 14/100\n",
      "156378/156378 [==============================] - 47s 301us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 15/100\n",
      "156378/156378 [==============================] - 48s 304us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 16/100\n",
      "156378/156378 [==============================] - 47s 299us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 17/100\n",
      "156378/156378 [==============================] - 47s 303us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 18/100\n",
      "156378/156378 [==============================] - 48s 309us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 19/100\n",
      "156378/156378 [==============================] - 48s 305us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 20/100\n",
      "156378/156378 [==============================] - 49s 313us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 21/100\n",
      "156378/156378 [==============================] - 64s 408us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 22/100\n",
      "156378/156378 [==============================] - 50s 321us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 23/100\n",
      "156378/156378 [==============================] - 48s 310us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 24/100\n",
      "156378/156378 [==============================] - 47s 302us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 25/100\n",
      "156378/156378 [==============================] - 64s 407us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 26/100\n",
      "156378/156378 [==============================] - 53s 342us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 27/100\n",
      "156378/156378 [==============================] - 50s 318us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 28/100\n",
      "156378/156378 [==============================] - 64s 406us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 29/100\n",
      "156378/156378 [==============================] - 61s 390us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 30/100\n",
      "156378/156378 [==============================] - 63s 405us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 31/100\n",
      "156378/156378 [==============================] - 95s 608us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 32/100\n",
      "156378/156378 [==============================] - 75s 478us/sample - loss: 1.4565 - binary_accuracy: 0.9055A: 0s - loss: 1.4587 - \n",
      "Epoch 33/100\n",
      "156378/156378 [==============================] - 73s 465us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 34/100\n",
      "156378/156378 [==============================] - 81s 519us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 35/100\n",
      "156378/156378 [==============================] - 66s 425us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.455\n",
      "Epoch 36/100\n",
      "156378/156378 [==============================] - 65s 415us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4574 - binary_acc\n",
      "Epoch 37/100\n",
      "156378/156378 [==============================] - 66s 419us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 38/100\n",
      "156378/156378 [==============================] - 42s 266us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 39/100\n",
      "156378/156378 [==============================] - 41s 265us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4558 - binary\n",
      "Epoch 40/100\n",
      "156378/156378 [==============================] - 53s 338us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 41/100\n",
      "156378/156378 [==============================] - ETA: 0s - loss: 1.4567 - binary_accuracy: 0.905 - 40s 257us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 42/100\n",
      "156378/156378 [==============================] - 46s 295us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 43/100\n",
      "156378/156378 [==============================] - 37s 236us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4559 - binary_accuracy\n",
      "Epoch 44/100\n",
      "156378/156378 [==============================] - 55s 352us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 45/100\n",
      "156378/156378 [==============================] - 44s 280us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 46/100\n",
      "156378/156378 [==============================] - 44s 282us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 47/100\n",
      "156378/156378 [==============================] - 36s 232us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 48/100\n",
      "156378/156378 [==============================] - 34s 217us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4578\n",
      "Epoch 49/100\n",
      "156378/156378 [==============================] - 39s 247us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 50/100\n",
      "156378/156378 [==============================] - 34s 219us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 51/100\n",
      "156378/156378 [==============================] - 36s 232us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 52/100\n",
      "156378/156378 [==============================] - 32s 205us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 53/100\n",
      "156378/156378 [==============================] - 38s 244us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 54/100\n",
      "156378/156378 [==============================] - 42s 268us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 55/100\n",
      "156378/156378 [==============================] - 42s 271us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4580 - bin\n",
      "Epoch 56/100\n",
      "156378/156378 [==============================] - 48s 304us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 57/100\n",
      "156378/156378 [==============================] - 47s 298us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 58/100\n",
      "156378/156378 [==============================] - 48s 309us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 59/100\n",
      "156378/156378 [==============================] - 48s 309us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4574 - binary_\n",
      "Epoch 60/100\n",
      "156378/156378 [==============================] - 48s 307us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4556 - binary_acc\n",
      "Epoch 61/100\n",
      "156378/156378 [==============================] - 61s 391us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss:\n",
      "Epoch 62/100\n",
      "156378/156378 [==============================] - 83s 528us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 63/100\n",
      "156378/156378 [==============================] - 46s 295us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 64/100\n",
      "156378/156378 [==============================] - 53s 340us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4565 - binary_accuracy - ETA: 3s - loss: 1.4563 - bin - ETA: 2s - loss:\n",
      "Epoch 65/100\n",
      "156378/156378 [==============================] - 59s 375us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 66/100\n",
      "156378/156378 [==============================] - 61s 387us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 67/100\n",
      "156378/156378 [==============================] - 56s 357us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 68/100\n",
      "156378/156378 [==============================] - 49s 311us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 69/100\n",
      "156378/156378 [==============================] - 61s 392us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 70/100\n",
      "156378/156378 [==============================] - 82s 526us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 71/100\n",
      "156378/156378 [==============================] - 62s 399us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 72/100\n",
      "156378/156378 [==============================] - 81s 519us/sample - loss: 1.4565 - binary_accuracy: 0.9055: 1s - loss: 1.4584 - binary_accuracy: 0.9 - ETA: 1s - loss: 1.4583 - bina\n",
      "Epoch 73/100\n",
      "156378/156378 [==============================] - 66s 424us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 74/100\n",
      "156378/156378 [==============================] - 63s 405us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 75/100\n",
      "156378/156378 [==============================] - 68s 437us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 76/100\n",
      "156378/156378 [==============================] - 59s 378us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 77/100\n",
      "156378/156378 [==============================] - 77s 495us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 78/100\n",
      "156378/156378 [==============================] - 61s 388us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 79/100\n",
      "156378/156378 [==============================] - 69s 442us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - ETA: 1s - loss: 1.45\n",
      "Epoch 80/100\n",
      "156378/156378 [==============================] - 69s 441us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 81/100\n",
      "156378/156378 [==============================] - 58s 371us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4563 - binary_a\n",
      "Epoch 82/100\n",
      "156378/156378 [==============================] - 82s 526us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 83/100\n",
      "156378/156378 [==============================] - 60s 387us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 84/100\n",
      "156378/156378 [==============================] - 53s 338us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 85/100\n",
      "156378/156378 [==============================] - 54s 344us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 86/100\n",
      "156378/156378 [==============================] - 69s 439us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 87/100\n",
      "156378/156378 [==============================] - 61s 392us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 88/100\n",
      "156378/156378 [==============================] - 61s 389us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 89/100\n",
      "156378/156378 [==============================] - 73s 466us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 90/100\n",
      "156378/156378 [==============================] - 61s 391us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 91/100\n",
      "156378/156378 [==============================] - 54s 345us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 92/100\n",
      "156378/156378 [==============================] - 65s 414us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 93/100\n",
      "156378/156378 [==============================] - 48s 310us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 94/100\n",
      "156378/156378 [==============================] - 47s 299us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.4573 - binary\n",
      "Epoch 95/100\n",
      "156378/156378 [==============================] - 47s 303us/sample - loss: 1.4565 - binary_accuracy: 0.9055 - loss: 1.45\n",
      "Epoch 96/100\n",
      "156378/156378 [==============================] - ETA: 0s - loss: 1.4563 - binary_accuracy: 0.905 - 46s 297us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 97/100\n",
      "156378/156378 [==============================] - 46s 296us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 98/100\n",
      "156378/156378 [==============================] - 70s 447us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 99/100\n",
      "156378/156378 [==============================] - 42s 267us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n",
      "Epoch 100/100\n",
      "156378/156378 [==============================] - 48s 306us/sample - loss: 1.4565 - binary_accuracy: 0.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-65894016.0, -65894020.0, -65894024.0, -65894030.0, -65894036.0, -65894040.0, -65894044.0, -65894050.0, -65894052.0, -65894056.0, -65894060.0, -65894064.0, -65894068.0, -65894070.0, -65894076.0, -65894080.0, -65894084.0, -65894090.0, -65894092.0, -65894096.0, -65894100.0, -65894104.0, -65894108.0, -65894110.0, -65894116.0, -65894120.0, -65894124.0, -65894130.0, -65894132.0, -65894136.0, -65894140.0, -65894144.0, -65894148.0, -65894150.0, -65894156.0, -65894160.0, -65894164.0, -65894170.0, -65894172.0, -65894176.0, -65894180.0, -65894184.0, -65894188.0, -65894190.0, -65894196.0, -65894200.0, -65894204.0, -65894210.0, -65894212.0, -65894216.0, -65894220.0, -65894224.0, -65894228.0, -65894230.0, -65894236.0, -65894240.0, -65894244.0, -65894250.0, -65894252.0, -65894256.0, -65894260.0, -65894344.0, -65894350.0, -65894356.0, -65894360.0, -65894364.0, -65894370.0, -65894372.0, -65894376.0, -65894380.0, -65894384.0, -65894388.0, -65894390.0, -65894396.0, -65894400.0, -65894404.0, -65894410.0, -65894412.0, -65894416.0, -65894420.0, -65894424.0, -65894428.0, -65894430.0, -65894436.0, -65894440.0, -65894444.0, -65894450.0, -65894452.0, -65894456.0, -65894460.0, -65894464.0, -65894468.0, -65894470.0, -65894476.0, -65894480.0, -65894484.0, -65894490.0, -65894492.0, -65894496.0, -65894500.0, -65894504.0, -65894508.0, -65894510.0, -65894516.0, -65894520.0, -65894524.0, -65894530.0, -65894532.0, -65894536.0, -65894540.0, -65894544.0, -65894548.0, -65894550.0, -65894556.0, -65894560.0, -65894564.0, -65894570.0, -65894572.0, -65894576.0, -65894580.0, -65894584.0, -65894588.0, -65894590.0, -65894596.0, -65894600.0, -65894604.0, -65894610.0, -65894612.0, -65894616.0, -65894620.0, -65894624.0, -65894628.0, -65894630.0, -65894636.0, -65894640.0, -65894644.0, -65894650.0, -65894652.0, -65894656.0, -65894660.0, -65894664.0, -65894668.0, -65894670.0, -65894676.0, -65894680.0, -65894684.0, -65894690.0, -65894692.0, -65894696.0, -65894700.0, -65894704.0, -65894708.0, -65894710.0, -65894716.0, -65894720.0, -65894724.0, -65894730.0, -65894732.0, -65894736.0, -65894740.0, -65894744.0, -65894748.0, -65894750.0, -65894756.0, -65894760.0, -65894764.0, -65894770.0, -65894772.0, -65894884.0, -65894890.0, -65894892.0, -65894896.0, -65894900.0, -65894904.0, -65894908.0, -65894910.0, -65894916.0, -65894920.0, -65894924.0, -65894930.0, -65894932.0, -65894936.0, -65894940.0, -65894944.0, -65894948.0, -65894950.0, -65894956.0, -65894960.0, -65894964.0, -65894970.0, -65894972.0, -65894976.0, -65894980.0, -65894984.0, -65894988.0, -65894990.0, -65894996.0, -65895000.0, -65895004.0, -65895010.0, -65895012.0, -65895016.0, -65895020.0, -65895024.0, -65895028.0, -65895030.0, -65895036.0, -65895040.0, -65895044.0, -65895050.0, -65895052.0, -65895056.0, -65895060.0, -65895064.0, -65895068.0, -65895070.0, -65895076.0, -65895080.0, -65895084.0, -65895090.0, -65895092.0, -65895096.0, -65895100.0, -65895104.0, -65895108.0, -65895110.0, -65895116.0, -65895120.0, -65895124.0, -65895130.0, -65895132.0, -65895136.0, -65895140.0, -65895144.0, -65895148.0, -65895150.0, -65895156.0, -65895160.0, -65895164.0, -65895170.0, -65895172.0, -65895176.0, -65895180.0, -65895184.0, -65895188.0, -65895190.0, -65895196.0, -65895200.0, -65895204.0, -65895210.0, -65895212.0, -65895216.0, -65895220.0, -65895224.0, -65895228.0, -65895230.0, -65895236.0, -65895240.0, -65895244.0, -65895250.0, -65895252.0, -65895256.0, -65895260.0, -65895264.0, -65895268.0, -65895270.0, -65895276.0, -65895280.0, -65895284.0, -65895290.0, -65895292.0, -65895296.0, -65895300.0, -65895304.0, -65895308.0, -65895310.0, -65895316.0, -65895320.0, -65895324.0, -65895330.0, -65895332.0, -65895336.0, -65895340.0, -65895344.0, -65895350.0, -65897696.0, -65897700.0, -65897704.0, -65897708.0, -65897710.0, -65897716.0, -65897720.0, -65897724.0, -65897730.0, -65897732.0, -65897736.0, -65897740.0, -65897744.0, -65897748.0, -65897750.0, -65897756.0, -65897760.0, -65897764.0, -65897770.0, -65897772.0, -65897776.0, -65897780.0, -65897784.0, -65897788.0, -65897790.0, -65897796.0, -65897800.0, -65897804.0, -65897810.0, -65897812.0, -65897816.0, -65897820.0, -65897824.0, -65897828.0, -65897830.0, -65897836.0, -65897840.0, -65897844.0, -65897850.0, -65897852.0, -65897856.0, -65897860.0, -65897864.0, -65897868.0, -65897870.0, -65897876.0, -65897880.0, -65897884.0, -65897890.0, -65897892.0, -65897896.0, -65897900.0, -65897904.0, -65897908.0, -65897910.0, -65897916.0, -65897920.0, -65897924.0, -65897930.0, -65897932.0, -65897936.0, -65897940.0, -65897944.0, -65897948.0, -65897950.0, -65897956.0, -65897960.0, -65897964.0, -65897970.0, -65897972.0, -65897976.0, -65897980.0, -65897984.0, -65897988.0, -65897990.0, -65897996.0, -65898000.0, -65898004.0, -65898010.0, -65898012.0, -65898016.0, -65898020.0, -65898024.0, -65898028.0, -65898030.0, -65898036.0, -65898040.0, -65898044.0, -65898050.0, -65898052.0, -65898056.0, -65898060.0, -65898064.0, -65898068.0, -65898070.0, -65898076.0, -65898080.0, -65898084.0, -65898090.0, -65898092.0, -65898096.0, -65898100.0, -65898104.0, -65898108.0, -65898110.0, -65898116.0, -65898120.0, -65898124.0, -65898130.0, -65898132.0, -65898136.0, -65898140.0, -65898144.0, -65898148.0, -65898150.0, -65898156.0, -65898160.0, -65898164.0, -65898170.0, -65898172.0, -65898176.0, -65898180.0, -65898184.0, -65898188.0, -65898190.0, -65898196.0, -65898200.0, -65898204.0, -65898210.0, -65898212.0, -65898216.0, -65898220.0, -65898224.0, -65898228.0, -65898230.0, -65898236.0, -65898244.0, -65898250.0, -65898280.0, -65898284.0, -65898290.0, -65898292.0, -65898296.0, -65898300.0, -65898304.0, -65898308.0, -65898310.0, -65898316.0, -65898320.0, -65898324.0, -65898330.0, -65898332.0, -65898336.0, -65898340.0, -65898344.0, -65898348.0, -65898350.0, -65898356.0, -65898360.0, -65898364.0, -65898370.0, -65898372.0, -65898376.0, -65898380.0, -65898384.0, -65898388.0, -65898390.0, -65898396.0, -65898400.0, -65898404.0, -65898410.0, -65898412.0, -65898416.0, -65898420.0, -65898424.0, -65898428.0, -65898430.0, -65898436.0, -65898440.0, -65898444.0, -65898450.0, -65898452.0, -65898456.0, -65898460.0, -65898464.0, -65898468.0, -65898470.0, -65898476.0, -65898480.0, -65898484.0, -65898490.0, -65898492.0, -65898496.0, -65898500.0, -65898504.0, -65898508.0, -65898510.0, -65898516.0, -65898520.0, -65898524.0, -65898530.0, -65898532.0, -65898536.0, -65898540.0, -65898544.0, -65898548.0, -65898550.0, -65898556.0, -65898560.0, -65898564.0, -65898570.0, -65898572.0, -65898576.0, -65898580.0, -65898584.0, -65898588.0, -65898590.0, -65898596.0, -65898600.0, -65898604.0, -65898610.0, -65898612.0, -65898616.0, -65898620.0, -65898624.0, -65898628.0, -65898630.0, -65898636.0, -65898640.0, -65898644.0, -65898650.0, -65898652.0, -65898656.0, -65898660.0, -65898664.0, -65898668.0, -65898670.0, -65898676.0, -65898680.0, -65898690.0, -65898696.0, -65898710.0, -65898812.0, -65898820.0, -65898824.0, -65898830.0, -65898836.0, -65898840.0, -65898844.0, -65898850.0, -65898852.0, -65898856.0, -65898860.0, -65898864.0, -65898868.0, -65898870.0, -65898876.0, -65898880.0, -65898884.0, -65898890.0, -65898892.0, -65898896.0, -65898900.0, -65898904.0, -65898908.0, -65898910.0, -65898916.0, -65898920.0, -65898924.0, -65898930.0, -65898932.0, -65898936.0, -65898940.0, -65898944.0, -65898948.0, -65898950.0, -65898956.0, -65898960.0, -65898964.0, -65898970.0, -65898972.0, -65898976.0, -65898980.0, -65898984.0, -65898988.0, -65898990.0, -65898996.0, -65899000.0, -65899004.0, -65899010.0, -65899012.0, -65899016.0, -65899020.0, -65899024.0, -65899028.0, -65899030.0, -65899036.0, -65899040.0, -65899044.0, -65899050.0, -65899052.0, -65899056.0, -65899060.0, -65899064.0, -65899068.0, -65899070.0, -65899076.0, -65899080.0, -65899084.0, -65899090.0, -65899092.0, -65899096.0, -65899100.0, -65899104.0, -65899108.0, -65899110.0, -65899116.0, -65899120.0, -65899124.0, -65899130.0, -65899132.0, -65899136.0, -65899140.0, -65899144.0, -65899148.0, -65899150.0, -65899156.0, -65899160.0, -65899164.0, -65899170.0, -65899172.0, -65899176.0, -65899180.0, -65899184.0, -65899188.0, -65899190.0, -65899196.0, -65899200.0, -65899204.0, -65899210.0, -65899212.0, -65899216.0, -65899220.0, -65899224.0, -65899228.0, -65899230.0, -65899236.0, -65899240.0, -65899244.0, -65899250.0, -65899252.0, -65899256.0, -65899260.0, -65899264.0, -65899268.0, -65899270.0, -65899276.0, -65899280.0, -65899284.0, -65899290.0, -65899292.0, -65899296.0, -65899300.0, -65899304.0, -65899308.0, -65899310.0, -65899316.0, -65899320.0, -65899324.0, -65899330.0, -65899332.0, -65899336.0, -65899340.0, -65899344.0, -65899348.0, -65899350.0, -65899356.0, -65899360.0, -65899364.0, -65899370.0, -65899372.0, -65899376.0, -65899380.0, -65899384.0, -65899388.0, -65899390.0, -65899396.0, -65899400.0, -65899700.0, -65899704.0, -65899708.0, -65899710.0, -65899720.0, -65899724.0, -65899730.0, -65899732.0, -65899736.0, -65899740.0, -65899744.0, -65899748.0, -65899750.0, -65899756.0, -65899760.0, -65899764.0, -65899770.0, -65899772.0, -65899776.0, -65899780.0, -65899784.0, -65899788.0, -65899790.0, -65899796.0, -65899800.0, -65899804.0, -65899810.0, -65899812.0, -65899816.0, -65899820.0, -65899824.0, -65899828.0, -65899830.0, -65899836.0, -65899840.0, -65899844.0, -65899850.0, -65899852.0, -65899856.0, -65899860.0, -65899864.0, -65899868.0, -65899870.0, -65899876.0, -65899880.0, -65899884.0, -65899890.0, -65899892.0, -65899896.0, -65899900.0, -65899904.0, -65899908.0, -65899910.0, -65899916.0, -65899920.0, -65899924.0, -65899930.0, -65899932.0, -65899936.0, -65899940.0, -65899944.0, -65899948.0, -65899950.0, -65899956.0, -65899960.0, -65899964.0, -65899970.0, -65899972.0, -65899976.0, -65899980.0, -65899984.0, -65899988.0, -65899990.0, -65899996.0, -65900000.0, -65900004.0, -65900010.0, -65900012.0, -65900016.0, -65900020.0, -65900024.0, -65900028.0, -65900030.0, -65900036.0, -65900040.0, -65900044.0, -65900050.0, -65900052.0, -65900056.0, -65900060.0, -65900064.0, -65900068.0, -65900070.0, -65900076.0, -65900080.0, -65900084.0, -65900090.0, -65900092.0, -65900096.0, -65900100.0, -65900104.0, -65900108.0, -65900110.0, -65900116.0, -65900120.0, -65900124.0, -65900130.0, -65900132.0, -65900136.0, -65900140.0, -65900144.0, -65900148.0, -65900150.0, -65900156.0, -65900160.0, -65900164.0, -65900170.0, -65900172.0, -65900176.0, -65900180.0, -65900184.0, -65900188.0, -65900190.0, -65900196.0, -65900200.0, -65900204.0, -65900210.0, -65900212.0, -65900216.0, -65900220.0, -65900224.0, -65900228.0, -65900230.0, -65900236.0, -65900240.0, -65900244.0, -65900250.0, -65900252.0, -65900256.0, -65900260.0, -65900264.0, -65900268.0, -65900270.0, -65900276.0, -65900280.0, -65900284.0, -65900290.0, -65900292.0, -65900296.0, -65900300.0, -65900304.0, -65900308.0, -65900310.0, -65900316.0, -65900320.0, -65900324.0, -65900330.0, -65900332.0, -65900336.0, -65900340.0, -65900344.0, -65900348.0, -65900350.0, -65900356.0, -65900360.0, -65900364.0, -65900370.0, -65900372.0, -65900376.0, -65900380.0, -65900384.0, -65900388.0, -65900390.0, -65900396.0, -65900400.0, -65900404.0, -65900410.0, -65900412.0, -65900416.0, -65900420.0, -65900424.0, -65900428.0, -65900430.0, -65900436.0, -65900440.0, -65900444.0, -65900450.0, -65900452.0, -65900456.0, -65900460.0, -65900464.0, -65900468.0, -65900470.0, -65900476.0, -65900480.0, -65900484.0, -65900490.0, -65900492.0, -65900496.0, -65900500.0, -65900504.0, -65900508.0, -65900510.0, -65900516.0, -65900520.0, -65900524.0, -65900530.0, -65900532.0, -65900536.0, -65900540.0, -65900544.0, -65900548.0, -65900550.0, -65900556.0, -65900560.0, -65900564.0, -65900570.0, -65900572.0, -65900576.0, -65900580.0, -65900584.0, -65900588.0, -65900590.0, -65900596.0, -65900600.0, -65900604.0, -65900610.0, -65900612.0, -65900616.0, -65900620.0, -65900624.0, -65900628.0, -65900630.0, -65900636.0, -65900640.0, -65900644.0, -65900650.0, -65900652.0, -65900656.0, -65900660.0, -65900664.0, -65900668.0, -65900670.0, -65900676.0, -65900680.0, -65900684.0, -65900690.0, -65900732.0, -65900736.0, -65900740.0, -65900744.0, -65900748.0, -65900750.0, -65900756.0, -65900760.0, -65902730.0, -65902736.0, -65902740.0, -65902744.0, -65902748.0, -65902750.0, -65902756.0, -65902760.0, -65902764.0, -65902770.0, -65902772.0, -65902776.0, -65902780.0, -65902784.0, -65902788.0, -65902790.0, -65902796.0, -65902800.0, -65902804.0, -65902810.0, -65902812.0, -65902816.0, -65902820.0, -65902824.0, -65902828.0, -65902830.0, -65902836.0, -65902840.0, -65902844.0, -65902850.0, -65902852.0, -65902856.0, -65902860.0, -65902864.0, -65902868.0, -65902870.0, -65902876.0, -65902880.0, -65902884.0, -65902890.0, -65902892.0, -65902896.0, -65902900.0, -65902904.0, -65902908.0, -65902910.0, -65902916.0, -65902920.0, -65902924.0, -65902930.0, -65902932.0, -65902936.0, -65902940.0, -65902944.0, -65902948.0, -65902950.0, -65902956.0, -65902960.0, -65902964.0, -65902970.0, -65902972.0, -65902976.0, -65902980.0, -65902984.0, -65902988.0, -65902990.0, -65902996.0, -65903000.0, -65903004.0, -65903010.0, -65903012.0, -65903016.0, -65903020.0, -65903024.0, -65903028.0, 27086648.0, 27086668.0, 27086710.0, 27086712.0, 27086716.0, 27086718.0, 27086720.0, 27086722.0, 27086724.0, 27086726.0, 27086728.0, 27086730.0, 27086732.0, 27086734.0, 27086736.0, 27086738.0, 27086740.0, 27086742.0, 27086744.0, 27086746.0, 27086748.0, 27086750.0, 27086752.0, 27086754.0, 27086756.0, 27086758.0, 27086760.0, 27086762.0, 27086764.0, 27086766.0, 27086768.0, 27086770.0, 27086772.0, 27086774.0, 27086776.0, 27086778.0, 27086780.0, 27086782.0, 27086784.0, 27086786.0, 27086788.0, 27086790.0, 27086792.0, 27086794.0, 27086796.0, 27086798.0, 27086800.0, 27086802.0, 27086804.0, 27086806.0, 27086808.0, 27086810.0, 27086812.0, 27086814.0, 27086816.0, 27086818.0, 27086820.0, 27086822.0, 27086824.0, 27086826.0, 27086828.0, 27086830.0, 27086832.0, 27086834.0, 27086836.0, 27086838.0, 27086840.0, 27086842.0, 27086844.0, 27086846.0, 27086848.0, 27086850.0, 27086852.0, 27086854.0, 27086856.0, 27086858.0, 27086860.0, 27086862.0, 27086864.0, 27086866.0, 27086868.0, 27086870.0, 27086872.0, 27086874.0, 27086876.0, 27086878.0, 27086880.0, 27086882.0, 27086884.0, 27086886.0, 27086888.0, 27086890.0, 27086892.0, 27086894.0, 27086896.0, 27086898.0, 27086900.0, 27086902.0, 27086904.0, 27086906.0, 27086908.0, 27086910.0, 27086912.0, 27086914.0, 27086916.0, 27086918.0, 27086920.0, 27086922.0, 27086924.0, 27086926.0, 27086928.0, 27086930.0, 27086932.0, 27086934.0, 27086936.0, 27086938.0, 27086940.0, 27086942.0, 27086944.0, 27086946.0, 27086948.0, 27086950.0, 27086952.0, 27086954.0, 27086956.0, 27086958.0, 27086960.0, 27086962.0, 27086964.0, 27086966.0, 27086968.0, 27086970.0, 27086972.0, 27086974.0, 27086976.0, 27086978.0, 27086980.0, 27086982.0, 27086984.0, 27086986.0, 27086988.0, 27086990.0, 27086992.0, 27086994.0, 27086996.0, 27086998.0, 27087000.0, 27087002.0, 27087004.0, 27087006.0, 27087008.0, 27087010.0, 27087012.0, 27087014.0, 27087016.0, 27087018.0, 27087020.0, 27087022.0, 27087024.0, 27087026.0, 27087028.0, 27087030.0, 27087032.0, 27087034.0, 27087036.0, 27087038.0, 27087040.0, 27087042.0, 27087044.0, 27087046.0, 27087048.0, 27087050.0, 27087052.0, 27087054.0, 27087056.0, 27087058.0, 27087060.0, 27087062.0, 27087064.0, 27087066.0, 27087068.0, 27087070.0, 27087072.0, 27087074.0, 27087076.0, 27087078.0, 27087080.0, 27087082.0, 27087084.0, 27087086.0, 27087088.0, 27087090.0, 27087092.0, 27087094.0, 27087096.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8660919834245664\n",
      "Hamming Loss: 0.06775975853072083\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93     34183\n",
      "           1       0.00      0.00      0.00      4911\n",
      "\n",
      "    accuracy                           0.87     39094\n",
      "   macro avg       0.44      0.50      0.47     39094\n",
      "weighted avg       0.76      0.87      0.82     39094\n",
      "\n",
      "y_pred shape: \n",
      "(39094, 2)\n",
      "y_pred2 shape:\n",
      "(39094, 2)\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89         4\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Hamming Loss: 0.0891124985526341\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "y_pred3, pred3, y_test3, hl3 = list(),list(),list(),list()\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(1)\n",
    "  #]) \n",
    "    \n",
    "  #inputs = tf.keras.layers.Input(shape=(X_train.shape))\n",
    "  #outputs = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "  #model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "\n",
    "  #outputs = keras.layers.Dense(tf.keras.layers.Dense(number_of_classes)) #(x) \n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(2)\n",
    "  #])   \n",
    "  \n",
    "  #model = tf.contrib.learn.DNNClassifier(hidden_units=[5,10,5],\n",
    "  #                                          n_classes=4)    \n",
    "    \n",
    "\n",
    "  inputs = tf.keras.Input(shape=(X_train.shape))\n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    \n",
    "  outputs = tf.keras.layers.Dense(number_of_classes)  #, activation=activation)\n",
    "                             #kernel_initializer=initializer,\n",
    "                             #activation=activation) #(x) \n",
    "  #model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(2)\n",
    "  ]) \n",
    "    \n",
    "  #model.add(layers.Activation(activations.relu))\n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  #model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.001))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  #print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "    \n",
    "  #y_pred2.append(y_pred)\n",
    "  y_pred3 = np.append(y_pred3, y_pred2)\n",
    "  #pred2.append(pred)\n",
    "  pred3 = np.append(pred3, pred2)\n",
    "  #y_test2.append(y_test)\n",
    "  y_test3 = np.append(y_test3, y_test2)\n",
    "  #hl2.append(hl)\n",
    "  hl3 = np.append(hl3, hl)\n",
    "    \n",
    "print(\"y_pred shape: \")\n",
    "print(np.shape(y_pred))\n",
    "\n",
    "print(\"y_pred2 shape:\")\n",
    "print(np.shape(y_pred2))\n",
    "\n",
    "y_pred3 = np.concatenate((y_pred3[0], y_pred3[1], y_pred3[2], y_pred3[3], y_pred3[4]), axis=None)\n",
    "pred3 = np.concatenate((pred3[0], pred3[1], pred3[2], pred3[3], pred3[4]), axis=None)\n",
    "y_test3 = np.concatenate((y_test3[0], y_test3[1], y_test3[2], y_test3[3], y_test3[4]), axis=None)\n",
    "hl3 = np.concatenate((hl3[0], hl3[1], hl3[2], hl3[3], hl3[4]), axis=None)\n",
    "\n",
    "print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\")\n",
    "#print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "print(classification_report(y_test3, pred3))\n",
    "hl3_avg = sum(hl3) / len(hl3)\n",
    "print(\"Hamming Loss: \" + str(hl3_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5040, 85)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_KGFS75.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 460us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 158us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 156us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 169us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 176us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 158us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 149us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 113us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 158us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 123us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 116us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 123us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 120us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 122us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 114us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 114us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 120us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 115us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 117us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 121us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 130us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 120us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 162us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 162us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 161us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 1s 182us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 1s 183us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 117us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 133us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - ETA: 0s - loss: 6.0751 - binary_accuracy: 0.603 - 0s 138us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 164us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 117us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 116us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 6.0413 - binary_accuracy: 0.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [235401780.0, 235402600.0, 235402820.0, 235405920.0, 235405970.0, 235408640.0, 235408670.0, 235409010.0, 235411100.0, 235412430.0, 235413660.0, 235413940.0, 235414400.0, 235415500.0, 235415800.0, 235416590.0, 235416770.0, 235417000.0, 235417060.0, 235417360.0, 235417470.0, 235418000.0, 235418480.0, 235418780.0, 235421140.0, 235421220.0, 235421540.0, 235422220.0, 235422370.0, 235422380.0, 235423100.0, 235423820.0, 235424350.0, 235424400.0, 235424910.0, 235425000.0, 235425490.0, 235426140.0, 235427100.0, 235428050.0, 235490450.0, 235490770.0, 235491780.0, 235492560.0, 235492590.0, 235492740.0, 235493340.0, 235493700.0, 235494240.0, 235494270.0, 235495140.0, 235495420.0, 235497090.0, 235497230.0, 235497280.0, 235498500.0, 235498560.0, 235498620.0, 235498780.0, 235499090.0, 235499460.0, 235500770.0, 235500780.0, 235501120.0, 235501570.0, 235502270.0, 235502850.0, 235503840.0, 235504180.0, 235504320.0, 235505700.0, 235505760.0, 235506020.0, 235507250.0, 235507490.0, 235510620.0, 235511870.0, 235512060.0, 235512130.0, 235512930.0, 235513020.0, 235513180.0, 235513680.0, 235513730.0, 235513890.0, 235513900.0, 235513980.0, 235514110.0, 235514400.0, 235514560.0, 235514690.0, 235576740.0, 235577300.0, 235577680.0, 235577800.0, 235577840.0, 235578100.0, 235578320.0, 235578400.0, 235578430.0, 235578690.0, 235578880.0, 235579310.0, 235580100.0, 235580110.0, 235580160.0, 235580770.0, 235581100.0, 235581390.0, 235581760.0, 235581920.0, 235582000.0, 235582020.0, 235582610.0, 235582640.0, 235582670.0, 235582780.0, 235582880.0, 235583170.0, 235585220.0, 235586340.0, 235586580.0, 235587840.0, 235588320.0, 235588670.0, 235589010.0, 235589400.0, 235590110.0, 235590200.0, 235590940.0, 235593780.0, 235594560.0, 235594880.0, 235595360.0, 235595780.0, 235596420.0, 235597060.0, 235597180.0, 235597570.0, 235597660.0, 235598300.0, 235598340.0, 235599260.0, 235600180.0, 235600930.0, 235601220.0, 235662530.0, 235663170.0, 235663840.0, 235664000.0, 235664210.0, 235664340.0, 235665000.0, 235665010.0, 235665070.0, 235665810.0, 235665950.0, 235665970.0, 235666290.0, 235666350.0, 235666540.0, 235666900.0, 235666980.0, 235667020.0, 235667100.0, 235667260.0, 235667400.0, 235667870.0, 235668340.0, 235669580.0, 235670560.0, 235670800.0, 235671580.0, 235672100.0, 235672380.0, 235672540.0, 235673280.0, 235673660.0, 235673980.0, 235674370.0, 235674380.0, 235674940.0, 235677950.0, 235678620.0, 235679140.0, 235680130.0, 235681420.0, 235682140.0, 235682610.0, 235682830.0, 235683420.0, 235683800.0, 235683870.0, 235684590.0, 235684930.0, 235685980.0, 235687840.0, 235688060.0, 235749000.0, 235749090.0, 235749540.0, 235749680.0, 235750140.0, 235750420.0, 235751100.0, 235751490.0, 235752030.0, 235752960.0, 235753400.0, 235753950.0, 235754240.0, 235754270.0, 235755060.0, 235755180.0, 235755760.0, 235755780.0, 235756380.0, 235757340.0, 235757360.0, 235757780.0, 235758340.0, 235758850.0, 235758930.0, 235759710.0, 235759780.0, 235760510.0, 235760530.0, 235761040.0, 235762720.0, 235763300.0, 235767100.0, 235767360.0, 235767460.0, 235767630.0, 235767710.0, 235767780.0, 235768000.0, 235768370.0, 235768560.0, 235769630.0, 235770670.0, 235773120.0, 235773890.0, 235773900.0, 235774530.0, 235835420.0, 235835780.0, 235836220.0, 235836590.0, 235837060.0, 235837400.0, 235837440.0, 235837740.0, 235838320.0, 235838460.0, 235838700.0, 235838940.0, 235840590.0, 235840660.0, 235840670.0, 235840820.0, 235840900.0, 235841570.0, 235841980.0, 235842530.0, 235842600.0, 235842720.0, 235842820.0, 235842940.0, 235843040.0, 235843170.0, 235843220.0, 235843410.0, 235843520.0, 235843980.0, 235845020.0, 235845860.0, 235846750.0, 235846880.0, 235847680.0, 235849470.0, 235849920.0, 235850060.0, 235851870.0, 235851980.0, 235852690.0, 235852740.0, 235852770.0, 235852800.0, 235852930.0, 235853460.0, 235855300.0, 235856100.0, 235858080.0, 235858640.0, 235860740.0, 235860770.0, 235922640.0, 235923140.0, 235923310.0, 235923340.0, 235925660.0, 235925680.0, 235926000.0, 235926020.0, 235926480.0, 235927550.0, 235927650.0, 235928060.0, 235928210.0, 235928300.0, 235928480.0, 235929780.0, 235930030.0, 235930110.0, 235930220.0, 235930300.0, 235930780.0, 235932160.0, 235932770.0, 235933470.0, 235934260.0, 235935100.0, 235935440.0, 235935470.0, 235935600.0, 235936130.0, 235936320.0, 235936350.0, 235936850.0, 235936880.0, 235937250.0, 235937260.0, 235937700.0, 235937860.0, 235937890.0, 235937900.0, 235938110.0, 235938240.0, 235939420.0, 235939540.0, 235939680.0, 235940060.0, 235941020.0, 235941220.0, 235941600.0, 235942080.0, 235942160.0, 235942220.0, 235942530.0, 235942780.0, 235942880.0, 235943000.0, 235943090.0, 235943300.0, 235943360.0, 235943550.0, 235943780.0, 235943820.0, 235944080.0, 235944100.0, 235944670.0, 235944900.0, 235946430.0, 235946690.0, 236008110.0, 236009810.0, 236012300.0, 236012350.0, 236013360.0, 236013400.0, 236014220.0, 236014820.0, 236014900.0, 236015820.0, 236015860.0, 236015890.0, 236015920.0, 236016450.0, 236016530.0, 236016580.0, 236016600.0, 236017470.0, 236018300.0, 236019650.0, 236019660.0, 236020240.0, 236021010.0, 236021020.0, 236021060.0, 236021090.0, 236021310.0, 236021360.0, 236021920.0, 236022750.0, 236022850.0, 236023440.0, 236023580.0, 236023740.0, 236024350.0, 236025170.0, 236025310.0, 236025440.0, 236025860.0, 236026060.0, 236026450.0, 236026670.0, 236026820.0, 236027140.0, 236027330.0, 236027490.0, 236027900.0, 236028100.0, 236028160.0, 236028720.0, 236028740.0, 236028900.0, 236029150.0, 236029620.0, 236029660.0, 236030110.0, 236030900.0, 236030910.0, 236031680.0, 236031710.0, 236031740.0, 236031840.0, 236031870.0, 236032240.0, 236032270.0, 236032350.0, 236033500.0, 236033920.0, 236095600.0, 236095620.0, 236096800.0, 236097280.0, 236098300.0, 236098380.0, 236098670.0, 236098960.0, 236099300.0, 236100140.0, 236100210.0, 236100910.0, 236101100.0, 236101800.0, 236102780.0, 236102960.0, 236103070.0, 236103550.0, 236104420.0, 236104560.0, 236105360.0, 236105400.0, 236106240.0, 236107040.0, 236107740.0, 236107780.0, 236107940.0, 236108930.0, 236109340.0, 236109630.0, 236110220.0, 236111100.0, 236111280.0, 236111360.0, 236111400.0, 236111600.0, 236111920.0, 236111950.0, 236112030.0, 236112140.0, 236112200.0, 236112590.0, 236112780.0, 236112820.0, 236113120.0, 236113630.0, 236113700.0, 236114020.0, 236114110.0, 236114500.0, 236115460.0, 236115520.0, 236115550.0, 236115760.0, 236117250.0, 236117310.0, 236118080.0, 236120820.0, 236121070.0, 236121470.0, 236181120.0, 236182960.0, 236183180.0, 236183800.0, 236184140.0, 236185090.0, 236185470.0, 236186200.0, 236186240.0, 236187120.0, 236187860.0, 236187900.0, 236188240.0, 236188960.0, 236189020.0, 236189220.0, 236189520.0, 236189550.0, 236189870.0, 236190080.0, 236190560.0, 236190860.0, 236190880.0, 236191260.0, 236191460.0, 236191570.0, 236191740.0, 236192320.0, 236192340.0, 236192430.0, 236192540.0, 236192980.0, 236193090.0, 236193170.0, 236194960.0, 236195000.0, 236195200.0, 236195660.0, 236196860.0, 236196900.0, 236197400.0, 236199170.0, 236199280.0, 236200460.0, 236202750.0, 236202900.0, 236202940.0, 236203260.0, 236203710.0, 236204110.0, 236205040.0, 236205060.0, 236205100.0, 236205220.0, 236206080.0, 236206350.0, 236206830.0, 236207870.0, 236208770.0, 236270140.0, 236270220.0, 236270910.0, 236271920.0, 236272820.0, 236272900.0, 236273250.0, 236273340.0, 236274660.0, 236275230.0, 236276260.0, 236276350.0, 236276740.0, 236277040.0, 236278080.0, 236278430.0, 236278720.0, 236280540.0, 236280800.0, 236281570.0, 236281660.0, 236282080.0, 236282240.0, 236282270.0, 236282980.0, 236283120.0, 236283360.0, 236283400.0, 236283790.0, 236283820.0, 236284000.0, 236284380.0, 236284740.0, 236284770.0, 236284880.0, 236285060.0, 236285250.0, 236285340.0, 236285380.0, 236285630.0, 236285680.0, 236285890.0, 236285940.0, 236286300.0, 236286500.0, 236286910.0, 236287260.0, 236287470.0, 236288080.0, 236288300.0, 236288320.0, 236288700.0, 236288770.0, 236288820.0, 236288900.0, 236289180.0, 236289300.0, 236289760.0, 236289800.0, 236290910.0, 236291570.0, 236291780.0, 236292260.0, 236292320.0, 236292560.0, 236292590.0, 236292670.0, 236293400.0, 236293540.0, 236294100.0, 236294430.0, 236354110.0, 236354690.0, 236354720.0, 236354820.0, 236354850.0, 236354880.0, 236355040.0, 236355260.0, 236355410.0, 236355420.0, 236355570.0, 236355650.0, 236356030.0, 236356080.0, 236356580.0, 236356690.0, 236357150.0, 236357570.0, 236357860.0, 236357890.0, 236357920.0, 236359820.0, 236359840.0, 236359940.0, 236360510.0, 236360580.0, 236360600.0, 236360640.0, 236360850.0, 236360930.0, 236363220.0, 236363300.0, 236365980.0, 236367440.0, 236367520.0, 236367550.0, 236369180.0, 236369300.0, 236369340.0, 236369400.0, 236371840.0, 236371890.0, 236371900.0, 236371920.0, 236371980.0, 236372690.0, 236372830.0, 236372860.0, 236373220.0, 236373980.0, 236374430.0, 236374850.0, 236375460.0, 236377250.0, 236379200.0, 52762850.0, 52762984.0, 52763120.0, 52763576.0, 52764130.0, 52764210.0, 52764344.0, 52765064.0, 52765264.0, 52765504.0, 52765576.0, 52765704.0, 52765750.0, 52765920.0, 52765970.0, 52765984.0, 52766370.0, 52766550.0, 52767144.0, 52767190.0, 52767490.0, 52767520.0, 52767710.0, 52767784.0, 52767896.0, 52767990.0, 52768104.0, 52768184.0, 52768240.0, 52768730.0, 52768736.0, 52768770.0, 52768890.0, 52768896.0, 52769056.0, 52769176.0, 52769384.0, 52769390.0, 52781040.0, 52781456.0, 52781704.0, 52781736.0, 52781930.0, 52782024.0, 52782296.0, 52782480.0, 52782850.0, 52782910.0, 52782990.0, 52783150.0, 52783200.0, 52783250.0, 52783256.0, 52783320.0, 52783350.0, 52783424.0, 52783640.0, 52783690.0, 52783784.0, 52784000.0, 52784120.0, 52784170.0, 52784256.0, 52784464.0, 52784520.0, 52784570.0, 52784624.0, 52785064.0, 52785344.0, 52785390.0, 52785490.0, 52785520.0, 52785630.0, 52785680.0, 52785736.0, 52785776.0, 52785810.0, 52785870.0, 52785896.0, 52785910.0, 52786030.0, 52786064.0, 52786304.0, 52786416.0, 52787056.0, 52787536.0, 52787736.0, 52797360.0, 52798864.0, 52798920.0, 52798976.0, 52799070.0, 52799120.0, 52799130.0, 52799240.0, 52799250.0, 52799280.0, 52799576.0, 52799760.0, 52799776.0, 52799990.0, 52800130.0, 52800616.0, 52800930.0, 52800950.0, 52801250.0, 52801264.0, 52801310.0, 52801320.0, 52801456.0, 52801464.0, 52801616.0, 52801624.0, 52801630.0, 52801710.0, 52801760.0, 52801776.0, 52801816.0, 52801824.0, 52802216.0, 52802464.0, 52802530.0, 52802590.0, 52802600.0, 52802624.0, 52802830.0, 52802976.0, 52803000.0, 52803040.0, 52803090.0, 52803120.0, 52803770.0, 52804030.0, 52804416.0, 52804560.0, 52804704.0, 52805024.0, 52805056.0, 52805184.0, 52805456.0, 52805480.0, 52814496.0, 52816710.0, 52817104.0, 52817240.0, 52817264.0, 52817290.0, 52817304.0, 52817330.0, 52817390.0, 52817430.0, 52817560.0, 52817830.0, 52817930.0, 52818120.0, 52818256.0, 52818304.0, 52818320.0, 52818384.0, 52818400.0, 52818416.0, 52818456.0, 52818490.0, 52818550.0, 52818616.0, 52818624.0, 52818664.0, 52818690.0, 52818750.0, 52818760.0, 52818840.0, 52818864.0, 52818896.0, 52818910.0, 52818930.0, 52818960.0, 52819024.0, 52819104.0, 52819750.0, 52819770.0, 52819976.0, 52820050.0, 52820350.0, 52820456.0, 52820470.0, 52820480.0, 52820530.0, 52820584.0, 52820800.0, 52820896.0, 52820984.0, 52821010.0, 52821120.0, 52821350.0, 52821590.0, 52821616.0, 52834170.0, 52834310.0, 52834350.0, 52834510.0, 52834576.0, 52834704.0, 52834816.0, 52834976.0, 52835264.0, 52835280.0, 52835416.0, 52835656.0, 52835840.0, 52835856.0, 52835870.0, 52835936.0, 52836104.0, 52836176.0, 52836296.0, 52836344.0, 52836464.0, 52836590.0, 52836880.0, 52836990.0, 52837030.0, 52837040.0, 52837136.0, 52837224.0, 52837480.0, 52837690.0, 52837930.0, 52837960.0, 52838016.0, 52838080.0, 52838090.0, 52838096.0, 52838130.0, 52838150.0, 52838240.0, 52838330.0, 52838490.0, 52838670.0, 52839080.0, 52839270.0, 52839304.0, 52839344.0, 52839720.0, 52839736.0, 52852016.0, 52852160.0, 52852510.0, 52853120.0, 52853150.0, 52853410.0, 52853416.0, 52853504.0, 52853510.0, 52853590.0, 52853950.0, 52853970.0, 52854176.0, 52854200.0, 52854224.0, 52854270.0, 52854304.0, 52854350.0, 52854400.0, 52854450.0, 52854750.0, 52854800.0, 52854850.0, 52855090.0, 52855096.0, 52855130.0, 52855136.0, 52855160.0, 52855216.0, 52855250.0, 52855336.0, 52855370.0, 52855376.0, 52855400.0, 52855520.0, 52855584.0, 52855616.0, 52855920.0, 52856000.0, 52856496.0, 52856530.0, 52856560.0, 52856776.0, 52856824.0, 52857050.0, 52857170.0, 52857190.0, 52857736.0, 52857750.0, 52857920.0, 52857930.0, 52858000.0, 52869890.0, 52870030.0, 52870920.0, 52871704.0, 52871736.0, 52872130.0, 52872304.0, 52872370.0, 52872376.0, 52872690.0, 52872840.0, 52872850.0, 52872910.0, 52872920.0, 52873016.0, 52873090.0, 52873120.0, 52873190.0, 52873216.0, 52873270.0, 52873376.0, 52873424.0, 52873430.0, 52873530.0, 52873584.0, 52873590.0, 52873704.0, 52873850.0, 52874150.0, 52874240.0, 52874330.0, 52874370.0, 52874384.0, 52874630.0, 52874656.0, 52874664.0, 52874800.0, 52874864.0, 52875024.0, 52875216.0, 52875224.0, 52875350.0, 52875424.0, 52875440.0, 52875456.0, 52875624.0, 52875696.0, 52875750.0, 52875790.0, 52875816.0, 52875930.0, 52876210.0, 52876224.0, 52876350.0, 52876384.0, 52876464.0, 52876624.0, 52876630.0, 52876664.0, 52876704.0, 52876770.0, 52876816.0, 52877104.0, 52877200.0, 52887750.0, 52888504.0, 52888544.0, 52888904.0, 52888944.0, 52889030.0, 52889056.0, 52889270.0, 52889656.0, 52889850.0, 52889904.0, 52890090.0, 52890370.0, 52890384.0, 52890456.0, 52890520.0, 52890576.0, 52890664.0, 52890704.0, 52890710.0, 52890776.0, 52890870.0, 52891030.0, 52891096.0, 52891150.0, 52891176.0, 52891216.0, 52891360.0, 52891370.0, 52891496.0, 52891504.0, 52891520.0, 52891530.0, 52891584.0, 52891600.0, 52891790.0, 52891920.0, 52891950.0, 52892150.0, 52892360.0, 52892496.0, 52892504.0, 52892520.0, 52892640.0, 52892880.0, 52892930.0, 52892960.0, 52893016.0, 52893120.0, 52893230.0, 52893456.0, 52893470.0, 52893696.0, 52893710.0, 52893776.0, 52893810.0, 52893976.0, 52893984.0, 52894000.0, 52894024.0, 52894030.0, 52894040.0, 52894080.0, 52894224.0, 52894336.0, 52894730.0, 52895010.0, 52895080.0, 52897920.0, 52899616.0, 52899744.0, 52899760.0, 52899776.0, 52899824.0, 52899870.0, 52899970.0, 52899990.0, 52900136.0, 52900176.0, 52900290.0, 52900336.0, 52900370.0, 52900376.0, 52900384.0, 52900490.0, 52900504.0, 52900550.0, 52900560.0, 52900656.0, 52902336.0, 52902424.0, 52902440.0, 52902496.0, 52902504.0, 52902520.0, 52902530.0, 52902536.0, 52902550.0, 52902570.0, 52902610.0, 52902630.0, 52902650.0, 52902720.0, 52902816.0, 52902840.0, 52902864.0, 52902896.0, 52902904.0, 52902910.0, 52902920.0, 52902944.0, 52902970.0, 52902976.0, 52903000.0, 52903010.0, 52903024.0, 52903040.0, 52903070.0, 52903080.0, 52903090.0, 52903136.0, 52903160.0, 52903190.0, 52903240.0, 52903250.0, 52903320.0, 52903330.0, 52903376.0, 52903384.0, 52903424.0, 52903450.0, 52903464.0, 52903490.0, 52903504.0, 52903520.0, 52903570.0, 52903576.0, 52903616.0, 52903670.0, 52903696.0, 52903704.0, 52903790.0, 52903864.0, 52903904.0, 52904030.0, 52904690.0, 52904904.0, 52905010.0, 52905056.0, 52905110.0, 52905240.0, 52905250.0, 52905264.0, 52905290.0, 52905296.0, 52905310.0, 52905350.0, 52905376.0, 52905410.0, 52905424.0, 52905440.0, 52905510.0, 52905520.0, 52905576.0, 52905710.0, 52905720.0, 52905760.0, 52905800.0, 52905856.0, 52905984.0, 52906000.0, 52906024.0, 52906056.0, 52906064.0, 52906104.0, 52906310.0, 52906330.0, 52906336.0, 52906370.0, 52906390.0, 52906410.0, 52906416.0, 52906424.0, 52906470.0, 52906520.0, 52906544.0, 52906704.0, 52906830.0, 52906910.0, 52906920.0, 52906960.0, 52906984.0, 52907016.0, 52907056.0, 52907080.0, 52907110.0, 52907120.0, 52907176.0, 52907184.0, 52907190.0, 52907216.0, 52907230.0, 52907264.0, 52907270.0, 52907304.0, 52907330.0, 52907360.0, 52907424.0, 52907430.0, 52907450.0, 52907520.0, 52907664.0, 52907750.0, 52907760.0, 52907770.0, 52907870.0, 52908050.0, 52908056.0, 52908240.0, 52908296.0, 52908310.0, 52908320.0, 52908384.0, 52908390.0, 52908464.0, 52908510.0, 52908536.0, 52908600.0, 52908630.0, 52908670.0, 52908690.0, 52908696.0, 52908710.0, 52908736.0, 52908760.0, 52908816.0, 52908840.0, 52908870.0, 52908880.0, 52908890.0, 52908910.0, 52908990.0, 52909010.0, 52909016.0, 52909096.0, 52909104.0, 52909160.0, 52909176.0, 52909224.0, 52909230.0, 52909270.0, 52909310.0, 52909336.0, 52909440.0, 52909480.0, 52909496.0, 52909504.0, 52909536.0, 52909550.0, 52909576.0, 52909650.0, 52909696.0, 52909720.0, 52909730.0, 52909750.0, 52909776.0, 52909824.0, 52909830.0, 52909944.0, 52909960.0, 52909990.0, 52910000.0, 52910064.0, 52910144.0, 52910184.0, 52910264.0, 52910304.0, 52910310.0, 52910400.0, 52910430.0, 52910520.0, 52910530.0, 52910776.0, 52910856.0, 52910976.0, 52911136.0, 52911290.0, 52911430.0, 52911830.0, 52912210.0, 52912256.0, 52912344.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n",
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8210227272727273\n",
      "Hamming Loss: 0.08948863636363637\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       578\n",
      "           1       0.18      1.00      0.30       126\n",
      "\n",
      "    accuracy                           0.18       704\n",
      "   macro avg       0.09      0.50      0.15       704\n",
      "weighted avg       0.03      0.18      0.05       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 2s 634us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 155us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 133us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 117us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 130us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 122us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 129us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 117us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 132us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 176us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 167us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 171us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 154us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 160us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 1s 248us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 120us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 122us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 2s 661us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 114us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 163us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 160us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 176us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 1s 191us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 1s 183us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 1s 292us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 1s 201us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 1s 182us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 1s 278us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 1s 246us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 1s 189us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 1s 212us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 1s 194us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 1s 180us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 159us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 1s 258us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 155us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 158us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 166us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 149us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 1s 178us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 1s 211us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 156us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 157us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 149us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 168us/sample - loss: 9.2818 - binary_accuracy: 0.3983s - loss: 9.2791 - binary_accuracy: 0.3\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 148us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 149us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 162us/sample - loss: 9.2818 - binary_accuracy: 0.3983s - loss: 9.2726 - binary_accuracy: 0.39\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 162us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 165us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 1s 194us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 165us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 2s 738us/sample - loss: 9.2818 - binary_accuracy: 0.3983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-148599410.0, -148599570.0, -148599710.0, -148600660.0, -148601040.0, -148601100.0, -148601170.0, -148602050.0, -148602990.0, -148603000.0, -148603020.0, -148603490.0, -148603760.0, -148604130.0, -148604140.0, -148604430.0, -148604780.0, -148604980.0, -148605500.0, -148605620.0, -148605650.0, -148606050.0, -148606290.0, -148606990.0, -148607780.0, -148608350.0, -148608420.0, -148608480.0, -148608580.0, -148608600.0, -148608770.0, -148609200.0, -148609250.0, -148609300.0, -148609380.0, -148609400.0, -148609420.0, -148609570.0, -148609680.0, -148609700.0, -148609920.0, -148610200.0, -148610450.0, -148610900.0, -148610910.0, -148611170.0, -148611300.0, -148611710.0, -148611820.0, -148611940.0, -148612200.0, -148612260.0, -148612430.0, -148613390.0, -148613820.0, -148650480.0, -148650660.0, -148650740.0, -148650900.0, -148651070.0, -148651090.0, -148651150.0, -148651620.0, -148651650.0, -148651800.0, -148652260.0, -148652540.0, -148653380.0, -148654610.0, -148654720.0, -148654780.0, -148654850.0, -148655260.0, -148655280.0, -148655520.0, -148655570.0, -148655900.0, -148655980.0, -148656130.0, -148656240.0, -148656340.0, -148656400.0, -148657180.0, -148657870.0, -148657920.0, -148658290.0, -148658380.0, -148658990.0, -148659220.0, -148659310.0, -148659550.0, -148659800.0, -148659820.0, -148659870.0, -148659900.0, -148659950.0, -148660220.0, -148660260.0, -148660820.0, -148661540.0, -148661700.0, -148661820.0, -148661900.0, -148662290.0, -148662850.0, -148663470.0, -148663860.0, -148663900.0, -148663920.0, -148664060.0, -148664270.0, -148664880.0, -148700600.0, -148700740.0, -148701200.0, -148701650.0, -148701730.0, -148702320.0, -148702540.0, -148702660.0, -148703090.0, -148703150.0, -148703200.0, -148703250.0, -148703260.0, -148703280.0, -148703540.0, -148703550.0, -148703740.0, -148703940.0, -148704140.0, -148704210.0, -148704240.0, -148704270.0, -148704300.0, -148704320.0, -148704690.0, -148704800.0, -148704860.0, -148704900.0, -148705060.0, -148705140.0, -148705260.0, -148705280.0, -148705460.0, -148705980.0, -148706260.0, -148706450.0, -148707180.0, -148707200.0, -148707220.0, -148707330.0, -148707620.0, -148707700.0, -148707780.0, -148707820.0, -148707900.0, -148708380.0, -148708700.0, -148709200.0, -148709250.0, -148709360.0, -148709400.0, -148709760.0, -148709920.0, -148710260.0, -148710880.0, -148711200.0, -148711600.0, -148711920.0, -148712000.0, -148712140.0, -148712180.0, -148712260.0, -148712960.0, -148713000.0, -148713390.0, -148715490.0, -148715630.0, -148752080.0, -148752670.0, -148753420.0, -148753570.0, -148753710.0, -148753860.0, -148754340.0, -148754350.0, -148754370.0, -148754770.0, -148754800.0, -148754880.0, -148755440.0, -148755620.0, -148755630.0, -148756030.0, -148756770.0, -148756850.0, -148757260.0, -148757280.0, -148757440.0, -148757980.0, -148758000.0, -148758270.0, -148758860.0, -148759330.0, -148759780.0, -148760030.0, -148760050.0, -148760080.0, -148760620.0, -148761280.0, -148761340.0, -148761660.0, -148761860.0, -148761870.0, -148761920.0, -148761950.0, -148761970.0, -148761980.0, -148762020.0, -148762240.0, -148762380.0, -148762450.0, -148762830.0, -148762960.0, -148763490.0, -148763580.0, -148764100.0, -148764350.0, -148764450.0, -148764500.0, -148764620.0, -148764660.0, -148765250.0, -148765330.0, -148766830.0, -148767040.0, -148767140.0, -148801070.0, -148801330.0, -148803340.0, -148803780.0, -148803790.0, -148803800.0, -148803840.0, -148804030.0, -148804180.0, -148804340.0, -148804480.0, -148804770.0, -148805710.0, -148806020.0, -148806350.0, -148806370.0, -148807410.0, -148807440.0, -148807470.0, -148807550.0, -148807730.0, -148807780.0, -148807940.0, -148808020.0, -148808420.0, -148808820.0, -148809010.0, -148809140.0, -148809340.0, -148809490.0, -148809950.0, -148810140.0, -148810200.0, -148810240.0, -148810450.0, -148810510.0, -148810530.0, -148810660.0, -148810670.0, -148810780.0, -148811140.0, -148811440.0, -148811460.0, -148811550.0, -148811860.0, -148811980.0, -148812020.0, -148812530.0, -148812740.0, -148812820.0, -148812880.0, -148813020.0, -148813380.0, -148813630.0, -148813730.0, -148813890.0, -148814370.0, -148814500.0, -148814530.0, -148814540.0, -148814580.0, -148814660.0, -148814900.0, -148814940.0, -148815060.0, -148815090.0, -148815100.0, -148815200.0, -148815280.0, -148815470.0, -148815490.0, -148815520.0, -148815700.0, -148815870.0, -148816450.0, -148816530.0, -148816940.0, -148853090.0, -148853680.0, -148853870.0, -148854050.0, -148854160.0, -148854260.0, -148855150.0, -148855180.0, -148855230.0, -148855400.0, -148855920.0, -148855980.0, -148856020.0, -148856080.0, -148856850.0, -148857020.0, -148857090.0, -148857260.0, -148857680.0, -148857970.0, -148858060.0, -148858180.0, -148858910.0, -148858990.0, -148859040.0, -148859090.0, -148859220.0, -148859410.0, -148859660.0, -148859760.0, -148860110.0, -148860140.0, -148860290.0, -148860340.0, -148860940.0, -148861330.0, -148861970.0, -148862940.0, -148862980.0, -148865060.0, -148865100.0, -148866050.0, -148868300.0, -148868320.0, -148868720.0, -148906460.0, -148906580.0, -148906700.0, -148907280.0, -148907630.0, -148908020.0, -148908420.0, -148908460.0, -148908540.0, -148909140.0, -148909170.0, -148909340.0, -148909360.0, -148909380.0, -148909550.0, -148910300.0, -148910350.0, -148910380.0, -148910660.0, -148910820.0, -148911150.0, -148911890.0, -148912320.0, -148912540.0, -148912640.0, -148912670.0, -148912770.0, -148913060.0, -148913760.0, -148913950.0, -148914050.0, -148914430.0, -148914560.0, -148914820.0, -148915040.0, -148915140.0, -148915250.0, -148915310.0, -148915400.0, -148915550.0, -148915800.0, -148916240.0, -148916420.0, -148916600.0, -148916750.0, -148917660.0, -148917740.0, -148918400.0, -148955310.0, -148955520.0, -148956740.0, -148957000.0, -148957150.0, -148957220.0, -148958530.0, -148959020.0, -148959410.0, -148959780.0, -148960160.0, -148960240.0, -148960290.0, -148960600.0, -148960700.0, -148961660.0, -148961700.0, -148961840.0, -148962200.0, -148962400.0, -148962600.0, -148963170.0, -148963300.0, -148963400.0, -148963460.0, -148963520.0, -148963650.0, -148963760.0, -148964030.0, -148964060.0, -148964180.0, -148964700.0, -148965280.0, -148965340.0, -148965400.0, -148965540.0, -148965660.0, -148965710.0, -148965740.0, -148965760.0, -148965780.0, -148965810.0, -148965900.0, -148966060.0, -148966200.0, -148966750.0, -148967340.0, -148967400.0, -148969150.0, -148969540.0, -148970460.0, -149010380.0, -149010400.0, -149010670.0, -149010980.0, -149011260.0, -149011310.0, -149011470.0, -149011660.0, -149011740.0, -149011800.0, -149011980.0, -149012020.0, -149012080.0, -149012780.0, -149013000.0, -149013340.0, -149013420.0, -149013500.0, -149013520.0, -149013630.0, -149013700.0, -149013890.0, -149013920.0, -149013970.0, -149014110.0, -149014130.0, -149014140.0, -149014270.0, -149014510.0, -149014770.0, -149014940.0, -149015120.0, -149015180.0, -149015300.0, -149015380.0, -149015490.0, -149015500.0, -149015540.0, -149015680.0, -149015710.0, -149015800.0, -149015940.0, -149016270.0, -149016380.0, -149016420.0, -149016590.0, -149016740.0, -149017120.0, -149017150.0, -149017170.0, -149017680.0, -149018000.0, -149018050.0, -149018080.0, -149018130.0, -149018190.0, -149018220.0, -149018240.0, -149018560.0, -149018640.0, -149019970.0, -149020900.0, -149020960.0, -149054320.0, -149055570.0, -149055840.0, -149056620.0, -149057230.0, -149057440.0, -149057540.0, -149057800.0, -149057890.0, -149058140.0, -149058190.0, -149058300.0, -149058420.0, -149058800.0, -149059060.0, -149059180.0, -149060030.0, -149060100.0, -149060240.0, -149060340.0, -149060400.0, -149060860.0, -149061090.0, -149061490.0, -149061650.0, -149062270.0, -149062340.0, -149062620.0, -149062880.0, -149063330.0, -149063490.0, -149063660.0, -149063710.0, -149063940.0, -149064110.0, -149064210.0, -149064600.0, -149064640.0, -149064670.0, -149065020.0, -149065100.0, -149065170.0, -149066430.0, -149100000.0, -149100740.0, -149101170.0, -149101400.0, -149102860.0, -149102960.0, -149102980.0, -149103280.0, -149103490.0, -149103790.0, -149104830.0, -149104940.0, -149105300.0, -149105580.0, -149106260.0, -149106350.0, -149106400.0, -149106580.0, -149106720.0, -149107420.0, -149107500.0, -149107520.0, -149107870.0, -149107890.0, -149108260.0, -149108300.0, -149108480.0, -149108640.0, -149109010.0, -149109330.0, -149109820.0, -149109870.0, -149109970.0, -149110300.0, -149110340.0, -149110430.0, -149110600.0, -149110640.0, -149110830.0, -149110860.0, -149110900.0, -149110910.0, -149110960.0, -149111100.0, -149111220.0, -149111310.0, -149111380.0, -149111410.0, -149111520.0, -149111570.0, -149111710.0, -149111890.0, -149111920.0, -149112060.0, -149112210.0, -149112290.0, -149112600.0, -149112830.0, -149113000.0, -149113380.0, -149113400.0, -149113470.0, -149113570.0, -149113620.0, -149113800.0, -149113920.0, -149115420.0, -149115460.0, -149148940.0, -149149540.0, -149149940.0, -149149970.0, -149150140.0, -149150620.0, -149151570.0, -149152560.0, -149152590.0, -149152640.0, -149152700.0, -149152720.0, -149152860.0, -149153630.0, -149154110.0, -149154400.0, -149154460.0, -149154510.0, -149154560.0, -149155470.0, -149155890.0, -149156030.0, -149156100.0, -149156770.0, -149157010.0, -149157400.0, -149158130.0, -149158530.0, -149158820.0, -149158880.0, -149158980.0, -149159150.0, -149159330.0, -149159340.0, -149159620.0, -149159730.0, -149160700.0, -149161660.0, -149161890.0, -149161940.0, -156881180.0, -156881380.0, -156882340.0, -156883680.0, -156884740.0, -156884770.0, -156887090.0, -156888220.0, -156888260.0, -156888370.0, -156891740.0, -156891780.0, -156891860.0, -156892900.0, -156892940.0, -156892960.0, -156893000.0, -156894180.0, -156895360.0, -156895400.0, -156896420.0, -156896580.0, -156897570.0, -156897680.0, -156898530.0, -156898850.0, -156899730.0, -156899820.0, -156899890.0, -156901000.0, -156901040.0, -156901200.0, -156902190.0, -156903100.0, -156903260.0, -156903330.0, -156904220.0, -156904530.0, -156905490.0, -156905630.0, -156905650.0, -156905820.0, -156906640.0, -156906850.0, -156906880.0, -156906910.0, -156911380.0, -156911490.0, -156911520.0, -156912740.0, -156912780.0, -156912860.0, -156913810.0, -156913920.0, -156916130.0, -156917380.0, -157000500.0, -157000660.0, -157000670.0, -157000780.0, -157000820.0, -157000860.0, -157001920.0, -157001970.0, -157001980.0, -157002910.0, -157003060.0, -157003120.0, -157004200.0, -157005460.0, -157006480.0, -157007550.0, -157008830.0, -157009870.0, -157009920.0, -157010100.0, -157011060.0, -157011070.0, -157011140.0, -157011150.0, -157011170.0, -157011300.0, -157012460.0, -157013470.0, -157013500.0, -157013570.0, -157013600.0, -157014420.0, -157014850.0, -157015760.0, -157015800.0, -157016740.0, -157016930.0, -157018110.0, -157020430.0, -157020480.0, -157021570.0, -157021580.0, -157021620.0, -157021630.0, -157021730.0, -157022780.0, -157022830.0, -157022850.0, -157023740.0, -157026200.0, -157026320.0, -157028270.0, -157028290.0, -157028400.0, -157028530.0, -157029550.0, -157030780.0, -157030940.0, -157033180.0, -157107300.0, -157107410.0, -157108450.0, -157109600.0, -157110720.0, -157112060.0, -157113090.0, -157113170.0, -157113220.0, -157114340.0, -157114380.0, -157115380.0, -157115460.0, -157115470.0, -157115580.0, -157116510.0, -157116600.0, -157116700.0, -157116780.0, -157117500.0, -157117800.0, -157117980.0, -157118940.0, -157121330.0, -157121540.0, -157122300.0, -157122400.0, -157122480.0, -157122620.0, -157123520.0, -157123580.0, -157123660.0, -157123680.0, -157123700.0, -157124670.0, -157124780.0, -157125840.0, -157126900.0, -157126930.0, -157126940.0, -157127020.0, -157127140.0, -157129440.0, -157129490.0, -157130540.0, -157131630.0, -157131800.0, -157132830.0, -157132860.0, -157132880.0, -157133890.0, -157133980.0, -157134100.0, -157134260.0, -157135180.0, -157135200.0, -157138540.0, -157138750.0, -157139650.0, -157139680.0, -157139760.0, -157139920.0, -157140960.0, -157143260.0, -157147970.0, -157228020.0, -157229100.0, -157230300.0, -157230450.0, -157231280.0, -157231460.0, -157232640.0, -157234930.0, -157235020.0, -157235070.0, -157236160.0, -157237380.0, -157238400.0, -157238500.0, -157238530.0, -157238540.0, -157239410.0, -157239500.0, -157239630.0, -157243060.0, -157243070.0, -157245360.0, -157245600.0, -157246660.0, -157246700.0, -157246770.0, -157248770.0, -157248800.0, -157248860.0, -157248900.0, -157248960.0, -157249950.0, -157250200.0, -157251020.0, -157251400.0, -157252350.0, -157252380.0, -157252460.0, -157253440.0, -157253550.0, -157253570.0, -157253580.0, -157253600.0, -157254660.0, -157254820.0, -157255900.0, -157255940.0, -157256850.0, -157259340.0, -157259360.0, -157259400.0, -157259440.0, -157260640.0, -157261730.0, -157261800.0, -157262830.0, -157265120.0, -157265200.0, -157265230.0, -157335070.0, -157335120.0, -157341940.0, -157342060.0, -157342160.0, -157346580.0, -157346800.0, -157347740.0, -157347820.0, -157347860.0, -157348930.0, -157348960.0, -157349020.0, -157350080.0, -157350100.0, -157350200.0, -157351220.0, -157351230.0, -157351400.0, -157352460.0, -157352510.0, -157353540.0, -157353630.0, -157355940.0, -157357070.0, -157357150.0, -157357180.0, -157357200.0, -157358130.0, -157358340.0, -157358460.0, -157359460.0, -157360350.0, -157360460.0, -157361740.0, -157361820.0, -157361840.0, -157362700.0, -157362990.0, -157363000.0, -157364000.0, -157364030.0, -157364060.0, -157364200.0, -157364270.0, -157365180.0, -157365300.0, -157365490.0, -157366420.0, -157366430.0, -157366510.0, -157366530.0, -157366610.0, -157366640.0, -157367580.0, -157367630.0, -157367650.0, -157367660.0, -157368780.0, -157370000.0, -157372200.0, -157372260.0, -157373140.0, -157373260.0, -157373340.0, -157374430.0, -157374530.0, -157374600.0, -157374670.0, -157375520.0, -157375630.0, -157375710.0, -157375780.0, -157375800.0, -157376580.0, -157378050.0, -157452480.0, -157456030.0, -157456160.0, -157458400.0, -157459550.0, -157460690.0, -157460740.0, -157460750.0, -157461660.0, -157462990.0, -157463000.0, -157463020.0, -157463060.0, -157463980.0, -157464100.0, -157464160.0, -157464200.0, -157465020.0, -157465100.0, -157465380.0, -157466340.0, -157466460.0, -157467710.0, -157467820.0, -157468800.0, -157468900.0, -157469060.0, -157470030.0, -157470050.0, -157471140.0, -157472290.0, -157472370.0, -157474530.0, -157476910.0, -157477000.0, -157477140.0, -157478020.0, -157478060.0, -157479000.0, -157480380.0, -157488510.0, -157488620.0, -157490910.0, -157492860.0, -157492960.0, -157494240.0, -157575870.0, -157577000.0, -157577120.0, -157579360.0, -157579500.0, -157580290.0, -157580340.0, -157580420.0, -157580480.0, -157580560.0, -157580590.0, -157582780.0, -157582850.0, -157582910.0, -157584000.0, -157585020.0, -157585220.0, -157586290.0, -157587400.0, -157589630.0, -157589650.0, -157589820.0, -157589940.0, -157589950.0, -157589980.0, -157590980.0, -157591150.0, -157593230.0, -157593570.0, -157594300.0, -157594620.0, -157595650.0, -157595710.0, -157596800.0, -157596850.0, -157596930.0, -157597780.0, -157598140.0, -157598960.0, -157599100.0, -157599220.0, -157600020.0, -157600140.0, -157601150.0, -157601330.0, -157603680.0, -157605840.0, -157605870.0, -157608220.0, -157689940.0, -157692130.0, -157693310.0, -157693360.0, -157695360.0, -157695420.0, -157695570.0, -157695660.0, -157695740.0, -157696830.0, -157696930.0, -157698030.0, -157698050.0, -157700260.0, -157701600.0, -157701630.0, -157703650.0, -157703660.0, -157703760.0, -157703870.0, -157703940.0, -157704980.0, -157705020.0, -157705120.0, -157706130.0, -157706200.0, -157706220.0, -157706450.0, -157707220.0, -157707540.0, -157708320.0, -157708450.0, -157709500.0, -157709580.0, -157709660.0, -157710690.0, -157710850.0, -157712930.0, -157713300.0, -157715200.0, -157715260.0, -157715280.0, -157715300.0, -157715410.0, -157718660.0, -157718670.0, -157723460.0, -157724700.0, -157725740.0, -157813380.0, -157813390.0, -157813400.0, -157813470.0, -157814580.0, -157814620.0, -157816000.0, -157817040.0, -157817860.0, -157818200.0, -157818350.0, -157819140.0, -157819420.0, -157819470.0, -157820320.0, -157820640.0, -157820690.0, -157820720.0, -157820740.0, -157820770.0, -157821400.0, -157821490.0, -157821900.0, -157822050.0, -157823200.0, -157824240.0, -157824290.0, -157824320.0, -157824450.0, -157825140.0, -157825570.0, -157825660.0, -157826770.0, -157826800.0, -157826820.0, -157826910.0, -157827650.0, -157827680.0, -157829220.0, -157829950.0, -157830370.0, -157830480.0, -157830500.0, -157831140.0, -157831620.0, -157832820.0, -157832880.0, -157832910.0, -157832980.0, -157833680.0, -157834180.0, -157834260.0, -157834780.0, -157837230.0, -157837260.0, -157837340.0, -157837380.0, -157842080.0, -157844560.0, -157844670.0, -157930860.0, -157932340.0, -157932370.0, -157933540.0, -157934400.0, -157935650.0, -157936050.0, -157936060.0, -157936850.0, -157937300.0, -157938500.0, -157938540.0, -157939180.0, -157939680.0, -157940860.0, -157941000.0, -157941780.0, -157942100.0, -157942900.0, -157944460.0, -157945230.0, -157945780.0, -157945800.0, -157945870.0, -157946560.0, -157947040.0, -157947060.0, -157947730.0, -157948180.0, -157949470.0, -157950560.0, -157950640.0, -157950660.0, -157950690.0, -157951780.0, -157953070.0, -157953100.0, -157953170.0, -157954200.0, -157954400.0, -157962290.0, -158045200.0, -158045220.0, -158046530.0, -158050110.0, -158050160.0, -158051400.0, -158054880.0, -158054960.0, -158054980.0, -158056780.0, -158057460.0, -158058130.0, -158058340.0, -158058610.0, -158058670.0, -158058770.0, -158059820.0, -158061090.0, -158061180.0, -158062530.0, -158063150.0, -158063520.0, -158065840.0, -158065900.0, -158065970.0, -158066100.0, -158067970.0, -158068030.0, -158068320.0, -158069180.0, -158069470.0, -158070720.0, -158070750.0, -158071870.0, -158071920.0, -158071940.0, -158072020.0, -158072030.0, -158072880.0, -158073150.0, -158073250.0, -158073950.0, -158074180.0, -158074340.0, -158074400.0, -158074420.0, -158075180.0, -158075200.0, -158075710.0, -158075740.0, -158076370.0, -158076690.0, -158078850.0, -158078900.0, -158079330.0, -158079360.0, -158079500.0, -158079900.0, -158081300.0, -158082480.0, -158082530.0, -158082560.0, -158083760.0, -158083840.0, -158084910.0, -158084930.0, -158088540.0, -158170240.0, -158171330.0, -158171490.0, -158171550.0, -158172750.0, -158174800.0, -158178830.0, -158178850.0, -158178880.0, -158180020.0, -158180240.0, -158181280.0, -158183330.0, -158183600.0, -158183680.0, -158184860.0, -158184930.0, -158185000.0, -158185680.0, -158187300.0, -158188540.0, -158188700.0, -158190940.0, -158191020.0, -158192260.0, -158194770.0, -158195620.0, -158195800.0, -158195820.0, -158195840.0, -158195940.0, -158197040.0, -158197090.0, -158197140.0, -158198240.0, -158198340.0, -158198400.0, -158205090.0, -158207520.0, -158211330.0, -158211420.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7684659090909091\n",
      "Hamming Loss: 0.125\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       584\n",
      "           1       0.17      1.00      0.29       120\n",
      "\n",
      "    accuracy                           0.17       704\n",
      "   macro avg       0.09      0.50      0.15       704\n",
      "weighted avg       0.03      0.17      0.05       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 2s 642us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 164us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 151us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 9.2982 - binary_accuracy: 0.3972s - loss: 9.2887 - binary_accuracy: 0.\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 171us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 145us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 9.2982 - binary_accuracy: 0.3972s - loss: 9.3192 - binary_accuracy: \n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972s - loss: 9.2741 - binary_accuracy: 0.39\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 159us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 156us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - ETA: 0s - loss: 9.3106 - binary_accuracy: 0.396 - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 148us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 167us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 133us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 152us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972s - loss: 9.3514 - binary_accuracy: \n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 152us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 152us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 165us/sample - loss: 9.2982 - binary_accuracy: 0.3972s - loss: 9.2456 - binary_accuracy: \n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 9.2982 - binary_accuracy: 0.3972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-17689904.0, -17690336.0, -17690472.0, -17691016.0, -17691056.0, -17691328.0, -17691336.0, -17691832.0, -17692336.0, -17692544.0, -17692624.0, -17692720.0, -17692736.0, -17692752.0, -17692920.0, -17693064.0, -17693072.0, -17693400.0, -17693672.0, -17693792.0, -17693816.0, -17693856.0, -17693912.0, -17693968.0, -17694184.0, -17694472.0, -17694520.0, -17694664.0, -17694776.0, -17694920.0, -17695288.0, -17695336.0, -17695376.0, -17695464.0, -17695536.0, -17695656.0, -17695712.0, -17695736.0, -17696104.0, -17696136.0, -17696304.0, -17696392.0, -17696440.0, -17696592.0, -17696744.0, -17696864.0, -17697048.0, -17697072.0, -17697096.0, -17697496.0, -17697752.0, -17697792.0, -17697840.0, -17697896.0, -17697904.0, -17697936.0, -17697984.0, -17698096.0, -17698192.0, -17698336.0, -17698696.0, -17698704.0, -17698880.0, -17699240.0, -17700240.0, -17718208.0, -17718264.0, -17718320.0, -17719072.0, -17719088.0, -17719760.0, -17719768.0, -17720232.0, -17720248.0, -17720712.0, -17720728.0, -17720816.0, -17721040.0, -17721328.0, -17721384.0, -17721496.0, -17721520.0, -17721544.0, -17721832.0, -17722008.0, -17722224.0, -17722272.0, -17722360.0, -17722528.0, -17722552.0, -17722664.0, -17722728.0, -17722736.0, -17723168.0, -17723416.0, -17723464.0, -17723504.0, -17723512.0, -17723816.0, -17723904.0, -17723960.0, -17724016.0, -17724032.0, -17724368.0, -17724416.0, -17724592.0, -17724792.0, -17725040.0, -17725208.0, -17725248.0, -17725256.0, -17725536.0, -17725968.0, -17726432.0, -17726736.0, -17727744.0, -17728384.0, -17747936.0, -17748048.0, -17748896.0, -17749328.0, -17749632.0, -17750408.0, -17750584.0, -17750808.0, -17751096.0, -17751496.0, -17751536.0, -17751608.0, -17751768.0, -17751896.0, -17751968.0, -17752104.0, -17752208.0, -17752560.0, -17752744.0, -17752816.0, -17752904.0, -17752984.0, -17753040.0, -17753248.0, -17753280.0, -17753320.0, -17753408.0, -17753552.0, -17753664.0, -17753728.0, -17753944.0, -17754088.0, -17754136.0, -17754232.0, -17754392.0, -17754440.0, -17754816.0, -17755032.0, -17755360.0, -17755512.0, -17755600.0, -17756360.0, -17756544.0, -17757376.0, -17757536.0, -17777432.0, -17777456.0, -17778288.0, -17778328.0, -17778520.0, -17779072.0, -17779520.0, -17779544.0, -17779624.0, -17779640.0, -17779864.0, -17779888.0, -17780160.0, -17780392.0, -17780544.0, -17780576.0, -17780704.0, -17781144.0, -17781200.0, -17781344.0, -17781464.0, -17781496.0, -17781616.0, -17781648.0, -17781872.0, -17782128.0, -17782608.0, -17782688.0, -17782816.0, -17782944.0, -17783144.0, -17783648.0, -17784128.0, -17784184.0, -17784856.0, -17784936.0, -17785104.0, -17785384.0, -17786792.0, -17805488.0, -17806808.0, -17807336.0, -17807376.0, -17807568.0, -17807592.0, -17807632.0, -17807664.0, -17807728.0, -17808008.0, -17808152.0, -17808256.0, -17808656.0, -17808880.0, -17809088.0, -17809136.0, -17809200.0, -17809352.0, -17809408.0, -17809864.0, -17809872.0, -17810176.0, -17810200.0, -17810656.0, -17810672.0, -17810736.0, -17811056.0, -17811392.0, -17811672.0, -17812184.0, -17812240.0, -17812360.0, -17812680.0, -17813296.0, -17813376.0, -17813512.0, -17813528.0, -17813576.0, -17813632.0, -17813656.0, -17813768.0, -17813872.0, -17813904.0, -17814024.0, -17814992.0, -17815008.0, -17815096.0, -17815200.0, -17815360.0, -17815584.0, -17815608.0, -17815616.0, -17815648.0, -17815704.0, -17815712.0, -17834792.0, -17835576.0, -17835656.0, -17835832.0, -17835840.0, -17836088.0, -17836144.0, -17836584.0, -17837040.0, -17837088.0, -17837112.0, -17837208.0, -17837232.0, -17837368.0, -17837544.0, -17837568.0, -17837608.0, -17837616.0, -17837648.0, -17837672.0, -17837768.0, -17837976.0, -17838120.0, -17838168.0, -17838176.0, -17838192.0, -17838216.0, -17838416.0, -17838432.0, -17838744.0, -17839184.0, -17839384.0, -17839616.0, -17839656.0, -17839944.0, -17839952.0, -17839976.0, -17840016.0, -17840192.0, -17840208.0, -17841448.0, -17842520.0, -17842528.0, -17843000.0, -17843304.0, -17843648.0, -17843656.0, -17843848.0, -17843920.0, -17844432.0, -17844576.0, -17844640.0, -17863656.0, -17864096.0, -17865016.0, -17865384.0, -17865528.0, -17865960.0, -17866136.0, -17866144.0, -17866280.0, -17866344.0, -17866400.0, -17866464.0, -17866480.0, -17866488.0, -17866528.0, -17866640.0, -17866656.0, -17866960.0, -17867168.0, -17867264.0, -17867336.0, -17867920.0, -17867936.0, -17868048.0, -17868208.0, -17868248.0, -17868344.0, -17868360.0, -17868448.0, -17868456.0, -17868480.0, -17868520.0, -17868832.0, -17868928.0, -17868984.0, -17869016.0, -17869040.0, -17869056.0, -17869184.0, -17869256.0, -17869496.0, -17869584.0, -17869608.0, -17869616.0, -17869896.0, -17869936.0, -17869960.0, -17870040.0, -17870088.0, -17870184.0, -17870208.0, -17870224.0, -17870872.0, -17870944.0, -17871016.0, -17871344.0, -17871368.0, -17871480.0, -17871744.0, -17871896.0, -17872080.0, -17872160.0, -17872232.0, -17872240.0, -17872264.0, -17872280.0, -17872552.0, -17872784.0, -17872800.0, -17873016.0, -17873328.0, -17873672.0, -17892248.0, -17892432.0, -17892872.0, -17893304.0, -17893592.0, -17893864.0, -17893880.0, -17893888.0, -17893920.0, -17893976.0, -17894296.0, -17894688.0, -17894888.0, -17894960.0, -17895312.0, -17895336.0, -17895424.0, -17895584.0, -17895664.0, -17895776.0, -17895928.0, -17895960.0, -17895984.0, -17896072.0, -17896120.0, -17896176.0, -17896208.0, -17896416.0, -17896432.0, -17896528.0, -17896688.0, -17896760.0, -17896848.0, -17896872.0, -17897488.0, -17897536.0, -17897680.0, -17897856.0, -17897896.0, -17897928.0, -17897968.0, -17898376.0, -17898448.0, -17898504.0, -17898680.0, -17898824.0, -17898864.0, -17898880.0, -17899368.0, -17899432.0, -17899632.0, -17899912.0, -17900040.0, -17900200.0, -17900472.0, -17900584.0, -17900672.0, -17900936.0, -17901056.0, -17901464.0, -17901520.0, -17901720.0, -17902184.0, -17902256.0, -17902312.0, -17902600.0, -17922136.0, -17922680.0, -17923216.0, -17923376.0, -17923864.0, -17923880.0, -17924000.0, -17924048.0, -17924264.0, -17924352.0, -17924552.0, -17924696.0, -17924704.0, -17924760.0, -17925024.0, -17925240.0, -17925288.0, -17925320.0, -17925360.0, -17925368.0, -17925432.0, -17925672.0, -17925824.0, -17925968.0, -17926168.0, -17926192.0, -17926400.0, -17926408.0, -17926520.0, -17926664.0, -17926784.0, -17926968.0, -17927008.0, -17927080.0, -17927168.0, -17927400.0, -17927680.0, -17927688.0, -17927744.0, -17927760.0, -17927904.0, -17927912.0, -17927920.0, -17927936.0, -17928232.0, -17928272.0, -17928304.0, -17928312.0, -17928480.0, -17928512.0, -17928544.0, -17928832.0, -17929032.0, -17929096.0, -17929112.0, -17929120.0, -17929288.0, -17929520.0, -17929656.0, -17929864.0, -17929928.0, -17929936.0, -17930264.0, -17930352.0, -17930416.0, -17930456.0, -17930544.0, -17930592.0, -17931112.0, -17951320.0, -17951904.0, -17952088.0, -17952184.0, -17952296.0, -17952400.0, -17952424.0, -17952448.0, -17952768.0, -17952808.0, -17952848.0, -17952856.0, -17952984.0, -17953032.0, -17953152.0, -17953376.0, -17953544.0, -17953912.0, -17953984.0, -17954048.0, -17954216.0, -17954560.0, -17954744.0, -17954824.0, -17955016.0, -17955080.0, -17955088.0, -17955096.0, -17955304.0, -17955336.0, -17955440.0, -17955648.0, -17955696.0, -17955744.0, -17955864.0, -17955872.0, -17955880.0, -17955920.0, -17956280.0, -17956328.0, -17956440.0, -17956448.0, -17956456.0, -17956832.0, -17956912.0, -17957184.0, -17957320.0, -17957328.0, -17957472.0, -17957584.0, -17957648.0, -17957744.0, -17957768.0, -17957856.0, -17958336.0, -17958400.0, -17958480.0, -17958520.0, -17958552.0, -17958576.0, -17958792.0, -17959128.0, -17959440.0, -17959472.0, -17959576.0, -17959704.0, -17959992.0, -17979400.0, -17980032.0, -17980064.0, -17980424.0, -17980792.0, -17980960.0, -17981112.0, -17981160.0, -17981240.0, -17981328.0, -17981384.0, -17981416.0, -17981536.0, -17981616.0, -17981624.0, -17981632.0, -17981864.0, -17981872.0, -17981896.0, -17981928.0, -17982136.0, -17982224.0, -17982440.0, -17982576.0, -17982760.0, -17982816.0, -17982832.0, -17982992.0, -17983016.0, -17983024.0, -17983152.0, -17983192.0, -17983360.0, -17983384.0, -17983728.0, -17983816.0, -17983848.0, -17983880.0, -17983920.0, -17984208.0, -17984344.0, -17984600.0, -17984760.0, -17984792.0, -17985408.0, -17985608.0, -17985632.0, -17985816.0, -17985920.0, -17985968.0, -17985976.0, -17986096.0, -17986176.0, -17986304.0, -17986360.0, -17986392.0, -17986832.0, -17986856.0, -17986920.0, -17987080.0, -17987160.0, -17987168.0, -17987432.0, -17987720.0, -17987968.0, -17988200.0, -17988264.0, -17988344.0, -17988680.0, -17989544.0, -18007736.0, -18009024.0, -18009056.0, -18009064.0, -18009104.0, -18009232.0, -18009256.0, -18009336.0, -18010704.0, -18012384.0, -18012792.0, -18013808.0, -18014192.0, -18014688.0, -18014840.0, -18014856.0, -18014984.0, -18015024.0, -18015080.0, -18015096.0, -18015248.0, -18015312.0, -18015432.0, -18015584.0, -18015632.0, -18015664.0, -18015704.0, -18015720.0, -18015808.0, -18015928.0, -18015936.0, -18017448.0, -18017512.0, -18017688.0, -18017728.0, -95696420.0, -95696430.0, -95696920.0, -95696970.0, -95696984.0, -95697000.0, -95697384.0, -95697410.0, -95697950.0, -95698940.0, -95699256.0, -95699280.0, -95699576.0, -95699656.0, -95699720.0, -95699840.0, -95699950.0, -95699990.0, -95700040.0, -95700060.0, -95700080.0, -95700180.0, -95700370.0, -95700640.0, -95700856.0, -95700870.0, -95700936.0, -95700960.0, -95701000.0, -95701010.0, -95701170.0, -95701280.0, -95702000.0, -95702230.0, -95702270.0, -95702290.0, -95702344.0, -95702580.0, -95702690.0, -95702840.0, -95703340.0, -95703360.0, -95703430.0, -95703540.0, -95703590.0, -95703600.0, -95703790.0, -95703830.0, -95703860.0, -95703870.0, -95703900.0, -95704000.0, -95704210.0, -95704530.0, -95704570.0, -95704650.0, -95704690.0, -95704720.0, -95704750.0, -95704850.0, -95704900.0, -95705176.0, -95705460.0, -95705470.0, -95705864.0, -95706410.0, -95729190.0, -95729224.0, -95729304.0, -95729380.0, -95729900.0, -95729920.0, -95729940.0, -95730160.0, -95730170.0, -95730180.0, -95730210.0, -95730456.0, -95730480.0, -95730504.0, -95730536.0, -95730824.0, -95732150.0, -95732210.0, -95732470.0, -95732580.0, -95732696.0, -95732770.0, -95732780.0, -95732900.0, -95733020.0, -95733090.0, -95733460.0, -95733490.0, -95733520.0, -95733700.0, -95734056.0, -95734060.0, -95734140.0, -95734160.0, -95734260.0, -95734450.0, -95734670.0, -95734720.0, -95734740.0, -95734820.0, -95734930.0, -95735050.0, -95735064.0, -95735710.0, -95735840.0, -95736056.0, -95736060.0, -95736320.0, -95736350.0, -95736400.0, -95738240.0, -95738370.0, -95762080.0, -95762660.0, -95762770.0, -95763064.0, -95763120.0, -95763710.0, -95764190.0, -95764344.0, -95764390.0, -95764696.0, -95764940.0, -95765620.0, -95765624.0, -95765630.0, -95765660.0, -95765800.0, -95765880.0, -95766100.0, -95766360.0, -95766680.0, -95766690.0, -95766920.0, -95767304.0, -95767544.0, -95767610.0, -95767630.0, -95767720.0, -95767960.0, -95768160.0, -95768340.0, -95768660.0, -95768780.0, -95768850.0, -95768860.0, -95768910.0, -95768930.0, -95769336.0, -95769390.0, -95769580.0, -95769620.0, -95769690.0, -95769950.0, -95770860.0, -95770910.0, -95771160.0, -95795224.0, -95795940.0, -95796160.0, -95796200.0, -95796550.0, -95796770.0, -95796824.0, -95796950.0, -95797150.0, -95797180.0, -95797590.0, -95797660.0, -95797850.0, -95798130.0, -95798184.0, -95798220.0, -95798456.0, -95798504.0, -95798540.0, -95799470.0, -95799490.0, -95800180.0, -95800260.0, -95800470.0, -95800490.0, -95800510.0, -95801176.0, -95801390.0, -95801630.0, -95801944.0, -95802264.0, -95802330.0, -95802740.0, -95802800.0, -95802910.0, -95804000.0, -95826880.0, -95826990.0, -95827440.0, -95828040.0, -95828104.0, -95828180.0, -95828400.0, -95828500.0, -95828510.0, -95828740.0, -95828790.0, -95829040.0, -95829170.0, -95829180.0, -95829410.0, -95829840.0, -95829900.0, -95830120.0, -95830170.0, -95830240.0, -95830340.0, -95830900.0, -95830936.0, -95831010.0, -95831330.0, -95831760.0, -95832130.0, -95832616.0, -95832720.0, -95832880.0, -95832980.0, -95833650.0, -95833760.0, -95833940.0, -95833960.0, -95833980.0, -95834130.0, -95834140.0, -95834150.0, -95834200.0, -95834260.0, -95834430.0, -95834500.0, -95834510.0, -95835220.0, -95835700.0, -95835830.0, -95835910.0, -95835960.0, -95836270.0, -95836340.0, -95836420.0, -95836450.0, -95836480.0, -95836560.0, -95836584.0, -95859080.0, -95859256.0, -95859304.0, -95859670.0, -95859896.0, -95860540.0, -95860744.0, -95860840.0, -95860970.0, -95861100.0, -95861110.0, -95861280.0, -95861330.0, -95861520.0, -95861650.0, -95861710.0, -95861720.0, -95861770.0, -95861860.0, -95861980.0, -95862056.0, -95862060.0, -95862270.0, -95862360.0, -95862380.0, -95862450.0, -95862860.0, -95863050.0, -95863496.0, -95863576.0, -95863610.0, -95863620.0, -95863720.0, -95863730.0, -95863740.0, -95864000.0, -95864020.0, -95864100.0, -95864190.0, -95864270.0, -95865370.0, -95866720.0, -95867800.0, -95867864.0, -95867940.0, -95868130.0, -95868380.0, -95868420.0, -95868580.0, -95868980.0, -95869050.0, -95869080.0, -95892000.0, -95892220.0, -95892240.0, -95892380.0, -95892850.0, -95893170.0, -95893350.0, -95893480.0, -95893650.0, -95893840.0, -95893864.0, -95893870.0, -95894370.0, -95894500.0, -95894520.0, -95894570.0, -95894780.0, -95894800.0, -95894910.0, -95895000.0, -95895200.0, -95895260.0, -95895624.0, -95895656.0, -95895660.0, -95895680.0, -95895890.0, -95895910.0, -95896040.0, -95896070.0, -95896136.0, -95896250.0, -95896320.0, -95896430.0, -95896550.0, -95896700.0, -95897000.0, -95897220.0, -95897240.0, -95897416.0, -95897510.0, -95897520.0, -95897530.0, -95897580.0, -95897630.0, -95897720.0, -95897830.0, -95897920.0, -95897930.0, -95898040.0, -95898190.0, -95898480.0, -95898580.0, -95899350.0, -95899450.0, -95899520.0, -95899740.0, -95899760.0, -95900210.0, -95900270.0, -95900330.0, -95900490.0, -95900664.0, -95900696.0, -95900750.0, -95901064.0, -95901110.0, -95901304.0, -95901320.0, -95901336.0, -95901370.0, -95901784.0, -95924370.0, -95924504.0, -95924560.0, -95924580.0, -95924590.0, -95924630.0, -95924700.0, -95924750.0, -95924800.0, -95924850.0, -95924950.0, -95925970.0, -95926410.0, -95926430.0, -95926450.0, -95926520.0, -95926710.0, -95926780.0, -95926984.0, -95927070.0, -95927144.0, -95927260.0, -95927330.0, -95927336.0, -95927390.0, -95927570.0, -95927710.0, -95927740.0, -95927770.0, -95927940.0, -95927944.0, -95928100.0, -95928120.0, -95928510.0, -95928820.0, -95928940.0, -95928990.0, -95929140.0, -95929160.0, -95929190.0, -95929250.0, -95929740.0, -95929860.0, -95929940.0, -95930120.0, -95930150.0, -95930264.0, -95930640.0, -95930664.0, -95930830.0, -95930990.0, -95931170.0, -95931410.0, -95931490.0, -95931630.0, -95932410.0, -95932900.0, -95933096.0, -95933150.0, -95933390.0, -95933890.0, -95933930.0, -95934050.0, -95934140.0, -95934160.0, -95934344.0, -95957030.0, -95957350.0, -95957960.0, -95958230.0, -95958376.0, -95958380.0, -95958540.0, -95958640.0, -95958670.0, -95959230.0, -95959420.0, -95959464.0, -95959600.0, -95959970.0, -95959990.0, -95960110.0, -95960150.0, -95960200.0, -95960670.0, -95960720.0, -95960740.0, -95960936.0, -95960980.0, -95961250.0, -95961330.0, -95961410.0, -95961470.0, -95961590.0, -95962010.0, -95962040.0, -95962050.0, -95962184.0, -95962480.0, -95962610.0, -95962660.0, -95962780.0, -95962810.0, -95962920.0, -95962960.0, -95963020.0, -95963060.0, -95963120.0, -95963200.0, -95963260.0, -95963300.0, -95963400.0, -95963630.0, -95963680.0, -95963710.0, -95963784.0, -95963820.0, -95964184.0, -95964264.0, -95964300.0, -95964520.0, -95964664.0, -95964900.0, -95964940.0, -95965070.0, -95965180.0, -95965340.0, -95965700.0, -95965704.0, -95965816.0, -95965860.0, -95965864.0, -95965970.0, -95966090.0, -95966536.0, -95989490.0, -95989816.0, -95990110.0, -95990350.0, -95990620.0, -95990660.0, -95990670.0, -95990770.0, -95990940.0, -95990950.0, -95990980.0, -95991016.0, -95991350.0, -95991540.0, -95992000.0, -95992170.0, -95992280.0, -95992504.0, -95992880.0, -95993144.0, -95993280.0, -95993300.0, -95993400.0, -95993450.0, -95993460.0, -95993540.0, -95993580.0, -95994024.0, -95994050.0, -95994090.0, -95994140.0, -95994160.0, -95994180.0, -95994210.0, -95994230.0, -95994580.0, -95994590.0, -95994690.0, -95995020.0, -95995290.0, -95995300.0, -95995500.0, -95995520.0, -95996100.0, -95996120.0, -95996160.0, -95996200.0, -95996440.0, -95996456.0, -95996536.0, -95996550.0, -95996650.0, -95996840.0, -95997330.0, -95997440.0, -95997550.0, -95997576.0, -95997650.0, -95997816.0, -95998080.0, -95998110.0, -95998410.0, -95998510.0, -95998640.0, -95998800.0, -95999100.0, -95999240.0, -96022696.0, -96022740.0, -96022830.0, -96022960.0, -96023090.0, -96023160.0, -96023200.0, -96023220.0, -96023256.0, -96023520.0, -96023624.0, -96023710.0, -96023770.0, -96023790.0, -96023944.0, -96024110.0, -96024190.0, -96024590.0, -96024740.0, -96024770.0, -96024780.0, -96024800.0, -96024960.0, -96024990.0, -96025070.0, -96025120.0, -96025150.0, -96025190.0, -96025220.0, -96025240.0, -96025430.0, -96025460.0, -96025700.0, -96025970.0, -96026080.0, -96026420.0, -96026510.0, -96026730.0, -96027010.0, -96027020.0, -96027040.0, -96027064.0, -96027100.0, -96027670.0, -96027710.0, -96027860.0, -96028330.0, -96028460.0, -96028640.0, -96028780.0, -96028860.0, -96028930.0, -96028936.0, -96029020.0, -96029240.0, -96029304.0, -96029340.0, -96029360.0, -96029480.0, -96029650.0, -96029700.0, -96029860.0, -96030030.0, -96030184.0, -96030290.0, -96030320.0, -96030700.0, -96030820.0, -96031120.0, -96031140.0, -96031310.0, -96031630.0, -96054480.0, -96054510.0, -96054560.0, -96054664.0, -96054776.0, -96054780.0, -96055384.0, -96055700.0, -96056140.0, -96058060.0, -96059576.0, -96059690.0, -96060296.0, -96061170.0, -96061656.0, -96061720.0, -96061790.0, -96061840.0, -96061960.0, -96061990.0, -96062040.0, -96062136.0, -96062260.0, -96062560.0, -96062860.0, -96062910.0, -96063160.0, -96063240.0, -96063520.0, -96063540.0, -96063560.0, -96063760.0, -96063830.0, -96063850.0, -96064130.0, -96064160.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7784090909090909\n",
      "Hamming Loss: 0.11931818181818182\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       578\n",
      "           1       0.00      0.00      0.00       126\n",
      "\n",
      "    accuracy                           0.82       704\n",
      "   macro avg       0.41      0.50      0.45       704\n",
      "weighted avg       0.67      0.82      0.74       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 2s 617us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 161us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 162us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 152us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 158us/sample - loss: 13.6198 - binary_accuracy: 0.1147 - loss: 13.6835 - binary_\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 133us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 172us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 145us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147 - loss: 13.6885 - binary_accura\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 162us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.6198 - binary_accuracy: 0.1147 - loss: 13.6622 - binary_accuracy: \n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 155us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 1s 181us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 132us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 157us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 132us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 156us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 175us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 164us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 157us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 165us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.6198 - binary_accuracy: 0.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-120147624.0, -120148210.0, -120148540.0, -120148560.0, -120148600.0, -120149176.0, -120150000.0, -120150580.0, -120151416.0, -120151460.0, -120151470.0, -120152300.0, -120153680.0, -120153700.0, -120154000.0, -120154260.0, -120154290.0, -120154856.0, -120154870.0, -120155420.0, -120155544.0, -120156070.0, -120156630.0, -120157144.0, -120157736.0, -120158300.0, -120158860.0, -120159740.0, -120159780.0, -120159810.0, -120160200.0, -120160350.0, -120160630.0, -120160920.0, -120161224.0, -120161816.0, -120161830.0, -120162056.0, -120162410.0, -120162990.0, -120163580.0, -120164130.0, -120164160.0, -120164190.0, -120165370.0, -120201590.0, -120203340.0, -120203624.0, -120203704.0, -120203900.0, -120203950.0, -120204200.0, -120204480.0, -120204550.0, -120205150.0, -120205416.0, -120205930.0, -120205980.0, -120206510.0, -120206536.0, -120206570.0, -120207130.0, -120207140.0, -120207190.0, -120207410.0, -120207416.0, -120207440.0, -120207544.0, -120207960.0, -120207980.0, -120208264.0, -120208270.0, -120208330.0, -120208520.0, -120208540.0, -120208580.0, -120208660.0, -120208856.0, -120209170.0, -120209490.0, -120209710.0, -120209970.0, -120210010.0, -120210280.0, -120210630.0, -120210850.0, -120210910.0, -120211020.0, -120211210.0, -120211224.0, -120211430.0, -120211450.0, -120211464.0, -120212056.0, -120212060.0, -120212070.0, -120212130.0, -120212380.0, -120212530.0, -120212610.0, -120212920.0, -120213250.0, -120214056.0, -120214330.0, -120214640.0, -120214900.0, -120214904.0, -120214910.0, -120215250.0, -120215420.0, -120215470.0, -120215800.0, -120215830.0, -120216710.0, -120217060.0, -120217610.0, -120217980.0, -120218020.0, -120218080.0, -120218184.0, -120218520.0, -120218680.0, -120259340.0, -120259920.0, -120260520.0, -120261190.0, -120261330.0, -120261464.0, -120261660.0, -120261770.0, -120262350.0, -120262500.0, -120263600.0, -120263760.0, -120264360.0, -120264610.0, -120265230.0, -120265860.0, -120266260.0, -120266270.0, -120266290.0, -120266330.0, -120267510.0, -120267816.0, -120268050.0, -120268810.0, -120268940.0, -120268960.0, -120269170.0, -120269220.0, -120269780.0, -120270340.0, -120270880.0, -120270936.0, -120270970.0, -120271480.0, -120271500.0, -120271790.0, -120271820.0, -120272380.0, -120272440.0, -120272480.0, -120272504.0, -120272984.0, -120273000.0, -120273060.0, -120273170.0, -120273240.0, -120273530.0, -120274190.0, -120274220.0, -120274400.0, -120274580.0, -120274980.0, -120275240.0, -120275860.0, -120275890.0, -120276136.0, -120276170.0, -120276520.0, -120276770.0, -120277040.0, -120277064.0, -120277290.0, -120318220.0, -120318296.0, -120319220.0, -120319460.0, -120319480.0, -120320310.0, -120320340.0, -120320610.0, -120320620.0, -120320930.0, -120320950.0, -120320970.0, -120321140.0, -120321170.0, -120321180.0, -120321190.0, -120321800.0, -120322340.0, -120322344.0, -120322900.0, -120322910.0, -120323290.0, -120323490.0, -120323500.0, -120324616.0, -120324620.0, -120324710.0, -120325250.0, -120325780.0, -120325896.0, -120326960.0, -120327864.0, -120328040.0, -120328190.0, -120328456.0, -120328710.0, -120329030.0, -120329360.0, -120329380.0, -120329620.0, -120329870.0, -120330610.0, -120330824.0, -120331300.0, -120331960.0, -120332490.0, -120332500.0, -120332504.0, -120332520.0, -120332560.0, -120333070.0, -120333130.0, -120334260.0, -120334504.0, -120376584.0, -120377230.0, -120377570.0, -120377576.0, -120377730.0, -120377784.0, -120377810.0, -120377816.0, -120377820.0, -120378350.0, -120378720.0, -120379500.0, -120380380.0, -120380440.0, -120380450.0, -120380640.0, -120380660.0, -120381040.0, -120381090.0, -120381230.0, -120381330.0, -120382960.0, -120382990.0, -120383030.0, -120383570.0, -120384180.0, -120384500.0, -120384700.0, -120385010.0, -120385040.0, -120385080.0, -120385100.0, -120385230.0, -120385370.0, -120385660.0, -120386820.0, -120386980.0, -120386990.0, -120387620.0, -120388140.0, -120388580.0, -120389096.0, -120389480.0, -120392580.0, -120434320.0, -120434940.0, -120435090.0, -120436184.0, -120437150.0, -120437310.0, -120438010.0, -120438420.0, -120439340.0, -120439576.0, -120439590.0, -120440160.0, -120440700.0, -120440720.0, -120441270.0, -120441330.0, -120441896.0, -120442250.0, -120442400.0, -120442430.0, -120442450.0, -120442584.0, -120442820.0, -120442950.0, -120443016.0, -120443070.0, -120443090.0, -120443410.0, -120443540.0, -120443560.0, -120443740.0, -120444370.0, -120444740.0, -120445420.0, -120446340.0, -120446616.0, -120447370.0, -120447600.0, -120448070.0, -120448580.0, -120449450.0, -120449950.0, -120449960.0, -120450020.0, -120450910.0, -120451144.0, -120451150.0, -120489760.0, -120491544.0, -120492670.0, -120493200.0, -120493256.0, -120493260.0, -120494424.0, -120496730.0, -120497320.0, -120497330.0, -120500210.0, -120501944.0, -120501950.0, -120501980.0, -120502270.0, -120502456.0, -120503630.0, -120504040.0, -120504190.0, -120504580.0, -120504610.0, -120504820.0, -120505370.0, -120505970.0, -120505990.0, -120507090.0, -120507544.0, -120507704.0, -120508060.0, -120508080.0, -120508104.0, -120508670.0, -120508720.0, -120508904.0, -120509210.0, -120548110.0, -120549256.0, -120549260.0, -120549680.0, -120550830.0, -120551770.0, -120551940.0, -120551970.0, -120552020.0, -120552140.0, -120552200.0, -120553380.0, -120554260.0, -120554450.0, -120554904.0, -120555060.0, -120555160.0, -120555704.0, -120555976.0, -120556040.0, -120556050.0, -120556264.0, -120556270.0, -120556560.0, -120557150.0, -120557180.0, -120557210.0, -120557816.0, -120557976.0, -120558540.0, -120558600.0, -120559820.0, -120560700.0, -120561200.0, -120561230.0, -120562050.0, -120563230.0, -120563320.0, -120563520.0, -120567016.0, -120567040.0, -120567600.0, -120567630.0, -120568230.0, -120606470.0, -120609350.0, -120609704.0, -120609870.0, -120609920.0, -120610000.0, -120610930.0, -120611050.0, -120611450.0, -120611550.0, -120611740.0, -120612020.0, -120612264.0, -120612584.0, -120612630.0, -120612680.0, -120612760.0, -120612810.0, -120612820.0, -120614380.0, -120614550.0, -120614610.0, -120615490.0, -120615576.0, -120615580.0, -120617270.0, -120618130.0, -120618400.0, -120618990.0, -120620170.0, -120620330.0, -120620696.0, -120621250.0, -120621304.0, -120621360.0, -120621544.0, -120621550.0, -120622664.0, -120622700.0, -120623016.0, -120623060.0, -120623110.0, -120623140.0, -120623150.0, -120623224.0, -120623780.0, -120624220.0, -120624344.0, -120624744.0, -120625940.0, -120665340.0, -120665800.0, -120666880.0, -120667100.0, -120667110.0, -120667480.0, -120667530.0, -120668104.0, -120668660.0, -120668930.0, -120669224.0, -120669496.0, -120670640.0, -120670880.0, -120670960.0, -120671140.0, -120671180.0, -120671496.0, -120671544.0, -120671720.0, -120671850.0, -120672070.0, -120672220.0, -120672380.0, -120672456.0, -120672690.0, -120672890.0, -120673230.0, -120673510.0, -120673520.0, -120673870.0, -120673976.0, -120674110.0, -120674210.0, -120674460.0, -120674660.0, -120675010.0, -120675030.0, -120675060.0, -120675160.0, -120675210.0, -120675250.0, -120675600.0, -120675630.0, -120676216.0, -120676376.0, -120676790.0, -120677280.0, -120677300.0, -120677430.0, -120677490.0, -120677520.0, -120677550.0, -120677830.0, -120678050.0, -120678490.0, -120678540.0, -120678664.0, -120679140.0, -120679260.0, -120679660.0, -120680260.0, -120680470.0, -120680824.0, -120681530.0, -120681896.0, -120682180.0, -120682504.0, -120682510.0, -120682690.0, -120683690.0, -120684800.0, -120684860.0, -120725890.0, -120726090.0, -120726390.0, -120726400.0, -120726410.0, -120726536.0, -120726550.0, -120726680.0, -120726960.0, -120726970.0, -120726984.0, -120727176.0, -120727250.0, -120727330.0, -120727780.0, -120727830.0, -120727880.0, -120728170.0, -120728900.0, -120729010.0, -120729896.0, -120730010.0, -120730130.0, -120730140.0, -120730184.0, -120730430.0, -120730536.0, -120730640.0, -120731090.0, -120731176.0, -120731190.0, -120731200.0, -120731720.0, -120731784.0, -120732150.0, -120732184.0, -120732260.0, -120732280.0, -120732744.0, -120733096.0, -120733304.0, -120733384.0, -120733520.0, -120733540.0, -120733960.0, -120734170.0, -120734530.0, -120734540.0, -120734664.0, -120735620.0, -120735660.0, -120736216.0, -120736880.0, -120737490.0, -120737630.0, -120737960.0, -120738136.0, -120738560.0, -120738590.0, -120738820.0, -120740300.0, -120742070.0, -120780720.0, -120781120.0, -120781400.0, -120781810.0, -120781864.0, -120782620.0, -120782780.0, -120783010.0, -120783016.0, -120783020.0, -120783180.0, -120783250.0, -120783340.0, -120783720.0, -120784230.0, -120784776.0, -120784940.0, -120784960.0, -120785190.0, -120785290.0, -120785310.0, -120785540.0, -120785890.0, -120785910.0, -120786110.0, -120786600.0, -120786630.0, -120786650.0, -120786664.0, -120786984.0, -120787010.0, -120787170.0, -120787600.0, -120787790.0, -120787850.0, -120787870.0, -120788120.0, -120788180.0, -120788220.0, -120788250.0, -120788480.0, -120788830.0, -120789100.0, -120789240.0, -120789940.0, -120790000.0, -120790630.0, -120790670.0, -120791120.0, -120791180.0, -120791224.0, -120791690.0, -120791700.0, -120791820.0, -120791980.0, -120792264.0, -120792456.0, -120792860.0, -120793010.0, -120793390.0, -120793464.0, -120793490.0, -120793530.0, -120795120.0, -120795176.0, -120795690.0, -120795920.0, -120795960.0, -120796310.0, -120796480.0, -120797550.0, -120797624.0, -120797740.0, -120798024.0, -120799190.0, 85826050.0, 85827440.0, 85827520.0, 85828056.0, 85828590.0, 85829490.0, 85829510.0, 85829810.0, 85830020.0, 85830480.0, 85831090.0, 85831170.0, 85832800.0, 85832830.0, 85833270.0, 85833320.0, 85833440.0, 85833500.0, 85834050.0, 85834460.0, 85834770.0, 85834776.0, 85835304.0, 85835440.0, 85835580.0, 85835980.0, 85836190.0, 85836810.0, 85836850.0, 85836940.0, 85837064.0, 85837310.0, 85837710.0, 85837890.0, 85838340.0, 85838750.0, 85838900.0, 85838970.0, 85838990.0, 85839130.0, 85839250.0, 85839260.0, 85839460.0, 85839980.0, 85840160.0, 85840640.0, 85840670.0, 85840776.0, 85841460.0, 85841500.0, 85841540.0, 85841624.0, 85841736.0, 85841790.0, 85841980.0, 85842090.0, 85842250.0, 85842320.0, 85842350.0, 85842400.0, 85842710.0, 85842820.0, 85842850.0, 85842980.0, 85843060.0, 85843770.0, 85844430.0, 85844600.0, 85844960.0, 85845200.0, 85845260.0, 85845304.0, 85845410.0, 85845720.0, 85845880.0, 85846840.0, 85847120.0, 85847270.0, 85847630.0, 85848170.0, 85893630.0, 85895640.0, 85897000.0, 85897710.0, 85897730.0, 85897760.0, 85898450.0, 85898720.0, 85899660.0, 85900120.0, 85900770.0, 85901020.0, 85901100.0, 85902420.0, 85902536.0, 85902830.0, 85903100.0, 85903150.0, 85903230.0, 85903256.0, 85903870.0, 85904220.0, 85904370.0, 85904590.0, 85905176.0, 85905250.0, 85905360.0, 85906130.0, 85906450.0, 85906500.0, 85907090.0, 85907096.0, 85907100.0, 85907170.0, 85907280.0, 85907670.0, 85907740.0, 85907800.0, 85907840.0, 85908450.0, 85908540.0, 85908640.0, 85908660.0, 85910050.0, 85910060.0, 85910080.0, 85910690.0, 85910720.0, 85910740.0, 85911330.0, 85911410.0, 85911630.0, 85911740.0, 85911920.0, 85912370.0, 85912460.0, 85912480.0, 85912590.0, 85912696.0, 85913310.0, 85961170.0, 85961220.0, 85962504.0, 85963170.0, 85963820.0, 85963890.0, 85964420.0, 85964560.0, 85964620.0, 85965980.0, 85966030.0, 85966610.0, 85967330.0, 85967880.0, 85967920.0, 85968500.0, 85968580.0, 85969090.0, 85969230.0, 85969760.0, 85970000.0, 85970370.0, 85970424.0, 85970584.0, 85970620.0, 85970700.0, 85970760.0, 85971330.0, 85971976.0, 85972104.0, 85972180.0, 85972264.0, 85972840.0, 85973060.0, 85973150.0, 85973250.0, 85973360.0, 85973590.0, 85974000.0, 85974024.0, 85974120.0, 85974240.0, 85974696.0, 85974750.0, 85974950.0, 85974960.0, 85975490.0, 85975540.0, 85975700.0, 85975860.0, 85975980.0, 85976104.0, 85976220.0, 85976300.0, 85976720.0, 85977200.0, 85977230.0, 85977304.0, 85977520.0, 85977540.0, 85977970.0, 85978200.0, 85978210.0, 85978220.0, 85978930.0, 85979500.0, 85979576.0, 85980270.0, 85981260.0, 85981580.0, 85981600.0, 85981660.0, 85981740.0, 85982590.0, 85983740.0, 85984070.0, 86030780.0, 86032104.0, 86032776.0, 86033250.0, 86033480.0, 86034150.0, 86034370.0, 86034620.0, 86034776.0, 86034860.0, 86035416.0, 86035550.0, 86035570.0, 86035600.0, 86036210.0, 86036300.0, 86036664.0, 86036790.0, 86037064.0, 86037330.0, 86038080.0, 86038880.0, 86039550.0, 86039650.0, 86041010.0, 86042936.0, 86043170.0, 86043200.0, 86043896.0, 86044410.0, 86045010.0, 86046660.0, 86046900.0, 86046940.0, 86046980.0, 86046990.0, 86047144.0, 86047160.0, 86047240.0, 86047336.0, 86047600.0, 86047980.0, 86048240.0, 86048990.0, 86049050.0, 86049060.0, 86049090.0, 86049896.0, 86050420.0, 86050460.0, 86050480.0, 86052860.0, 86099120.0, 86099760.0, 86099820.0, 86100380.0, 86100530.0, 86104470.0, 86104500.0, 86104530.0, 86105870.0, 86107120.0, 86107540.0, 86107920.0, 86108744.0, 86110140.0, 86110680.0, 86111170.0, 86111730.0, 86112290.0, 86112460.0, 86112510.0, 86113180.0, 86113304.0, 86113410.0, 86113650.0, 86113730.0, 86113864.0, 86113890.0, 86113970.0, 86113980.0, 86114030.0, 86115300.0, 86115680.0, 86115890.0, 86116104.0, 86116990.0, 86117020.0, 86118430.0, 86118460.0, 86118600.0, 86118620.0, 86118780.0, 86120024.0, 86120460.0, 86120480.0, 86121370.0, 86121770.0, 86169090.0, 86169870.0, 86170020.0, 86170300.0, 86170320.0, 86170510.0, 86170790.0, 86171220.0, 86171250.0, 86171420.0, 86172020.0, 86173010.0, 86173304.0, 86173340.0, 86174080.0, 86174340.0, 86174504.0, 86174560.0, 86175320.0, 86175380.0, 86175680.0, 86176424.0, 86176540.0, 86176730.0, 86177330.0, 86178500.0, 86181900.0, 86181920.0, 86182500.0, 86185350.0, 86187030.0, 86187060.0, 86187100.0, 86188744.0, 86189100.0, 86190780.0, 86236860.0, 86237016.0, 86238080.0, 86238160.0, 86238300.0, 86239580.0, 86240320.0, 86240616.0, 86240720.0, 86241490.0, 86242140.0, 86242450.0, 86243510.0, 86243840.0, 86244440.0, 86245260.0, 86245330.0, 86245840.0, 86246100.0, 86246380.0, 86246400.0, 86246530.0, 86246690.0, 86246910.0, 86247020.0, 86247060.0, 86247180.0, 86247630.0, 86248070.0, 86248080.0, 86248370.0, 86248390.0, 86248420.0, 86248900.0, 86249200.0, 86249224.0, 86249550.0, 86249860.0, 86250140.0, 86250200.0, 86251600.0, 86251680.0, 86252640.0, 86253224.0, 86253250.0, 86254740.0, 86255600.0, 86255950.0, 86256270.0, 86307040.0, 86310050.0, 86311130.0, 86311630.0, 86311650.0, 86312260.0, 86312300.0, 86312584.0, 86313800.0, 86314820.0, 86315000.0, 86315140.0, 86315680.0, 86315780.0, 86315810.0, 86315816.0, 86316000.0, 86316060.0, 86316344.0, 86316616.0, 86317020.0, 86317310.0, 86317490.0, 86319730.0, 86319770.0, 86319820.0, 86320560.0, 86320780.0, 86320910.0, 86320920.0, 86321140.0, 86321160.0, 86321250.0, 86321280.0, 86322570.0, 86323200.0, 86323230.0, 86323256.0, 86323360.0, 86323384.0, 86323410.0, 86324480.0, 86324700.0, 86324744.0, 86324904.0, 86325580.0, 86375930.0, 86376290.0, 86377220.0, 86377304.0, 86377860.0, 86377920.0, 86377930.0, 86377940.0, 86377970.0, 86378130.0, 86378730.0, 86379190.0, 86379816.0, 86379950.0, 86380950.0, 86381144.0, 86381520.0, 86381600.0, 86381730.0, 86381840.0, 86382060.0, 86382590.0, 86382690.0, 86383300.0, 86383336.0, 86384700.0, 86385120.0, 86385370.0, 86385976.0, 86386024.0, 86386584.0, 86387390.0, 86387410.0, 86387420.0, 86388500.0, 86388540.0, 86388860.0, 86388900.0, 86389120.0, 86389464.0, 86390080.0, 86390090.0, 86390130.0, 86390180.0, 86390200.0, 86390720.0, 86390920.0, 86391624.0, 86391700.0, 86391740.0, 86392130.0, 86392190.0, 86392320.0, 86392340.0, 86393540.0, 86393590.0, 86393720.0, 86443280.0, 86443330.0, 86443490.0, 86443520.0, 86443760.0, 86444160.0, 86444504.0, 86444810.0, 86444820.0, 86444904.0, 86445544.0, 86445900.0, 86446830.0, 86446860.0, 86447144.0, 86447210.0, 86447710.0, 86448290.0, 86448370.0, 86448430.0, 86448770.0, 86448930.0, 86448980.0, 86448984.0, 86449070.0, 86449460.0, 86449490.0, 86449660.0, 86449940.0, 86450190.0, 86450830.0, 86451520.0, 86451650.0, 86452880.0, 86452930.0, 86453100.0, 86453110.0, 86453980.0, 86454350.0, 86454856.0, 86454900.0, 86455030.0, 86455120.0, 86456160.0, 86456880.0, 86457710.0, 86458550.0, 86458790.0, 86458820.0, 86458990.0, 86460270.0, 86460420.0, 86460580.0, 86461520.0, 86461660.0, 86461840.0, 86461900.0, 86462190.0, 86462400.0, 86463150.0, 86512220.0, 86512370.0, 86512824.0, 86513060.0, 86513160.0, 86513180.0, 86513730.0, 86514450.0, 86515010.0, 86515050.0, 86515440.0, 86515600.0, 86515700.0, 86515840.0, 86515980.0, 86516510.0, 86516664.0, 86517170.0, 86517520.0, 86517870.0, 86518400.0, 86519010.0, 86519230.0, 86519500.0, 86519650.0, 86519740.0, 86519780.0, 86519790.0, 86519840.0, 86519850.0, 86520460.0, 86520790.0, 86520850.0, 86521130.0, 86521260.0, 86521440.0, 86521540.0, 86521950.0, 86522170.0, 86522250.0, 86522420.0, 86522620.0, 86522640.0, 86522700.0, 86523250.0, 86523280.0, 86523680.0, 86523880.0, 86523890.0, 86524140.0, 86524560.0, 86524570.0, 86524610.0, 86524660.0, 86524740.0, 86524980.0, 86525224.0, 86525980.0, 86526030.0, 86526050.0, 86526180.0, 86526200.0, 86526300.0, 86526370.0, 86526720.0, 86526780.0, 86526890.0, 86527310.0, 86527420.0, 86527470.0, 86528030.0, 86528040.0, 86528190.0, 86528710.0, 86528930.0, 86529330.0, 86529390.0, 86530100.0, 86530220.0, 86531330.0, 86531470.0, 86575730.0, 86577070.0, 86577180.0, 86577240.0, 86577800.0, 86578870.0, 86579100.0, 86579280.0, 86579900.0, 86580250.0, 86580440.0, 86580610.0, 86580950.0, 86581540.0, 86582020.0, 86582060.0, 86582200.0, 86582500.0, 86582530.0, 86583340.0, 86584000.0, 86585950.0, 86586060.0, 86586090.0, 86586700.0, 86586770.0, 86586880.0, 86587340.0, 86587380.0, 86587430.0, 86588030.0, 86588100.0, 86588140.0, 86588700.0, 86590770.0, 86592136.0, 86592210.0, 86592220.0, 86592930.0, 86593500.0, 86595140.0, 86595490.0, 86595510.0, 86595590.0, 86595650.0, 86596424.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7997159090909091\n",
      "Hamming Loss: 0.10014204545454546\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       563\n",
      "           1       0.20      1.00      0.33       141\n",
      "\n",
      "    accuracy                           0.20       704\n",
      "   macro avg       0.10      0.50      0.17       704\n",
      "weighted avg       0.04      0.20      0.07       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 2s 597us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 145us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 167us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 159us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 145us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 133us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 145us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 154us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085 - loss: 13.7246 - binary_accuracy: 0.\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 145us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 162us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 175us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 152us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085 - loss: 13.8729 - binary_ac\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 157us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 154us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 142us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 154us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 144us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 13.7152 - binary_accuracy: 0.1085 - loss: 13.7324 - binary_accuracy: 0.\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 149us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 139us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 146us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 147us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 148us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 140us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 157us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 137us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 143us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 138us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 134us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 141us/sample - loss: 13.7152 - binary_accuracy: 0.1085\n",
      "Accuracy: 0.7599431818181818\n",
      "Hamming Loss: 0.125\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       575\n",
      "           1       0.18      1.00      0.31       129\n",
      "\n",
      "    accuracy                           0.18       704\n",
      "   macro avg       0.09      0.50      0.15       704\n",
      "weighted avg       0.03      0.18      0.06       704\n",
      "\n",
      "y_pred shape: \n",
      "(704, 2)\n",
      "y_pred2 shape:\n",
      "(704, 2)\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         4\n",
      "         1.0       0.20      1.00      0.33         1\n",
      "\n",
      "    accuracy                           0.20         5\n",
      "   macro avg       0.10      0.50      0.17         5\n",
      "weighted avg       0.04      0.20      0.07         5\n",
      "\n",
      "Hamming Loss: 0.11178977272727272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-133138430.0, -133138490.0, -133139260.0, -133139390.0, -133141350.0, -133141380.0, -133142360.0, -133142370.0, -133142660.0, -133142690.0, -133142860.0, -133142950.0, -133142980.0, -133143860.0, -133143920.0, -133144000.0, -133144310.0, -133144350.0, -133144440.0, -133145020.0, -133145110.0, -133145144.0, -133145200.0, -133145330.0, -133145450.0, -133145470.0, -133145510.0, -133145820.0, -133145870.0, -133146030.0, -133146090.0, -133146190.0, -133146330.0, -133146650.0, -133146660.0, -133146700.0, -133146744.0, -133146780.0, -133146800.0, -133146860.0, -133146950.0, -133147020.0, -133147190.0, -133147630.0, -133147830.0, -133147976.0, -133148000.0, -133148590.0, -133149510.0, -133170880.0, -133171630.0, -133171650.0, -133171660.0, -133171970.0, -133172000.0, -133172010.0, -133172300.0, -133172660.0, -133172664.0, -133172740.0, -133173016.0, -133173100.0, -133173210.0, -133173310.0, -133173390.0, -133173460.0, -133173464.0, -133173680.0, -133173704.0, -133173710.0, -133173784.0, -133173800.0, -133174080.0, -133174220.0, -133174260.0, -133174270.0, -133174320.0, -133174370.0, -133174376.0, -133174380.0, -133174660.0, -133174700.0, -133174800.0, -133174960.0, -133175090.0, -133175470.0, -133175520.0, -133175590.0, -133175630.0, -133176000.0, -133176220.0, -133176270.0, -133176296.0, -133176536.0, -133176770.0, -133176810.0, -133177100.0, -133177270.0, -133177410.0, -133177490.0, -133177500.0, -133177620.0, -133177710.0, -133177840.0, -133177850.0, -133177976.0, -133178130.0, -133178300.0, -133178320.0, -133178370.0, -133178980.0, -133179140.0, -133179610.0, -133202510.0, -133202580.0, -133202776.0, -133202860.0, -133202970.0, -133202980.0, -133203010.0, -133203090.0, -133203100.0, -133203230.0, -133203256.0, -133203384.0, -133203416.0, -133203620.0, -133203650.0, -133204200.0, -133204450.0, -133204456.0, -133204536.0, -133204570.0, -133204700.0, -133204820.0, -133204830.0, -133204910.0, -133204990.0, -133205010.0, -133205064.0, -133205090.0, -133205120.0, -133205176.0, -133205180.0, -133205360.0, -133205660.0, -133205900.0, -133205930.0, -133206020.0, -133206060.0, -133206340.0, -133206650.0, -133206820.0, -133207070.0, -133207190.0, -133207300.0, -133207336.0, -133207420.0, -133207640.0, -133208000.0, -133208010.0, -133208200.0, -133208264.0, -133208390.0, -133208500.0, -133208690.0, -133208744.0, -133208930.0, -133209280.0, -133209500.0, -133209570.0, -133209620.0, -133209784.0, -133211220.0, -133211580.0, -133235300.0, -133235960.0, -133236180.0, -133236210.0, -133236220.0, -133236270.0, -133236290.0, -133236400.0, -133236410.0, -133236510.0, -133236520.0, -133236550.0, -133236800.0, -133236860.0, -133236930.0, -133237030.0, -133237200.0, -133237250.0, -133237330.0, -133237420.0, -133237576.0, -133237690.0, -133237750.0, -133237820.0, -133237830.0, -133237900.0, -133237970.0, -133238100.0, -133238170.0, -133238584.0, -133238590.0, -133238750.0, -133238776.0, -133239650.0, -133239670.0, -133239700.0, -133239950.0, -133240040.0, -133240270.0, -133240824.0, -133241600.0, -133241610.0, -133241840.0, -133241910.0, -133242180.0, -133242260.0, -133242290.0, -133242410.0, -133242720.0, -133242920.0, -133243060.0, -133243150.0, -133243544.0, -133244350.0, -133244380.0, -133267176.0, -133267576.0, -133267600.0, -133268216.0, -133268220.0, -133268250.0, -133268270.0, -133268504.0, -133268820.0, -133268830.0, -133269150.0, -133269496.0, -133269576.0, -133269740.0, -133270240.0, -133270340.0, -133270720.0, -133270750.0, -133271120.0, -133271130.0, -133271180.0, -133271200.0, -133271384.0, -133271440.0, -133271464.0, -133271500.0, -133271530.0, -133271540.0, -133271680.0, -133271736.0, -133271900.0, -133272190.0, -133272860.0, -133272870.0, -133273280.0, -133273460.0, -133273630.0, -133273640.0, -133274470.0, -133275580.0, -133275630.0, -133276690.0, -133299624.0, -133299730.0, -133299840.0, -133299880.0, -133299980.0, -133300260.0, -133300370.0, -133300420.0, -133300570.0, -133300690.0, -133300776.0, -133300820.0, -133300930.0, -133301120.0, -133301200.0, -133301520.0, -133301820.0, -133302190.0, -133302240.0, -133302290.0, -133302380.0, -133302530.0, -133302580.0, -133302824.0, -133302850.0, -133302910.0, -133303224.0, -133303390.0, -133303430.0, -133303510.0, -133303630.0, -133303840.0, -133304056.0, -133304100.0, -133304130.0, -133304380.0, -133304570.0, -133304616.0, -133304690.0, -133304800.0, -133305064.0, -133305080.0, -133305250.0, -133305350.0, -133305400.0, -133305550.0, -133305650.0, -133305780.0, -133305850.0, -133306250.0, -133306350.0, -133306380.0, -133306400.0, -133306664.0, -133306740.0, -133306980.0, -133307860.0, -133330900.0, -133331140.0, -133331280.0, -133331700.0, -133331740.0, -133332530.0, -133332540.0, -133332616.0, -133333080.0, -133333140.0, -133333540.0, -133333590.0, -133333630.0, -133333660.0, -133333820.0, -133333920.0, -133333950.0, -133333980.0, -133334030.0, -133334140.0, -133334260.0, -133334376.0, -133334420.0, -133334460.0, -133334610.0, -133334690.0, -133334700.0, -133334770.0, -133334830.0, -133334900.0, -133334960.0, -133335016.0, -133335060.0, -133335200.0, -133335270.0, -133335330.0, -133335460.0, -133335490.0, -133335530.0, -133335544.0, -133335740.0, -133335850.0, -133336240.0, -133336260.0, -133336296.0, -133336310.0, -133336340.0, -133336520.0, -133336590.0, -133336664.0, -133336680.0, -133336840.0, -133337096.0, -133337650.0, -133337740.0, -133338640.0, -133338740.0, -133338770.0, -133339130.0, -133339320.0, -133339340.0, -133339900.0, -133340170.0, -133340376.0, -133340590.0, -133362940.0, -133363180.0, -133363464.0, -133363470.0, -133363500.0, -133363540.0, -133364020.0, -133364140.0, -133364370.0, -133364530.0, -133365140.0, -133365290.0, -133365720.0, -133365750.0, -133366040.0, -133366050.0, -133366450.0, -133366560.0, -133366730.0, -133366990.0, -133367280.0, -133367300.0, -133367550.0, -133367590.0, -133367630.0, -133367850.0, -133368090.0, -133368790.0, -133368940.0, -133369816.0, -133369880.0, -133370190.0, -133370420.0, -133370850.0, -133370860.0, -133370870.0, -133371410.0, -133371470.0, -133371490.0, -133371950.0, -133372120.0, -133372430.0, -133394936.0, -133395800.0, -133395870.0, -133396110.0, -133396200.0, -133396450.0, -133396480.0, -133396560.0, -133396744.0, -133396856.0, -133397090.0, -133397140.0, -133397520.0, -133397740.0, -133397760.0, -133397780.0, -133398024.0, -133398050.0, -133398664.0, -133398690.0, -133398700.0, -133398730.0, -133398856.0, -133399090.0, -133399220.0, -133399240.0, -133399300.0, -133399350.0, -133399370.0, -133399624.0, -133399700.0, -133399980.0, -133400240.0, -133400490.0, -133400670.0, -133400690.0, -133400696.0, -133400990.0, -133401010.0, -133401020.0, -133401290.0, -133401544.0, -133401620.0, -133401870.0, -133401920.0, -133401980.0, -133402060.0, -133402270.0, -133402490.0, -133402590.0, -133402860.0, -133402960.0, -133403150.0, -133403270.0, -133403650.0, -133427120.0, -133427140.0, -133427440.0, -133427500.0, -133427576.0, -133427780.0, -133427790.0, -133427840.0, -133427890.0, -133427896.0, -133428030.0, -133428190.0, -133428216.0, -133428240.0, -133428260.0, -133428264.0, -133428470.0, -133428490.0, -133428520.0, -133428590.0, -133428800.0, -133428840.0, -133429416.0, -133429464.0, -133429496.0, -133429540.0, -133430136.0, -133430190.0, -133430520.0, -133430744.0, -133430780.0, -133430900.0, -133430960.0, -133431030.0, -133431230.0, -133431304.0, -133431310.0, -133431390.0, -133431440.0, -133431620.0, -133431670.0, -133431700.0, -133431730.0, -133431740.0, -133431770.0, -133431900.0, -133432430.0, -133432450.0, -133432536.0, -133432690.0, -133432960.0, -133432980.0, -133433310.0, -133433460.0, -133433820.0, -133434130.0, -133434230.0, -133434350.0, -133434560.0, -133434930.0, -133434936.0, -133434970.0, -133435200.0, -133435250.0, -133435600.0, -133435860.0, -133436210.0, -133436344.0, -133436390.0, -133436570.0, -133436600.0, -133436700.0, -133436730.0, -133436770.0, -133436984.0, -133437040.0, -133458330.0, -133458570.0, -133458610.0, -133459310.0, -133459460.0, -133459470.0, -133459630.0, -133459970.0, -133460140.0, -133460210.0, -133460250.0, -133460560.0, -133460870.0, -133460910.0, -133460930.0, -133460990.0, -133461110.0, -133461150.0, -133461180.0, -133461270.0, -133461330.0, -133461650.0, -133461680.0, -133461780.0, -133462100.0, -133462180.0, -133462210.0, -133462216.0, -133462240.0, -133462280.0, -133462296.0, -133462420.0, -133462640.0, -133462820.0, -133462880.0, -133463050.0, -133463176.0, -133463240.0, -133463250.0, -133463320.0, -133463410.0, -133463416.0, -133463420.0, -133463440.0, -133463500.0, -133463640.0, -133463656.0, -133463720.0, -133463890.0, -133464050.0, -133464060.0, -133464080.0, -133464250.0, -133464450.0, -133464460.0, -133464750.0, -133465420.0, -133465460.0, -133465500.0, -133466640.0, -133466856.0, -133466960.0, -133467270.0, -133467330.0, -133467670.0, -133490710.0, -133490960.0, -133491010.0, -133491600.0, -133491650.0, -133491690.0, -133491700.0, -133491730.0, -133491736.0, -133491760.0, -133491960.0, -133491980.0, -133491990.0, -133492216.0, -133492290.0, -133492300.0, -133492620.0, -133492660.0, -133492940.0, -133492970.0, -133493230.0, -133493704.0, -133493890.0, -133494060.0, -133494140.0, -133494190.0, -133494200.0, -133494230.0, -133494300.0, -133494430.0, -133494610.0, -133494850.0, -133494860.0, -133494910.0, -133495410.0, -133495520.0, -133495530.0, -133495690.0, -133496140.0, -133496440.0, -133496530.0, -133496750.0, -133496780.0, -133497710.0, -133497740.0, -133498750.0, -133498890.0, -133499650.0, -133499970.0, -133500320.0, -133500376.0, -133500430.0, 63536040.0, 63536360.0, 63536400.0, 63536430.0, 63536440.0, 63536480.0, 63536550.0, 63536570.0, 63536600.0, 63536640.0, 63536656.0, 63536664.0, 63536730.0, 63536776.0, 63536784.0, 63536790.0, 63536824.0, 63536870.0, 63536910.0, 63536930.0, 63536976.0, 63537110.0, 63537130.0, 63537136.0, 63537176.0, 63537184.0, 63537230.0, 63537280.0, 63537320.0, 63537336.0, 63537344.0, 63537350.0, 63537360.0, 63537370.0, 63537376.0, 63537400.0, 63537440.0, 63537450.0, 63537490.0, 63537510.0, 63537590.0, 63537664.0, 63537690.0, 63537760.0, 63537770.0, 63537824.0, 63537970.0, 63537990.0, 63538230.0, 63538410.0, 63541216.0, 63541336.0, 63541390.0, 63541510.0, 63541624.0, 63541696.0, 63541710.0, 63541730.0, 63541784.0, 63541810.0, 63541824.0, 63541880.0, 63541896.0, 63541920.0, 63541930.0, 63541936.0, 63541950.0, 63541984.0, 63542024.0, 63542030.0, 63542080.0, 63542090.0, 63542130.0, 63542144.0, 63542160.0, 63542190.0, 63542200.0, 63542216.0, 63542224.0, 63542230.0, 63542264.0, 63542296.0, 63542304.0, 63542344.0, 63542384.0, 63542390.0, 63542400.0, 63542440.0, 63542456.0, 63542470.0, 63542490.0, 63542504.0, 63542520.0, 63542530.0, 63542600.0, 63542630.0, 63542640.0, 63542664.0, 63542750.0, 63542776.0, 63542810.0, 63542840.0, 63542870.0, 63542904.0, 63543030.0, 63543040.0, 63543070.0, 63543160.0, 63543520.0, 63543550.0, 63546176.0, 63546280.0, 63546304.0, 63546330.0, 63546336.0, 63546344.0, 63546560.0, 63546656.0, 63546730.0, 63546736.0, 63546744.0, 63546824.0, 63546840.0, 63546850.0, 63546856.0, 63546930.0, 63546970.0, 63546990.0, 63547000.0, 63547016.0, 63547110.0, 63547130.0, 63547136.0, 63547160.0, 63547170.0, 63547224.0, 63547230.0, 63547250.0, 63547264.0, 63547290.0, 63547296.0, 63547320.0, 63547330.0, 63547384.0, 63547400.0, 63547416.0, 63547450.0, 63547496.0, 63547530.0, 63547550.0, 63547560.0, 63547570.0, 63547584.0, 63547624.0, 63547630.0, 63547664.0, 63547670.0, 63547680.0, 63547690.0, 63547730.0, 63547736.0, 63547760.0, 63547770.0, 63547776.0, 63547784.0, 63547790.0, 63547864.0, 63547920.0, 63547970.0, 63548056.0, 63548070.0, 63548110.0, 63548184.0, 63548190.0, 63548200.0, 63548680.0, 63548744.0, 63551664.0, 63551680.0, 63551704.0, 63551840.0, 63551864.0, 63551880.0, 63551930.0, 63552030.0, 63552070.0, 63552096.0, 63552104.0, 63552136.0, 63552150.0, 63552176.0, 63552184.0, 63552210.0, 63552240.0, 63552270.0, 63552400.0, 63552430.0, 63552470.0, 63552480.0, 63552510.0, 63552520.0, 63552550.0, 63552600.0, 63552640.0, 63552656.0, 63552664.0, 63552670.0, 63552690.0, 63552710.0, 63552720.0, 63552770.0, 63552824.0, 63552840.0, 63552864.0, 63552880.0, 63552904.0, 63552950.0, 63552970.0, 63552990.0, 63553144.0, 63553210.0, 63553230.0, 63553250.0, 63553264.0, 63553376.0, 63553430.0, 63553490.0, 63556944.0, 63557024.0, 63557184.0, 63557224.0, 63557240.0, 63557250.0, 63557270.0, 63557310.0, 63557344.0, 63557350.0, 63557430.0, 63557480.0, 63557510.0, 63557550.0, 63557630.0, 63557650.0, 63557750.0, 63557760.0, 63557800.0, 63557810.0, 63557816.0, 63557850.0, 63557864.0, 63557970.0, 63557990.0, 63558000.0, 63558136.0, 63558144.0, 63558150.0, 63558176.0, 63558200.0, 63558210.0, 63558224.0, 63558256.0, 63558330.0, 63558350.0, 63558536.0, 63558640.0, 63558680.0, 63558690.0, 63558696.0, 63562190.0, 63562350.0, 63562424.0, 63562600.0, 63562616.0, 63562640.0, 63562664.0, 63562720.0, 63562744.0, 63562750.0, 63562760.0, 63562790.0, 63562800.0, 63562824.0, 63562864.0, 63562870.0, 63562880.0, 63562904.0, 63562944.0, 63562950.0, 63562970.0, 63562984.0, 63563010.0, 63563016.0, 63563030.0, 63563050.0, 63563056.0, 63563064.0, 63563070.0, 63563080.0, 63563090.0, 63563144.0, 63563184.0, 63563216.0, 63563230.0, 63563240.0, 63563250.0, 63563330.0, 63563376.0, 63563390.0, 63563450.0, 63563456.0, 63563520.0, 63563576.0, 63563624.0, 63563680.0, 63563710.0, 63563776.0, 63563784.0, 63563810.0, 63563816.0, 63563840.0, 63563856.0, 63563870.0, 63563890.0, 63563896.0, 63564040.0, 63564136.0, 63567450.0, 63567550.0, 63567570.0, 63567584.0, 63567640.0, 63567690.0, 63567696.0, 63567704.0, 63567830.0, 63567864.0, 63567880.0, 63567896.0, 63567950.0, 63567970.0, 63567976.0, 63568010.0, 63568096.0, 63568104.0, 63568130.0, 63568144.0, 63568150.0, 63568184.0, 63568216.0, 63568296.0, 63568400.0, 63568464.0, 63568470.0, 63568490.0, 63568510.0, 63568544.0, 63568560.0, 63568584.0, 63568616.0, 63568624.0, 63568670.0, 63568704.0, 63568750.0, 63568784.0, 63568790.0, 63568824.0, 63568830.0, 63568890.0, 63568896.0, 63568910.0, 63568936.0, 63569040.0, 63569090.0, 63569170.0, 63569216.0, 63569240.0, 63569310.0, 63569330.0, 63569344.0, 63572704.0, 63572730.0, 63572770.0, 63573030.0, 63573040.0, 63573080.0, 63573104.0, 63573110.0, 63573176.0, 63573184.0, 63573210.0, 63573216.0, 63573270.0, 63573290.0, 63573344.0, 63573410.0, 63573416.0, 63573440.0, 63573450.0, 63573510.0, 63573520.0, 63573530.0, 63573570.0, 63573576.0, 63573610.0, 63573650.0, 63573670.0, 63573744.0, 63573750.0, 63573776.0, 63573840.0, 63573930.0, 63574064.0, 63574090.0, 63574150.0, 63574216.0, 63574224.0, 63574290.0, 63574390.0, 63574400.0, 63574496.0, 63577920.0, 63578024.0, 63578080.0, 63578120.0, 63578144.0, 63578160.0, 63578216.0, 63578224.0, 63578240.0, 63578250.0, 63578410.0, 63578440.0, 63578456.0, 63578496.0, 63578504.0, 63578510.0, 63578520.0, 63578530.0, 63578576.0, 63578656.0, 63578704.0, 63578710.0, 63578736.0, 63578744.0, 63578750.0, 63578784.0, 63578824.0, 63578830.0, 63578850.0, 63578864.0, 63578870.0, 63578890.0, 63578896.0, 63578930.0, 63578936.0, 63578970.0, 63578984.0, 63579000.0, 63579016.0, 63579030.0, 63579040.0, 63579056.0, 63579090.0, 63579096.0, 63579190.0, 63579480.0, 63579544.0, 63579550.0, 63579720.0, 63583350.0, 63583490.0, 63583530.0, 63583616.0, 63583630.0, 63583640.0, 63583650.0, 63583696.0, 63583710.0, 63583730.0, 63583790.0, 63583840.0, 63583870.0, 63583880.0, 63583896.0, 63583910.0, 63583920.0, 63583936.0, 63583950.0, 63583970.0, 63583984.0, 63584000.0, 63584024.0, 63584030.0, 63584064.0, 63584080.0, 63584096.0, 63584144.0, 63584200.0, 63584210.0, 63584264.0, 63584270.0, 63584296.0, 63584310.0, 63584336.0, 63584350.0, 63584370.0, 63584376.0, 63584384.0, 63584440.0, 63584450.0, 63584496.0, 63584510.0, 63584544.0, 63584630.0, 63584656.0, 63584680.0, 63584800.0, 63584824.0, 63584880.0, 63584930.0, 63585136.0, 63585160.0, 63588264.0, 63588370.0, 63588440.0, 63588510.0, 63588520.0, 63588550.0, 63588576.0, 63588590.0, 63588690.0, 63588704.0, 63588710.0, 63588730.0, 63588736.0, 63588750.0, 63588790.0, 63588830.0, 63588880.0, 63588890.0, 63588950.0, 63588970.0, 63589000.0, 63589010.0, 63589030.0, 63589056.0, 63589064.0, 63589070.0, 63589136.0, 63589144.0, 63589176.0, 63589216.0, 63589250.0, 63589270.0, 63589304.0, 63589330.0, 63589336.0, 63589344.0, 63589360.0, 63589370.0, 63589384.0, 63589410.0, 63589424.0, 63589470.0, 63589510.0, 63589590.0, 63589616.0, 63589624.0, 63589640.0, 63589720.0, 63589770.0, 63589830.0, 63589890.0, 63590144.0, 63590160.0, 63590200.0, 63590240.0, 63593424.0, 63593520.0, 63593550.0, 63593640.0, 63593650.0, 63593730.0, 63593744.0, 63593770.0, 63593800.0, 63593810.0, 63593824.0, 63593856.0, 63593880.0, 63593896.0, 63593944.0, 63594016.0, 63594024.0, 63594030.0, 63594040.0, 63594064.0, 63594080.0, 63594110.0, 63594136.0, 63594176.0, 63594184.0, 63594230.0, 63594264.0, 63594270.0, 63594310.0, 63594360.0, 63594370.0, 63594390.0, 63594410.0, 63594424.0, 63594430.0, 63594464.0, 63594470.0, 63594510.0, 63594560.0, 63594600.0, 63594650.0, 63594704.0, 63594856.0, 63594910.0, 63598650.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "y_pred3, pred3, y_test3, hl3 = list(),list(),list(),list()\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(1)\n",
    "  #]) \n",
    "    \n",
    "  #inputs = tf.keras.layers.Input(shape=(X_train.shape))\n",
    "  #outputs = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "  #model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "\n",
    "  #outputs = keras.layers.Dense(tf.keras.layers.Dense(number_of_classes)) #(x) \n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(2)\n",
    "  #])   \n",
    "  \n",
    "  #model = tf.contrib.learn.DNNClassifier(hidden_units=[5,10,5],\n",
    "  #                                          n_classes=4)    \n",
    "    \n",
    "\n",
    "  inputs = tf.keras.Input(shape=(X_train.shape))\n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    \n",
    "  outputs = tf.keras.layers.Dense(number_of_classes)  #, activation=activation)\n",
    "                             #kernel_initializer=initializer,\n",
    "                             #activation=activation) #(x) \n",
    "  #model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(2)\n",
    "  ]) \n",
    "    \n",
    "  #model.add(layers.Activation(activations.relu))\n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  #model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.001))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  #print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "    \n",
    "  #y_pred2.append(y_pred)\n",
    "  y_pred3 = np.append(y_pred3, y_pred2)\n",
    "  #pred2.append(pred)\n",
    "  pred3 = np.append(pred3, pred2)\n",
    "  #y_test2.append(y_test)\n",
    "  y_test3 = np.append(y_test3, y_test2)\n",
    "  #hl2.append(hl)\n",
    "  hl3 = np.append(hl3, hl)\n",
    "    \n",
    "print(\"y_pred shape: \")\n",
    "print(np.shape(y_pred))\n",
    "\n",
    "print(\"y_pred2 shape:\")\n",
    "print(np.shape(y_pred2))\n",
    "\n",
    "y_pred3 = np.concatenate((y_pred3[0], y_pred3[1], y_pred3[2], y_pred3[3], y_pred3[4]), axis=None)\n",
    "pred3 = np.concatenate((pred3[0], pred3[1], pred3[2], pred3[3], pred3[4]), axis=None)\n",
    "y_test3 = np.concatenate((y_test3[0], y_test3[1], y_test3[2], y_test3[3], y_test3[4]), axis=None)\n",
    "hl3 = np.concatenate((hl3[0], hl3[1], hl3[2], hl3[3], hl3[4]), axis=None)\n",
    "\n",
    "print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded, Knowledge Graph-Based Feature Selection - TensorFlow CNN AGGREGATE\")\n",
    "#print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "print(classification_report(y_test3, pred3))\n",
    "hl3_avg = sum(hl3) / len(hl3)\n",
    "print(\"Hamming Loss: \" + str(hl3_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
